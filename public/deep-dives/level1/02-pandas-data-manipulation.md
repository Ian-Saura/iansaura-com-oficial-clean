---
id: "l1-pandas-manipulation"
version: "1.0.0"
lastUpdated: "2026-01-05"

title:
  es: "Pandas: ManipulaciÃ³n de Datos a Escala"
  en: "Pandas: Data Manipulation at Scale"
  pt: "Pandas: ManipulaÃ§Ã£o de Dados em Escala"

subtitle:
  es: "De Excel a DataFrames: el salto profesional"
  en: "From Excel to DataFrames: the professional leap"
  pt: "Do Excel para DataFrames: o salto profissional"

level: 1
phase: "l1-pandas"
estimatedTime: "20-25 horas"

prerequisites:
  - "l1-python-fundamentals"

tags:
  - "python"
  - "pandas"
  - "data-manipulation"
  - "etl"

theoreticalFoundations:
  - "Ãlgebra relacional"
  - "VectorizaciÃ³n SIMD"
  - "Ãndices y estructuras de datos"
  - "Lazy vs Eager evaluation"
---

<!-- 
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“š BLOQUE: PANDAS DATA MANIPULATION                         â•‘
â•‘  Nivel: 1 | Fase: ManipulaciÃ³n de Datos                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-->

# ğŸ¼ Pandas: ManipulaciÃ³n de Datos a Escala

> **Objetivo**: Dominar Pandas como herramienta de transformaciÃ³n de datos profesional, entendiendo cuÃ¡ndo usarlo y cuÃ¡ndo necesitas algo mÃ¡s potente (Spark, Polars).

---

## ğŸ§  Mapa Conceptual

```mermaid
mindmap
  root((Pandas
    Mastery))
    ğŸ”¬ Fundamentos
      DataFrame
        Ãndice inmutable
        Columnas tipadas
        Memoria contigua
      Series
        Array 1D con Ã­ndice
        VectorizaciÃ³n
      Index
        Label-based access
        Hierarchical MultiIndex
    ğŸ“– Operaciones Core
      SelecciÃ³n
        loc - por label
        iloc - por posiciÃ³n
        Boolean indexing
      TransformaciÃ³n
        apply/map/transform
        assign
        pipe
      AgregaciÃ³n
        groupby
        pivot_table
        resample
      Merge/Join
        merge
        concat
        join
    âš¡ Performance
      VectorizaciÃ³n
        Evitar apply con lambda
        Usar operaciones nativas
      Tipos de Datos
        category para strings
        datetime64
        nullable Int64
      Chunks
        read_csv chunks
        Procesamiento iterativo
    ğŸ”„ IntegraciÃ³n
      SQL
        read_sql
        to_sql
      Parquet
        Columnar storage
        CompresiÃ³n
      Arrow
        Interop con Spark
        Zero-copy
```

---

## ğŸ”— First Principles: De la TeorÃ­a a la PrÃ¡ctica

| Concepto CS | QuÃ© significa | ImplementaciÃ³n en Pandas |
|-------------|---------------|--------------------------|
| **Ãlgebra Relacional** | Operaciones matemÃ¡ticas sobre conjuntos de tuplas | `merge` = JOIN, `groupby` = GROUP BY, boolean indexing = WHERE. Pandas implementa operaciones relacionales. |
| **VectorizaciÃ³n (SIMD)** | Una instrucciÃ³n opera sobre mÃºltiples datos | Operaciones como `df['col'] * 2` se ejecutan en C/NumPy, no Python. 100x mÃ¡s rÃ¡pido que loops. |
| **Columnar Storage** | Datos almacenados por columna, no por fila | Parquet agrupa valores del mismo tipo â†’ mejor compresiÃ³n y lectura selectiva de columnas. |
| **Immutability** | Objetos que no cambian despuÃ©s de crearse | El Index de Pandas es inmutable. Muchas operaciones retornan copias, no modifican in-place. |
| **Locality of Reference** | Datos cercanos en memoria se acceden juntos | Arrays NumPy subyacentes son contiguos â†’ cache-friendly â†’ rÃ¡pido. |
| **Lazy vs Eager** | Computar ahora vs computar cuando se necesite | Pandas es eager (ejecuta inmediatamente). Alternativas lazy: Polars, Dask, Spark. |

> [!IMPORTANT]
> ğŸ§  **First Principle clave**: Pandas estÃ¡ construido sobre **NumPy arrays contiguos en memoria**. Cada vez que iteras fila por fila, rompes esta optimizaciÃ³n y pagas el costo de Python puro. **Piensa en columnas, no en filas.**

---

## ğŸ“‹ Technical Cheat Sheet

### ğŸ–¥ï¸ Comandos CLI / Inicio RÃ¡pido

```python
import pandas as pd
import numpy as np

# ConfiguraciÃ³n recomendada para desarrollo
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
pd.set_option('display.width', None)
pd.set_option('display.float_format', '{:.2f}'.format)

# Leer archivos (especificar tipos mejora memoria 5-10x)
df = pd.read_csv('data.csv', 
    dtype={'category_col': 'category', 'id': 'int32'},
    parse_dates=['date_col'],
    usecols=['col1', 'col2', 'col3']  # Solo columnas necesarias
)

# Leer archivos grandes en chunks
chunks = pd.read_csv('huge.csv', chunksize=100_000)
for chunk in chunks:
    process(chunk)

# Parquet: SIEMPRE preferir sobre CSV para producciÃ³n
df.to_parquet('data.parquet', compression='snappy', index=False)
df = pd.read_parquet('data.parquet', columns=['col1', 'col2'])
```

### ğŸ“ Snippets de Alta Densidad

#### PatrÃ³n 1: SelecciÃ³n Eficiente (loc vs iloc)

```python
# ğŸ”¥ BEST PRACTICE: Entender loc vs iloc previene bugs sutiles
# loc = Label-based, iloc = Integer-position based

# SelecciÃ³n por label (nombres de columnas/Ã­ndice)
df.loc[df['status'] == 'active', ['name', 'email']]

# SelecciÃ³n por posiciÃ³n (nÃºmeros)
df.iloc[0:100, [0, 2, 5]]  # Primeras 100 filas, columnas 0, 2, 5

# âš ï¸ CUIDADO: Si el Ã­ndice es numÃ©rico, loc y iloc difieren
df = pd.DataFrame({'a': [1,2,3]}, index=[10, 20, 30])
df.loc[10]   # Fila con Ã­ndice label 10
df.iloc[0]   # Primera fila (Ã­ndice posiciÃ³n 0)

# Boolean indexing (el mÃ¡s usado)
mask = (df['age'] > 25) & (df['country'].isin(['AR', 'CL', 'MX']))
df_filtered = df.loc[mask]

# Query syntax (mÃ¡s legible para condiciones complejas)
df.query('age > 25 and country in ["AR", "CL", "MX"]')
```

#### PatrÃ³n 2: Transformaciones Vectorizadas

```python
# ğŸ”¥ BEST PRACTICE: Vectorizar TODO lo posible
# CuÃ¡ndo usar: SIEMPRE que transformes datos

# âŒ LENTO - apply con lambda (ejecuta Python por cada fila)
df['total'] = df.apply(lambda row: row['price'] * row['qty'], axis=1)

# âœ… RÃPIDO - OperaciÃ³n vectorizada (ejecuta en C)
df['total'] = df['price'] * df['qty']

# âŒ LENTO - Loop explÃ­cito
for idx, row in df.iterrows():
    df.loc[idx, 'upper'] = row['name'].upper()

# âœ… RÃPIDO - MÃ©todos de string vectorizados
df['upper'] = df['name'].str.upper()

# Operaciones condicionales vectorizadas
df['category'] = np.where(df['value'] > 100, 'high', 'low')

# MÃºltiples condiciones con np.select
conditions = [
    df['value'] > 100,
    df['value'] > 50,
    df['value'] > 0
]
choices = ['high', 'medium', 'low']
df['category'] = np.select(conditions, choices, default='unknown')
```

#### PatrÃ³n 3: GroupBy como un Pro

```python
# ğŸ”¥ BEST PRACTICE: GroupBy es tu herramienta mÃ¡s poderosa
# CuÃ¡ndo usar: Agregaciones, transformaciones por grupo

# AgregaciÃ³n bÃ¡sica
df.groupby('category')['sales'].sum()

# MÃºltiples agregaciones
df.groupby('category').agg({
    'sales': ['sum', 'mean', 'count'],
    'profit': 'sum',
    'customer_id': 'nunique'
})

# Named aggregations (Pandas 0.25+) - MÃ¡s limpio
df.groupby('category').agg(
    total_sales=('sales', 'sum'),
    avg_sales=('sales', 'mean'),
    unique_customers=('customer_id', 'nunique')
)

# Transform: Retorna mismo shape que input (Ãºtil para normalizaciÃ³n)
df['sales_pct_of_category'] = df.groupby('category')['sales'].transform(
    lambda x: x / x.sum()
)

# Filter: Filtrar grupos completos
# Solo categorÃ­as con mÃ¡s de 100 ventas
df.groupby('category').filter(lambda x: x['sales'].sum() > 100)
```

#### PatrÃ³n 4: Merge/Join Correctamente

```python
# ğŸ”¥ BEST PRACTICE: Validar siempre los resultados del merge
# CuÃ¡ndo usar: Combinar tablas relacionadas

# Merge bÃ¡sico (inner por defecto)
df_merged = pd.merge(df_orders, df_customers, on='customer_id')

# Left join con validaciÃ³n
df_merged = pd.merge(
    df_orders, 
    df_customers, 
    on='customer_id',
    how='left',
    validate='m:1',  # Muchos orders, un customer
    indicator=True   # AÃ±ade columna _merge para debug
)

# Verificar el merge
print(df_merged['_merge'].value_counts())
# both          950  â† OK, match encontrado
# left_only      50  â† Ã“rdenes sin customer (investigar!)
# right_only      0

# Merge con claves diferentes
df_merged = pd.merge(
    df_left, df_right,
    left_on='id_cliente',
    right_on='customer_id'
)

# Anti-join (registros que NO tienen match)
df_no_match = df_left.merge(df_right, on='id', how='left', indicator=True)
df_no_match = df_no_match[df_no_match['_merge'] == 'left_only']
```

#### PatrÃ³n 5: Manejo de Fechas

```python
# ğŸ”¥ BEST PRACTICE: Siempre parsear fechas al leer
# CuÃ¡ndo usar: Cualquier dato temporal

# Al leer
df = pd.read_csv('data.csv', parse_dates=['fecha'])

# Convertir columna existente
df['fecha'] = pd.to_datetime(df['fecha'], format='%Y-%m-%d')

# Extraer componentes
df['year'] = df['fecha'].dt.year
df['month'] = df['fecha'].dt.month
df['day_of_week'] = df['fecha'].dt.dayofweek  # 0=Monday
df['is_weekend'] = df['fecha'].dt.dayofweek >= 5

# Resampling (agrupar por perÃ­odo)
df.set_index('fecha').resample('M')['ventas'].sum()  # Por mes
df.set_index('fecha').resample('W')['ventas'].mean()  # Por semana

# Rolling windows
df['sales_7d_avg'] = df['sales'].rolling(window=7).mean()
df['sales_30d_sum'] = df['sales'].rolling(window=30).sum()

# Shift (comparar con perÃ­odo anterior)
df['sales_yesterday'] = df['sales'].shift(1)
df['sales_diff'] = df['sales'] - df['sales'].shift(1)
df['sales_pct_change'] = df['sales'].pct_change()
```

### ğŸ—ï¸ Patrones de DiseÃ±o Aplicados

#### 1. Method Chaining (Fluent Interface)

```python
# âœ… PATRÃ“N RECOMENDADO: Encadenar operaciones para cÃ³digo legible

result = (
    df
    .query('status == "active"')
    .assign(
        total=lambda x: x['price'] * x['quantity'],
        year=lambda x: x['date'].dt.year
    )
    .groupby(['year', 'category'])
    .agg(
        revenue=('total', 'sum'),
        orders=('order_id', 'count')
    )
    .reset_index()
    .sort_values('revenue', ascending=False)
)
```

**CuÃ¡ndo usar**: Pipelines de transformaciÃ³n. Cada paso es una transformaciÃ³n pura.

**Trade-offs**:
- âœ… Muy legible, fÃ¡cil de entender el flujo
- âœ… Cada paso es debuggeable (romper la cadena)
- âŒ No todos los mÃ©todos retornan DataFrame (verificar docs)

#### 2. Pipe Pattern para Funciones Reutilizables

```python
def clean_column_names(df: pd.DataFrame) -> pd.DataFrame:
    """Normaliza nombres de columnas."""
    df.columns = df.columns.str.lower().str.replace(' ', '_')
    return df

def remove_duplicates(df: pd.DataFrame, subset: list) -> pd.DataFrame:
    """Elimina duplicados manteniendo el primero."""
    return df.drop_duplicates(subset=subset, keep='first')

def add_audit_columns(df: pd.DataFrame) -> pd.DataFrame:
    """AÃ±ade columnas de auditorÃ­a."""
    return df.assign(
        _loaded_at=pd.Timestamp.now(),
        _source='pipeline_v1'
    )

# Usar con pipe
result = (
    df
    .pipe(clean_column_names)
    .pipe(remove_duplicates, subset=['id'])
    .pipe(add_audit_columns)
)
```

**CuÃ¡ndo usar**: Funciones de transformaciÃ³n reutilizables entre pipelines.

#### 3. Accessor Pattern (Custom Methods)

```python
@pd.api.extensions.register_dataframe_accessor("de")
class DataEngineeringAccessor:
    """Custom accessor para operaciones comunes de DE."""
    
    def __init__(self, pandas_obj):
        self._obj = pandas_obj
    
    def profile(self):
        """Genera perfil rÃ¡pido del DataFrame."""
        return pd.DataFrame({
            'dtype': self._obj.dtypes,
            'nulls': self._obj.isnull().sum(),
            'null_pct': (self._obj.isnull().sum() / len(self._obj) * 100).round(2),
            'unique': self._obj.nunique()
        })
    
    def to_warehouse(self, table_name: str, conn):
        """Escribe a warehouse con convenciones estÃ¡ndar."""
        df = self._obj.copy()
        df.columns = df.columns.str.lower()
        df.to_sql(table_name, conn, if_exists='append', index=False)

# Uso
df.de.profile()
df.de.to_warehouse('fact_sales', connection)
```

### âš ï¸ Gotchas de Nivel Senior

> [!WARNING]
> **Gotcha #1: SettingWithCopyWarning (El mÃ¡s comÃºn)**
> 
> Pandas no sabe si estÃ¡s modificando una vista o una copia.
> 
> ```python
> # âŒ PELIGROSO - Puede no modificar el original
> df[df['status'] == 'active']['price'] = 100
> 
> # âœ… CORRECTO - Usa .loc para modificaciÃ³n in-place
> df.loc[df['status'] == 'active', 'price'] = 100
> 
> # âœ… CORRECTO - Copia explÃ­cita si quieres una nueva variable
> df_active = df[df['status'] == 'active'].copy()
> df_active['price'] = 100
> ```

> [!WARNING]
> **Gotcha #2: Memory Explosion con Merge**
> 
> Un merge many-to-many puede explotar exponencialmente.
> 
> ```python
> # Si df1 tiene 1M filas y df2 tiene 1M filas,
> # un merge con claves duplicadas puede generar BILLONES de filas
> 
> # âœ… SIEMPRE verificar cardinalidad ANTES del merge
> print(f"df1 keys unique: {df1['key'].nunique()}")
> print(f"df2 keys unique: {df2['key'].nunique()}")
> print(f"df1 duplicates: {df1['key'].duplicated().sum()}")
> 
> # âœ… Usar validate para detectar problemas
> df.merge(df2, on='key', validate='one_to_one')  # Falla si hay duplicados
> ```

> [!WARNING]
> **Gotcha #3: Object dtype = Memory Hog**
> 
> El dtype `object` guarda punteros Python, no datos optimizados.
> 
> ```python
> # Ver uso de memoria
> df.info(memory_usage='deep')
> 
> # Columna de strings con pocos valores Ãºnicos
> # âŒ object dtype: 800 MB
> # âœ… category dtype: 50 MB
> 
> # Convertir a category
> df['country'] = df['country'].astype('category')
> 
> # Al leer
> df = pd.read_csv('data.csv', dtype={'country': 'category'})
> ```

> [!WARNING]
> **Gotcha #4: NaN Propagation**
> 
> NaN se propaga en operaciones matemÃ¡ticas.
> 
> ```python
> # NaN + cualquier cosa = NaN
> pd.Series([1, 2, np.nan]).sum()   # 3.0 (NaN ignorado por defecto)
> pd.Series([1, 2, np.nan]).mean()  # 1.5 (NaN ignorado)
> 
> # Pero en operaciones elemento a elemento...
> pd.Series([1, 2, np.nan]) + 10  # [11, 12, NaN]
> 
> # âœ… Decidir explÃ­citamente quÃ© hacer con NaN
> df['total'] = df['price'].fillna(0) * df['qty'].fillna(0)
> ```

> [!WARNING]
> **Gotcha #5: inplace=True es un Anti-patrÃ³n**
> 
> `inplace=True` rompe method chaining y serÃ¡ deprecado.
> 
> ```python
> # âŒ EVITAR
> df.drop(columns=['temp'], inplace=True)
> df.reset_index(inplace=True)
> 
> # âœ… PREFERIR - Reasignar
> df = df.drop(columns=['temp'])
> df = df.reset_index()
> 
> # âœ… O mejor, encadenar
> df = df.drop(columns=['temp']).reset_index()
> ```

---

## ğŸ“Š MÃ©tricas y Benchmarks

| OperaciÃ³n (1M filas) | Loop Python | apply() | Vectorizado | Mejora |
|---------------------|-------------|---------|-------------|--------|
| Suma de columnas | 2.5s | 0.8s | 0.003s | **800x** |
| String upper | 4.2s | 1.2s | 0.15s | **28x** |
| Filtro booleano | 3.1s | N/A | 0.02s | **150x** |
| GroupBy sum | N/A | N/A | 0.05s | - |

| Formato archivo (100M filas) | TamaÃ±o | Tiempo lectura | Notas |
|------------------------------|--------|----------------|-------|
| CSV sin compresiÃ³n | 5.2 GB | 180s | âŒ Evitar en producciÃ³n |
| CSV gzip | 1.1 GB | 220s | Menor tamaÃ±o pero mÃ¡s lento |
| Parquet snappy | 0.8 GB | 12s | âœ… **Recomendado** |
| Parquet + columnas selectas | N/A | 3s | Solo lee lo necesario |

---

## ğŸ“š BibliografÃ­a AcadÃ©mica y Profesional

### ğŸ“– Libros Seminales

| Libro | Autor | CapÃ­tulos relevantes | Por quÃ© leerlo |
|-------|-------|---------------------|----------------|
| **Python for Data Analysis** (3rd Ed) | Wes McKinney | Caps. 4-10 | El creador de Pandas explica el diseÃ±o interno. |
| **Effective Pandas** | Matt Harrison | Todo | Patrones modernos y optimizaciÃ³n. Enfoque prÃ¡ctico. |
| **Pandas Cookbook** (2nd Ed) | Matt Harrison | SegÃºn necesidad | Recetas para problemas especÃ­ficos. |

### ğŸ“„ Papers de InvestigaciÃ³n

1. **"pandas: a Foundational Python Library for Data Analysis and Statistics"** (2010) - Wes McKinney
   - ğŸ”— [Paper original](https://www.dlr.de/sc/Portaldata/15/Resources/dokumente/pyhpc2011/submissions/pyhpc2011_submission_9.pdf)
   - ğŸ’¡ **Insight clave**: MotivaciÃ³n original y decisiones de diseÃ±o de Pandas.

2. **"Apache Arrow: A Cross-language Development Platform for In-memory Analytics"** (2019)
   - ğŸ”— [arrow.apache.org](https://arrow.apache.org/)
   - ğŸ’¡ **Insight clave**: El futuro de Pandas estÃ¡ en Arrow (PyArrow backend).

### ğŸ“‹ Whitepapers y DocumentaciÃ³n TÃ©cnica

- **Pandas User Guide**
  - ğŸ”— [pandas.pydata.org](https://pandas.pydata.org/docs/user_guide/)
  - Relevancia: DocumentaciÃ³n oficial, siempre actualizada.

- **Pandas 2.0 Release Notes**
  - ğŸ”— [Release Notes](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html)
  - Relevancia: PyArrow backend, nullable dtypes, breaking changes.

- **Polars User Guide**
  - ğŸ”— [pola.rs](https://pola.rs/user-guide/)
  - Relevancia: Alternativa mÃ¡s rÃ¡pida a Pandas. Conocerla es ventaja competitiva.

### ğŸ“ Alternativas Modernas (Conocer el Ecosistema)

| Herramienta | CuÃ¡ndo usar | Ventaja sobre Pandas |
|-------------|-------------|---------------------|
| **Polars** | Datos en memoria, performance crÃ­tica | 10-100x mÃ¡s rÃ¡pido, lazy evaluation |
| **DuckDB** | SQL sobre archivos locales | Motor columnar optimizado |
| **Dask** | Datos que no caben en memoria | API compatible con Pandas, distribuido |
| **PySpark** | Big Data distribuido | Escala a petabytes |

---

## ğŸ”„ Conexiones con Otros Bloques

| Bloque relacionado | Tipo de conexiÃ³n | DescripciÃ³n |
|-------------------|------------------|-------------|
| **Python Fundamentals** | Prerequisito | Necesitas dominar Python core primero |
| **SQL Fundamentals** | Paralelo | Mismo modelo mental (operaciones relacionales) |
| **dbt** (Nivel 1) | Alternativa | SQL para transformaciones, Pandas para exploraciÃ³n |
| **PySpark** (Nivel 2) | EvoluciÃ³n | Misma API pero distribuida |
| **Polars** (Nivel 2) | Alternativa moderna | Cuando Pandas es lento |

---

## âœ… Checklist de Dominio

Antes de avanzar, verifica que puedes:

- [ ] Leer CSV/Parquet especificando dtypes correctos
- [ ] Filtrar con boolean indexing sin pensar
- [ ] Usar `loc` vs `iloc` correctamente
- [ ] Escribir groupby con mÃºltiples agregaciones
- [ ] Hacer merge con validaciÃ³n y indicator
- [ ] Manejar fechas con `.dt` accessor
- [ ] Evitar loops usando vectorizaciÃ³n
- [ ] Explicar por quÃ© `apply()` es lento
- [ ] Usar method chaining para pipelines legibles
- [ ] Identificar cuÃ¡ndo Pandas no es suficiente (â†’ Spark/Polars)

---

## ğŸ’¬ Preguntas de AutoevaluaciÃ³n

1. **Conceptual**: Â¿Por quÃ© `df.apply(lambda x: x['a'] + x['b'], axis=1)` es 100x mÃ¡s lento que `df['a'] + df['b']`? Explica quÃ© pasa a nivel de memoria.

2. **PrÃ¡ctica**: Tienes un CSV de 50GB. Tu laptop tiene 16GB RAM. Â¿CÃ³mo lo procesarÃ­as con Pandas? (Hay al menos 3 estrategias vÃ¡lidas)

3. **DiseÃ±o**: Necesitas un pipeline que se ejecute diariamente, lea de S3, transforme, y escriba a Redshift. El archivo crece 10% cada mes. Â¿UsarÃ­as Pandas? Â¿Por quÃ© sÃ­/no? Â¿QuÃ© alternativas considerarÃ­as?

---

*Ãšltima actualizaciÃ³n: Enero 2026 | VersiÃ³n: 1.0.0*

