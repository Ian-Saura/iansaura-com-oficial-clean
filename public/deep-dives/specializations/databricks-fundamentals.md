---
id: "spec-databricks-fundamentals"
version: "2.0.0"
lastUpdated: "2026-01-06"

# ACTUALIZADO: Enero 2026 - Free Edition (reemplaza Community Edition)
# - Serverless compute, Unity Catalog, DLT, Databricks Assistant incluidos
# - Solo Python y SQL en Free Edition (no R/Scala)

title:
  es: "Databricks: La Plataforma Unificada de Datos"
  en: "Databricks: The Unified Data Platform"
  pt: "Databricks: A Plataforma Unificada de Dados"

subtitle:
  es: "Spark + Delta Lake + Unity Catalog = El stack del futuro"
  en: "Spark + Delta Lake + Unity Catalog = The stack of the future"
  pt: "Spark + Delta Lake + Unity Catalog = A stack do futuro"

level: "specialization"
phase: "databricks"
estimatedTime: "60-80 horas"

prerequisites:
  - "l2-spark-distributed"
  - "l1-sql-advanced"

tags:
  - "databricks"
  - "spark"
  - "delta-lake"
  - "lakehouse"
  - "certification"

theoreticalFoundations:
  - "Arquitectura Lakehouse"
  - "ACID en Data Lakes"
  - "OptimizaciÃ³n de Spark"
  - "Data Governance"
  - "Unity Catalog"
---

<!-- 
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ðŸ“š ESPECIALIZACIÃ“N: DATABRICKS                              â•‘
â•‘  La plataforma unificada mÃ¡s demandada del mercado           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-->

# ðŸ”¶ Databricks: La Plataforma Unificada de Datos

> **Objetivo**: Dominar Databricks desde cero hasta la certificaciÃ³n DE Associate. Incluye Spark optimizado, Delta Lake, Unity Catalog, y Workflows.

---

## ðŸ§  Mapa Conceptual

```mermaid
mindmap
  root((Databricks
    Mastery))
    ðŸ—ï¸ Arquitectura
      Lakehouse
        Lo mejor de DW
        Lo mejor de Data Lake
        ACID + Schema
      Control Plane
        Workspace
        Notebooks
        Jobs
      Data Plane
        Clusters
        Storage
        Processing
    âš¡ Apache Spark
      DataFrames
        Transformations
        Actions
        Catalyst optimizer
      Spark SQL
        Tables
        Views
        Delta tables
      Performance
        AQE
        Caching
        Broadcast
    ðŸ’Ž Delta Lake
      ACID Transactions
        Atomicity
        Consistency
        Isolation
        Durability
      Time Travel
        Versioning
        Audit
        Rollback
      Optimizations
        Z-Order
        OPTIMIZE
        VACUUM
    ðŸ” Unity Catalog
      Governance
        Data lineage
        Access control
        Audit logs
      Metastore
        3-level namespace
        Catalog.Schema.Table
      Permissions
        GRANT/REVOKE
        Row-level security
    ðŸ”„ Workflows
      Jobs
        Tasks
        Clusters
        Schedules
      DLT
        Pipelines
        Expectations
        CDC
```

---

## ðŸ”— First Principles: De la TeorÃ­a a la PrÃ¡ctica

| Concepto | QuÃ© significa | ImplementaciÃ³n en Databricks |
|----------|---------------|------------------------------|
| **Lakehouse** | Combinar DW (estructura, ACID) + Data Lake (escala, formatos) | Delta Lake aÃ±ade ACID y schema enforcement sobre Parquet en cloud storage. Lo mejor de ambos mundos. |
| **ACID en Data Lake** | Transacciones atÃ³micas sobre archivos | Delta mantiene transaction log. Writes son atÃ³micos. Reads son consistentes. No mÃ¡s archivos corruptos. |
| **Time Travel** | Acceder a versiones anteriores de datos | Delta guarda historial. `SELECT * FROM table VERSION AS OF 5` o `TIMESTAMP AS OF '2024-01-01'`. Audit y rollback fÃ¡cil. |
| **Unity Catalog** | Governance centralizada | Un metastore para todo: tablas, ML models, dashboards. Permisos, lineage, audit en un solo lugar. |
| **Photon** | Engine C++ para queries | Reemplaza JVM para operaciones SQL. 2-8x mÃ¡s rÃ¡pido para workloads analÃ­ticos. |

> [!IMPORTANT]
> ðŸ§  **First Principle clave**: Databricks es la evoluciÃ³n de Spark. AÃ±ade **Delta Lake** para reliability, **Unity Catalog** para governance, y **Photon** para performance. Es Spark con las rough edges pulidas.

---

## ðŸ“‹ Technical Cheat Sheet

### ðŸ–¥ï¸ Comandos CrÃ­ticos en Notebooks

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURACIÃ“N Y CONTEXTO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# SparkSession ya estÃ¡ disponible como 'spark'
# No necesitas crear SparkSession en Databricks

# Ver configuraciÃ³n del cluster
spark.conf.get("spark.executor.memory")

# Ver catÃ¡logos disponibles (Unity Catalog)
spark.catalog.listCatalogs()

# Cambiar a un catÃ¡logo/schema
spark.sql("USE CATALOG my_catalog")
spark.sql("USE SCHEMA my_schema")

# O con 3-level namespace
spark.sql("USE my_catalog.my_schema")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LECTURA Y ESCRITURA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Leer Delta table
df = spark.read.table("catalog.schema.table_name")
df = spark.table("my_table")  # Si ya estÃ¡s en el schema

# Leer desde path
df = spark.read.format("delta").load("/path/to/delta")
df = spark.read.parquet("/path/to/parquet")
df = spark.read.csv("/path/to/csv", header=True, inferSchema=True)

# Escribir como Delta table (managed)
df.write.mode("overwrite").saveAsTable("my_table")

# Escribir como Delta (external)
df.write.format("delta").mode("overwrite").save("/path/to/delta")

# Merge (upsert)
from delta.tables import DeltaTable

target = DeltaTable.forName(spark, "target_table")
target.alias("t").merge(
    source_df.alias("s"),
    "t.id = s.id"
).whenMatchedUpdate(
    set={"name": "s.name", "updated_at": "current_timestamp()"}
).whenNotMatchedInsert(
    values={"id": "s.id", "name": "s.name", "created_at": "current_timestamp()"}
).execute()
```

### ðŸ“ Snippets de Alta Densidad

#### PatrÃ³n 1: Delta Lake Operations

```python
# ðŸ”¥ BEST PRACTICE: Operaciones Delta que debes dominar

from delta.tables import DeltaTable

# â•â•â• TIME TRAVEL â•â•â•
# Por versiÃ³n
df_v5 = spark.read.format("delta").option("versionAsOf", 5).load("/path/delta")

# Por timestamp
df_yesterday = spark.sql("""
    SELECT * FROM my_table TIMESTAMP AS OF '2024-01-15 10:00:00'
""")

# Ver historial
spark.sql("DESCRIBE HISTORY my_table").show()

# Restaurar a versiÃ³n anterior
spark.sql("RESTORE TABLE my_table TO VERSION AS OF 5")

# â•â•â• OPTIMIZE â•â•â•
# Compactar archivos pequeÃ±os (critical para performance)
spark.sql("OPTIMIZE my_table")

# Con Z-ORDER para queries frecuentes
spark.sql("OPTIMIZE my_table ZORDER BY (date, region)")

# â•â•â• VACUUM â•â•â•
# Eliminar archivos viejos (default: 7 dÃ­as retention)
spark.sql("VACUUM my_table")  # Dry run primero
spark.sql("VACUUM my_table RETAIN 168 HOURS")  # 7 dÃ­as

# âš ï¸ CUIDADO: DespuÃ©s de VACUUM, no puedes hacer time travel a versiones eliminadas

# â•â•â• SCHEMA EVOLUTION â•â•â•
# Agregar columnas automÃ¡ticamente
df.write.option("mergeSchema", "true").mode("append").saveAsTable("my_table")

# O permitir overwrite de schema
df.write.option("overwriteSchema", "true").mode("overwrite").saveAsTable("my_table")
```

#### PatrÃ³n 2: OptimizaciÃ³n de Queries

```python
# ðŸ”¥ BEST PRACTICE: Performance tuning en Databricks

# â•â•â• CACHING â•â•â•
# Cache en memoria (para reutilizaciÃ³n)
df.cache()
df.count()  # Materializar cache

# Uncache cuando no se necesita
df.unpersist()

# â•â•â• BROADCAST â•â•â•
from pyspark.sql.functions import broadcast

# Forzar broadcast para tablas pequeÃ±as
df_result = df_large.join(broadcast(df_small), "key")

# Verificar en plan
df_result.explain()  # Debe mostrar BroadcastHashJoin

# â•â•â• PARTITION PRUNING â•â•â•
# Particionar por columnas de filtro frecuente
df.write.partitionBy("year", "month").saveAsTable("partitioned_table")

# Query se beneficia automÃ¡ticamente
df = spark.sql("SELECT * FROM partitioned_table WHERE year = 2024 AND month = 1")
# Solo lee particiones relevantes

# â•â•â• Z-ORDER â•â•â•
# Para columnas de filtro que no son partition columns
spark.sql("""
    OPTIMIZE my_table 
    ZORDER BY (customer_id, product_id)
""")
# Queries con WHERE customer_id = X serÃ¡n mucho mÃ¡s rÃ¡pidas

# â•â•â• ADAPTIVE QUERY EXECUTION (AQE) â•â•â•
# Habilitado por defecto en Databricks
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# AQE automÃ¡ticamente:
# - Combina particiones pequeÃ±as post-shuffle
# - Convierte sort-merge joins a broadcast si una tabla es pequeÃ±a
# - Maneja data skew
```

#### PatrÃ³n 3: Delta Live Tables (DLT)

```python
# ðŸ”¥ BEST PRACTICE: DLT para pipelines declarativos

import dlt
from pyspark.sql.functions import *

# â•â•â• STREAMING TABLE (append-only) â•â•â•
@dlt.table(
    comment="Raw events from Kafka",
    table_properties={"quality": "bronze"}
)
def raw_events():
    return (
        spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "...")
        .option("subscribe", "events")
        .load()
        .select(
            col("key").cast("string"),
            from_json(col("value").cast("string"), schema).alias("data"),
            col("timestamp")
        )
    )

# â•â•â• MATERIALIZED VIEW (con transformaciÃ³n) â•â•â•
@dlt.table(
    comment="Cleaned events",
    table_properties={"quality": "silver"}
)
@dlt.expect_or_drop("valid_user", "user_id IS NOT NULL")
@dlt.expect("valid_amount", "amount > 0")  # Solo warn, no drop
def cleaned_events():
    return (
        dlt.read_stream("raw_events")
        .select(
            "data.user_id",
            "data.event_type",
            "data.amount",
            "timestamp"
        )
        .withColumn("processed_at", current_timestamp())
    )

# â•â•â• AGGREGATED TABLE â•â•â•
@dlt.table(
    comment="Daily metrics",
    table_properties={"quality": "gold"}
)
def daily_metrics():
    return (
        dlt.read("cleaned_events")
        .groupBy(to_date("timestamp").alias("date"))
        .agg(
            count("*").alias("event_count"),
            sum("amount").alias("total_amount"),
            countDistinct("user_id").alias("unique_users")
        )
    )
```

#### PatrÃ³n 4: Unity Catalog

```sql
-- ðŸ”¥ BEST PRACTICE: Governance con Unity Catalog

-- â•â•â• CREAR ESTRUCTURA â•â•â•
CREATE CATALOG IF NOT EXISTS production;
CREATE SCHEMA IF NOT EXISTS production.sales;

-- â•â•â• PERMISOS â•â•â•
-- Dar acceso de lectura a un grupo
GRANT SELECT ON SCHEMA production.sales TO `data-analysts`;

-- Dar acceso completo a un usuario
GRANT ALL PRIVILEGES ON SCHEMA production.sales TO `john.doe@company.com`;

-- Ver permisos
SHOW GRANTS ON SCHEMA production.sales;

-- â•â•â• ROW-LEVEL SECURITY â•â•â•
-- Crear funciÃ³n de filtro
CREATE FUNCTION production.sales.region_filter()
RETURNS STRING
RETURN CASE 
    WHEN is_member('latam-team') THEN 'LATAM'
    WHEN is_member('emea-team') THEN 'EMEA'
    WHEN is_member('admin') THEN NULL  -- Sin filtro
    ELSE 'NONE'
END;

-- Aplicar a tabla
ALTER TABLE production.sales.orders
SET ROW FILTER production.sales.region_filter ON (region);

-- â•â•â• COLUMN MASKING â•â•â•
CREATE FUNCTION production.sales.mask_email(email STRING)
RETURNS STRING
RETURN CASE
    WHEN is_member('pii-access') THEN email
    ELSE CONCAT(LEFT(email, 3), '***@***.com')
END;

ALTER TABLE production.sales.customers
ALTER COLUMN email SET MASK production.sales.mask_email;

-- â•â•â• LINEAGE â•â•â•
-- Unity Catalog trackea lineage automÃ¡ticamente
-- Ver en UI: Catalog Explorer > Table > Lineage tab
```

### ðŸ—ï¸ Patrones de DiseÃ±o Aplicados

#### 1. Medallion Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MEDALLION ARCHITECTURE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   BRONZE (Raw)              SILVER (Cleaned)         GOLD (Business)   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ raw_events   â”‚   â”€â”€â”€â–¶  â”‚ cleaned_     â”‚   â”€â”€â”€â–¶  â”‚ daily_       â”‚   â”‚
â”‚   â”‚              â”‚         â”‚ events       â”‚         â”‚ metrics      â”‚   â”‚
â”‚   â”‚ - As-is from â”‚         â”‚              â”‚         â”‚              â”‚   â”‚
â”‚   â”‚   source     â”‚         â”‚ - Deduped    â”‚         â”‚ - Aggregated â”‚   â”‚
â”‚   â”‚ - Schema on  â”‚         â”‚ - Validated  â”‚         â”‚ - Business   â”‚   â”‚
â”‚   â”‚   read       â”‚         â”‚ - Enriched   â”‚         â”‚   logic      â”‚   â”‚
â”‚   â”‚ - Append onlyâ”‚         â”‚ - Typed      â”‚         â”‚ - Ready for  â”‚   â”‚
â”‚   â”‚              â”‚         â”‚              â”‚         â”‚   BI/ML      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚   CaracterÃ­sticas:                                                      â”‚
â”‚   - Cada capa en Delta Lake                                            â”‚
â”‚   - Bronze: RetenciÃ³n larga, schema flexible                           â”‚
â”‚   - Silver: RetenciÃ³n media, schema estricto                           â”‚
â”‚   - Gold: Optimizado para queries, Z-Order en columnas de filtro       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. Incremental Processing Pattern

```python
# PatrÃ³n para procesamiento incremental eficiente

from delta.tables import DeltaTable

def incremental_load(source_path: str, target_table: str, merge_keys: list):
    """
    Carga incremental con merge.
    Asume que source tiene columna 'updated_at'.
    """
    target = DeltaTable.forName(spark, target_table)
    
    # Obtener Ãºltima marca de agua
    max_updated = spark.sql(f"""
        SELECT COALESCE(MAX(updated_at), '1900-01-01') 
        FROM {target_table}
    """).collect()[0][0]
    
    # Leer solo datos nuevos
    new_data = (
        spark.read.parquet(source_path)
        .filter(f"updated_at > '{max_updated}'")
    )
    
    if new_data.count() == 0:
        print("No new data to process")
        return
    
    # Merge
    merge_condition = " AND ".join([f"t.{k} = s.{k}" for k in merge_keys])
    
    target.alias("t").merge(
        new_data.alias("s"),
        merge_condition
    ).whenMatchedUpdateAll(
    ).whenNotMatchedInsertAll(
    ).execute()
    
    # Optimizar si hay muchos archivos nuevos
    spark.sql(f"OPTIMIZE {target_table}")
```

### âš ï¸ Gotchas de Nivel Senior

> [!WARNING]
> **Gotcha #1: VACUUM y Time Travel**
> 
> VACUUM elimina archivos viejos. DespuÃ©s no puedes hacer time travel a esas versiones.
> 
> ```python
> # âŒ PELIGROSO si necesitas audit histÃ³rico
> spark.sql("VACUUM my_table RETAIN 0 HOURS")  # Elimina TODO
> 
> # âœ… Mantener retenciÃ³n apropiada
> # 7 dÃ­as es el default y mÃ­nimo seguro
> spark.sql("VACUUM my_table RETAIN 168 HOURS")
> 
> # âœ… Para compliance, considera retenciÃ³n mÃ¡s larga
> # O guarda snapshots en storage separado
> ```

> [!WARNING]
> **Gotcha #2: Small files problem**
> 
> Muchos archivos pequeÃ±os = queries lentas.
> 
> ```python
> # Ver nÃºmero y tamaÃ±o de archivos
> spark.sql("DESCRIBE DETAIL my_table").show()
> 
> # Si numFiles es muy alto y avgFileSize muy bajo:
> spark.sql("OPTIMIZE my_table")
> 
> # Configurar auto-optimize para escrituras frecuentes
> spark.sql("""
>     ALTER TABLE my_table 
>     SET TBLPROPERTIES (
>         'delta.autoOptimize.optimizeWrite' = 'true',
>         'delta.autoOptimize.autoCompact' = 'true'
>     )
> """)
> ```

> [!WARNING]
> **Gotcha #3: Cluster sizing**
> 
> Cluster muy grande = caro. Cluster muy pequeÃ±o = lento o falla.
> 
> ```
> REGLAS DE THUMB:
> 
> - 1 core = 1 task en paralelo
> - 4GB RAM por core mÃ­nimo para Spark
> - Para ETL batch: i3.xlarge o similar
> - Para SQL analytics: Photon habilitado
> 
> PARA ESTIMAR:
> - Datos a procesar / 128MB = nÃºmero mÃ­nimo de particiones
> - Particiones / 2 = cores mÃ­nimos para paralelismo
> 
> SIEMPRE:
> - Usar autoscaling para workloads variables
> - Usar spot instances para jobs tolerantes a fallos
> ```

---

## ðŸ“š BibliografÃ­a y CertificaciÃ³n

### ðŸ“– Recursos Oficiales

| Recurso | Tipo | Por quÃ© leerlo |
|---------|------|----------------|
| **Databricks Academy** | Cursos gratis | PreparaciÃ³n oficial para certificaciÃ³n |
| **Delta Lake Documentation** | Docs | Referencia completa de Delta |
| **Databricks Blog** | Blog tÃ©cnico | Best practices y features nuevas |

### ðŸŽ“ CertificaciÃ³n Databricks DE Associate

```
TEMAS DEL EXAMEN:

1. Databricks Lakehouse Platform (24%)
   - Arquitectura lakehouse
   - Delta Lake features
   - Photon

2. ELT with Spark SQL and Python (29%)
   - DataFrames y SQL
   - Delta Lake operations
   - Structured Streaming

3. Incremental Data Processing (22%)
   - Auto Loader
   - COPY INTO
   - Streaming

4. Production Pipelines (16%)
   - Delta Live Tables
   - Jobs
   - Scheduling

5. Data Governance (9%)
   - Unity Catalog
   - Permisos
   - Lineage

FORMATO:
- 45 preguntas
- 90 minutos
- 70% para aprobar
- MÃºltiple choice y mÃºltiple respuesta
```

### ðŸ“‹ Recursos de PrÃ¡ctica

- **Databricks Free Edition** - ðŸ”— [databricks.com/try-databricks-free](https://www.databricks.com/try-databricks-free) - Gratis para aprender (serverless, Unity Catalog, DLT, AI Assistant)
- **Databricks Academy** - ðŸ”— [databricks.com/learn/training](https://www.databricks.com/learn/training) - Cursos oficiales gratuitos
- **Delta Lake Documentation** - ðŸ”— [docs.delta.io](https://docs.delta.io/)

---

## âœ… Checklist de Dominio

Antes de la certificaciÃ³n, verifica que puedes:

- [ ] Explicar la arquitectura Lakehouse
- [ ] Crear y manipular Delta tables
- [ ] Usar Time Travel y RESTORE
- [ ] Optimizar tablas con OPTIMIZE y Z-ORDER
- [ ] Implementar Medallion architecture
- [ ] Escribir DLT pipelines con expectations
- [ ] Configurar Unity Catalog (catalogs, schemas, grants)
- [ ] Usar Auto Loader para ingesta incremental
- [ ] Crear y schedular Jobs
- [ ] Debuggear queries con Spark UI

---

*Ãšltima actualizaciÃ³n: Enero 2026 | VersiÃ³n: 2.0.0*

