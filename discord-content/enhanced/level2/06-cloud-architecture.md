---
id: "l2-cloud-architecture"
version: "1.0.0"
lastUpdated: "2026-01-05"

title:
  es: "Cloud Architecture: Patrones para Data"
  en: "Cloud Architecture: Patterns for Data"
  pt: "Cloud Architecture: PadrÃµes para Dados"

subtitle:
  es: "Multi-cloud, serverless, y cost optimization para pipelines"
  en: "Multi-cloud, serverless, and cost optimization for pipelines"
  pt: "Multi-cloud, serverless e otimizaÃ§Ã£o de custos para pipelines"

level: 2
phase: "l2-cloud"
estimatedTime: "22-28 horas"

prerequisites:
  - "l1-docker-containers"
  - "l1-python-fundamentals"

tags:
  - "cloud"
  - "aws"
  - "gcp"
  - "azure"
  - "serverless"
  - "finops"

theoreticalFoundations:
  - "Infrastructure as Code"
  - "Serverless computing"
  - "Object storage patterns"
  - "Cost optimization"
---

<!-- 
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ðŸ“š BLOQUE: CLOUD ARCHITECTURE                               â•‘
â•‘  Nivel: 2 | Fase: Cloud Data Platforms                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-->

# â˜ï¸ Cloud Architecture: Patrones para Data

> **Objetivo**: Dominar patrones de arquitectura cloud para data engineering. Serverless, object storage, y FinOps para pipelines eficientes.

---

## ðŸ§  Mapa Conceptual

```mermaid
mindmap
  root((Cloud Data
    Architecture))
    ðŸ”¬ Paradigms
      Serverless
        Event-driven
        Pay-per-use
        Auto-scale
      Containers
        Docker
        Kubernetes
        ECS/EKS/GKE
      Managed Services
        Glue/Dataflow
        BigQuery/Redshift
        EMR/Dataproc
    ðŸ“– Storage Patterns
      Object Storage
        S3/GCS/Blob
        Partitioning
        File formats
      Data Lake
        Raw/Curated/Refined
        Lakehouse
        Open formats
      Warehouse
        Columnar
        MPP
        Separation
    âš¡ Compute Patterns
      Batch
        Spark
        Glue Jobs
        Dataflow
      Streaming
        Kinesis/Pub-Sub
        Kafka
        Flink
      Serverless
        Lambda/Functions
        Event triggers
        Cold starts
    ðŸ’° FinOps
      Cost Monitoring
        Budgets
        Alerts
        Dashboards
      Optimization
        Right-sizing
        Spot instances
        Reserved capacity
      Showback
        Tagging
        Attribution
        Chargebacks
```

---

## ðŸ”— First Principles: De la TeorÃ­a a la PrÃ¡ctica

| Concepto | QuÃ© significa | ImplementaciÃ³n Cloud |
|----------|---------------|---------------------|
| **Serverless** | No manejas servidores, solo cÃ³digo | Lambda/Cloud Functions. Pagas por ejecuciÃ³n, no por uptime. |
| **Object Storage** | Almacenamiento inmutable por objetos | S3/GCS. Partitioning por prefijos, file formats optimizados. |
| **Data Lake** | Repositorio central de datos raw | Raw zone con datos sin procesar, curated zone con datos limpios. |
| **Lakehouse** | Lake + Warehouse capabilities | Delta Lake, Iceberg. ACID sobre object storage. |
| **FinOps** | Operaciones financieras de cloud | Monitoreo continuo, optimization, allocation de costos. |

> [!IMPORTANT]
> ðŸ§  **First Principle clave**: En cloud, **pagas por lo que usas**. Esto cambia fundamentalmente cÃ³mo diseÃ±as: optimizar para costo es tan importante como optimizar para performance.

---

## ðŸ“‹ Technical Cheat Sheet

### ðŸ–¥ï¸ AWS CLI para Data Engineering

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# S3 OPERATIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Listar buckets
aws s3 ls

# Listar objetos con prefijo (particiÃ³n)
aws s3 ls s3://my-bucket/data/year=2026/month=01/ --recursive

# Copiar local a S3
aws s3 cp data.parquet s3://my-bucket/raw/data.parquet

# Sync directorio (solo archivos nuevos/modificados)
aws s3 sync ./local-data s3://my-bucket/data/ --exclude "*.tmp"

# Copiar entre buckets/regiones
aws s3 cp s3://source-bucket/data.parquet s3://dest-bucket/data.parquet

# EstadÃ­sticas de bucket
aws s3 ls s3://my-bucket --recursive --human-readable --summarize

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GLUE OPERATIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Listar jobs
aws glue get-jobs --query "Jobs[].Name"

# Ejecutar job
aws glue start-job-run --job-name my-etl-job

# Ver estado de run
aws glue get-job-run --job-name my-etl-job --run-id jr_xxx

# Listar tablas en catalog
aws glue get-tables --database-name my_database

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LAMBDA OPERATIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Listar funciones
aws lambda list-functions --query "Functions[].FunctionName"

# Invocar funciÃ³n
aws lambda invoke \
  --function-name process-s3-event \
  --payload '{"bucket": "my-bucket", "key": "data.csv"}' \
  response.json

# Ver logs
aws logs tail /aws/lambda/process-s3-event --follow

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# COST EXPLORER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Costo Ãºltimos 30 dÃ­as por servicio
aws ce get-cost-and-usage \
  --time-period Start=2026-01-01,End=2026-01-31 \
  --granularity MONTHLY \
  --metrics "BlendedCost" \
  --group-by Type=DIMENSION,Key=SERVICE
```

### ðŸ“ Data Lake Structure

```
s3://company-data-lake/
â”‚
â”œâ”€â”€ raw/                          # ðŸ”´ RAW ZONE - Datos sin procesar
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â”œâ”€â”€ year=2026/
â”‚   â”‚   â”‚   â”œâ”€â”€ month=01/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ day=05/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ orders_20260105_001.json
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ orders_20260105_002.json
â”‚   â”‚
â”‚   â”œâ”€â”€ customers/
â”‚   â”‚   â””â”€â”€ full_export_20260105.csv
â”‚   â”‚
â”‚   â””â”€â”€ events/
â”‚       â””â”€â”€ year=2026/month=01/day=05/hour=10/
â”‚           â””â”€â”€ events.json.gz
â”‚
â”œâ”€â”€ curated/                      # ðŸŸ¡ CURATED ZONE - Limpio, validado
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ year=2026/month=01/
â”‚   â”‚       â””â”€â”€ data.parquet      # Formato optimizado
â”‚   â”‚
â”‚   â””â”€â”€ customers/
â”‚       â””â”€â”€ latest/
â”‚           â””â”€â”€ customers.parquet
â”‚
â”œâ”€â”€ refined/                      # ðŸŸ¢ REFINED ZONE - Agregados, marts
â”‚   â”œâ”€â”€ daily_sales/
â”‚   â”‚   â””â”€â”€ date=2026-01-05/
â”‚   â”‚       â””â”€â”€ daily_sales.parquet
â”‚   â”‚
â”‚   â””â”€â”€ customer_360/
â”‚       â””â”€â”€ customer_360.parquet
â”‚
â””â”€â”€ _metadata/                    # ðŸ“‹ Metadata y schemas
    â”œâ”€â”€ schemas/
    â”‚   â”œâ”€â”€ orders_v1.json
    â”‚   â””â”€â”€ customers_v2.json
    â””â”€â”€ manifests/
        â””â”€â”€ orders_manifest.json
```

### ðŸ“ Serverless ETL Pattern

```python
# lambda_handler.py
# ðŸ”¥ BEST PRACTICE: Lambda para procesamiento event-driven

import json
import boto3
import pandas as pd
from io import BytesIO
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3_client = boto3.client('s3')
glue_client = boto3.client('glue')

def lambda_handler(event, context):
    """
    Triggered by S3 event when new file lands in raw zone.
    Processes and moves to curated zone.
    """
    
    # Parsear evento S3
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    logger.info(f"Processing: s3://{bucket}/{key}")
    
    try:
        # 1. Leer archivo desde S3
        response = s3_client.get_object(Bucket=bucket, Key=key)
        df = pd.read_json(BytesIO(response['Body'].read()), lines=True)
        
        # 2. Transformaciones bÃ¡sicas
        df = clean_and_validate(df)
        
        # 3. Escribir a curated zone en Parquet
        curated_key = key.replace('raw/', 'curated/').replace('.json', '.parquet')
        
        buffer = BytesIO()
        df.to_parquet(buffer, index=False, engine='pyarrow')
        buffer.seek(0)
        
        s3_client.put_object(
            Bucket=bucket,
            Key=curated_key,
            Body=buffer.getvalue()
        )
        
        logger.info(f"Written to: s3://{bucket}/{curated_key}")
        
        # 4. Actualizar Glue Catalog (opcional)
        # trigger_glue_crawler(curated_key)
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': 'Success',
                'records_processed': len(df),
                'output_path': f"s3://{bucket}/{curated_key}"
            })
        }
        
    except Exception as e:
        logger.error(f"Error processing {key}: {str(e)}")
        raise e

def clean_and_validate(df: pd.DataFrame) -> pd.DataFrame:
    """Limpieza y validaciÃ³n bÃ¡sica"""
    
    # Remover duplicados
    df = df.drop_duplicates()
    
    # Normalizar columnas
    df.columns = [c.lower().replace(' ', '_') for c in df.columns]
    
    # Validar campos requeridos
    required_cols = ['order_id', 'customer_id', 'amount']
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Missing required column: {col}")
    
    # Filtrar registros invÃ¡lidos
    df = df[df['order_id'].notna()]
    df = df[df['amount'] >= 0]
    
    return df
```

### ðŸ“ Infrastructure as Code (Terraform)

```hcl
# main.tf
# ðŸ”¥ BEST PRACTICE: IaC para reproducibilidad

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# S3 DATA LAKE BUCKETS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

resource "aws_s3_bucket" "data_lake" {
  bucket = "${var.project_name}-data-lake-${var.environment}"
  
  tags = {
    Environment = var.environment
    Project     = var.project_name
    Team        = "data-engineering"
    CostCenter  = "DE-001"
  }
}

resource "aws_s3_bucket_lifecycle_configuration" "data_lake_lifecycle" {
  bucket = aws_s3_bucket.data_lake.id
  
  # Raw zone: mover a Glacier despuÃ©s de 90 dÃ­as
  rule {
    id     = "raw-to-glacier"
    status = "Enabled"
    
    filter {
      prefix = "raw/"
    }
    
    transition {
      days          = 90
      storage_class = "GLACIER"
    }
    
    expiration {
      days = 365
    }
  }
  
  # Curated zone: mantener en Standard por mÃ¡s tiempo
  rule {
    id     = "curated-lifecycle"
    status = "Enabled"
    
    filter {
      prefix = "curated/"
    }
    
    transition {
      days          = 180
      storage_class = "STANDARD_IA"
    }
  }
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GLUE CATALOG DATABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

resource "aws_glue_catalog_database" "main" {
  name = "${var.project_name}_${var.environment}"
  
  create_table_default_permission {
    permissions = ["SELECT"]
  }
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LAMBDA FUNCTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

resource "aws_lambda_function" "process_s3" {
  function_name = "${var.project_name}-process-s3-${var.environment}"
  role          = aws_iam_role.lambda_role.arn
  handler       = "lambda_handler.lambda_handler"
  runtime       = "python3.11"
  timeout       = 300
  memory_size   = 1024
  
  filename         = "lambda.zip"
  source_code_hash = filebase64sha256("lambda.zip")
  
  environment {
    variables = {
      ENVIRONMENT = var.environment
      BUCKET_NAME = aws_s3_bucket.data_lake.id
    }
  }
  
  tags = {
    Environment = var.environment
    CostCenter  = "DE-001"
  }
}

# Trigger Lambda on S3 events
resource "aws_s3_bucket_notification" "lambda_trigger" {
  bucket = aws_s3_bucket.data_lake.id
  
  lambda_function {
    lambda_function_arn = aws_lambda_function.process_s3.arn
    events              = ["s3:ObjectCreated:*"]
    filter_prefix       = "raw/"
    filter_suffix       = ".json"
  }
}
```

### âš ï¸ Gotchas de Nivel Senior

> [!WARNING]
> **Gotcha #1: Lambda cold starts**
> 
> Funciones que no se ejecutan regularmente tienen cold starts de segundos.
> 
> ```python
> # âœ… Provisioned concurrency para funciones crÃ­ticas
> # En terraform:
> # provisioned_concurrent_executions = 5
> 
> # âœ… Mantener lambdas warm con scheduled events
> ```

> [!WARNING]
> **Gotcha #2: S3 listing es O(n)**
> 
> Listar millones de objetos es lento y costoso.
> 
> ```python
> # âŒ LENTO - List completo
> s3_client.list_objects_v2(Bucket='bucket', Prefix='data/')
> 
> # âœ… MEJOR - Usar manifests o Glue Catalog
> glue_client.get_partitions(DatabaseName='db', TableName='table')
> ```

> [!WARNING]
> **Gotcha #3: S3 Eventual Consistency (legacy)**
> 
> Desde 2020 S3 es strongly consistent, pero patrones legacy aÃºn existen.
> 
> ```python
> # âœ… S3 ahora es strongly consistent para PUT/GET
> # Ya no necesitas workarounds como sleep o check-then-read
> ```

> [!WARNING]
> **Gotcha #4: Cross-region data transfer costs**
> 
> Mover datos entre regiones es caro.
> 
> ```python
> # âŒ CARO - Procesar en regiÃ³n diferente a datos
> # Lambda en us-east-1, datos en eu-west-1
> 
> # âœ… BARATO - Colocar compute cerca de datos
> # Lambda y S3 en misma regiÃ³n
> ```

---

## ðŸ“Š Comparativa de Servicios Cloud

| Servicio | AWS | GCP | Azure | CuÃ¡ndo usar |
|----------|-----|-----|-------|-------------|
| **Object Storage** | S3 | GCS | Blob Storage | Data lake, archivos |
| **Warehouse** | Redshift | BigQuery | Synapse | Analytics SQL |
| **ETL Managed** | Glue | Dataflow | Data Factory | Pipelines batch |
| **Serverless** | Lambda | Cloud Functions | Functions | Event processing |
| **Streaming** | Kinesis | Pub/Sub | Event Hubs | Real-time |
| **Catalog** | Glue Catalog | Data Catalog | Purview | Metadata |

---

## ðŸ“š BibliografÃ­a AcadÃ©mica y Profesional

### ðŸ“– Recursos Seminales

| Recurso | Autor | Por quÃ© consumirlo |
|---------|-------|-------------------|
| **Fundamentals of Data Engineering** | Reis & Housley | Perspectiva moderna de arquitectura. |
| **Data Engineering on AWS** | Gareth Eagar | Patrones especÃ­ficos de AWS. |
| **Cloud FinOps** | J.R. Storment | GestiÃ³n de costos cloud. |

### ðŸ“„ Whitepapers Clave

1. **AWS Well-Architected Framework - Data Analytics Lens**
   - ðŸ”— [AWS Docs](https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/)
   - ðŸ’¡ **Insight clave**: Best practices oficiales de AWS.

2. **Google Cloud Architecture Framework**
   - ðŸ”— [cloud.google.com](https://cloud.google.com/architecture/framework)
   - ðŸ’¡ **Insight clave**: Patrones de diseÃ±o GCP.

---

## âœ… Checklist de Dominio

Antes de avanzar, verifica que puedes:

- [ ] DiseÃ±ar estructura de data lake con zonas raw/curated/refined
- [ ] Implementar partitioning eficiente en object storage
- [ ] Crear pipelines serverless con Lambda/Cloud Functions
- [ ] Escribir Infrastructure as Code con Terraform
- [ ] Configurar lifecycle policies para optimizar costos
- [ ] Monitorear costos con tagging y budgets
- [ ] Elegir entre servicios managed vs self-managed
- [ ] Implementar event-driven architecture con S3 events
- [ ] Optimizar para cold starts en serverless
- [ ] DiseÃ±ar para disaster recovery multi-region

---

*Ãšltima actualizaciÃ³n: Enero 2026 | VersiÃ³n: 1.0.0*

