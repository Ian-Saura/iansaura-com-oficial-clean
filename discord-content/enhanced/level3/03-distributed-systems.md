---
id: "l3-distributed-systems"
version: "1.0.0"
lastUpdated: "2026-01-05"

title:
  es: "Distributed Systems: Consenso y Replicaci√≥n"
  en: "Distributed Systems: Consensus and Replication"
  pt: "Sistemas Distribu√≠dos: Consenso e Replica√ß√£o"

subtitle:
  es: "Raft, Paxos, replicaci√≥n y el problema de los generales bizantinos"
  en: "Raft, Paxos, replication and the Byzantine generals problem"
  pt: "Raft, Paxos, replica√ß√£o e o problema dos generais bizantinos"

level: 3
phase: "l3-distributed"
estimatedTime: "30-40 horas"

prerequisites:
  - "l3-system-design"
  - "l2-kafka-streaming"

tags:
  - "distributed-systems"
  - "consensus"
  - "replication"
  - "raft"
  - "paxos"

theoreticalFoundations:
  - "CAP Theorem"
  - "Consensus algorithms"
  - "Failure modes"
  - "Byzantine fault tolerance"
---

<!-- 
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  üìö BLOQUE: DISTRIBUTED SYSTEMS                              ‚ïë
‚ïë  Nivel: 3 | Fase: Advanced Architecture                     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
-->

# üåê Distributed Systems: Consenso y Replicaci√≥n

> **Objetivo**: Entender los fundamentos te√≥ricos de sistemas distribuidos. CAP theorem, algoritmos de consenso, y patrones de replicaci√≥n usados en sistemas de datos.

---

## üß† Mapa Conceptual

```mermaid
mindmap
  root((Distributed
    Systems))
    üî¨ Fundamentals
      CAP Theorem
        Consistency
        Availability
        Partition Tolerance
      PACELC
        Latency trade-off
        Normal operation
        Partition state
      Failure Modes
        Crash-stop
        Crash-recovery
        Byzantine
    üìñ Consensus
      Paxos
        Prepare/Accept
        Multi-Paxos
        Complex
      Raft
        Leader election
        Log replication
        Understandable
      ZAB
        Zookeeper
        Primary-backup
        Total order
    ‚ö° Replication
      Leader-Follower
        Single writer
        Read replicas
        Failover
      Multi-Leader
        Write anywhere
        Conflict resolution
        CRDTs
      Leaderless
        Quorum
        Read repair
        Anti-entropy
    üèóÔ∏è Data Systems
      Databases
        PostgreSQL
        CockroachDB
        Spanner
      Message Queues
        Kafka
        Pulsar
        RabbitMQ
      Coordination
        ZooKeeper
        etcd
        Consul
```

---

## üîó First Principles: De la Teor√≠a a la Pr√°ctica

| Concepto | Qu√© significa | Sistemas que lo usan |
|----------|---------------|---------------------|
| **CAP Theorem** | Solo puedes elegir 2 de 3: Consistency, Availability, Partition tolerance | Todos los sistemas distribuidos. La partici√≥n es inevitable, eliges C o A. |
| **Consensus** | M√∫ltiples nodos acuerdan un valor | Raft en etcd/Consul, Paxos en Spanner, ZAB en ZooKeeper. |
| **Strong Consistency** | Todos ven el mismo valor al mismo tiempo | Spanner (TrueTime), CockroachDB. Alto costo de latencia. |
| **Eventual Consistency** | Eventualmente todos convergen al mismo valor | DynamoDB, Cassandra. Baja latencia, complejidad en app. |
| **Quorum** | Mayor√≠a necesaria para operaci√≥n | R + W > N para consistencia fuerte. Cassandra, Dynamo. |

> [!IMPORTANT]
> üß† **First Principle clave**: En sistemas distribuidos, **las fallas son la norma, no la excepci√≥n**. Dise√±a asumiendo que cualquier componente puede fallar en cualquier momento.

---

## üìã Technical Deep Dive

### üìê CAP Theorem Visualizado

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CAP THEOREM                            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ              Consistency                                    ‚îÇ
‚îÇ                  /\                                         ‚îÇ
‚îÇ                 /  \                                        ‚îÇ
‚îÇ                /    \                                       ‚îÇ
‚îÇ      CP      /      \      CA                              ‚îÇ
‚îÇ   Systems   /   ‚ùå   \   Systems                           ‚îÇ
‚îÇ            /  (Can't  \  (Impossible                       ‚îÇ
‚îÇ           /   have all)\  in practice)                     ‚îÇ
‚îÇ          /      3      \                                   ‚îÇ
‚îÇ         /________________\                                  ‚îÇ
‚îÇ   Availability ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Partition                         ‚îÇ
‚îÇ                          Tolerance                          ‚îÇ
‚îÇ              AP Systems                                     ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ CP: MongoDB, HBase, Redis Cluster, Spanner          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ AP: Cassandra, DynamoDB, CouchDB, Riak              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ CA: Single-node PostgreSQL (no distribution)        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìê Raft Algorithm Explained

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    RAFT CONSENSUS                           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                 1. LEADER ELECTION                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    [Follower] ‚îÄ‚îÄtimeout‚îÄ‚îÄ‚ñ∂ [Candidate]              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚ñ≤                      ‚îÇ                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                      ‚îÇ RequestVote        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                      ‚ñº                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    [Follower] ‚óÄ‚îÄ‚îÄmajority‚îÄ‚îÄ  [Leader]               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                   votes                              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                 2. LOG REPLICATION                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Client ‚îÄ‚îÄwrite‚îÄ‚îÄ‚ñ∂ [Leader]                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                         ‚îÇ                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                         ‚îÇ AppendEntries             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                         ‚ñº                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                    [Follower 1]                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                    [Follower 2]                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                         ‚îÇ                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                         ‚îÇ Majority ack              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                         ‚ñº                            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                    Commit entry                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                    Apply to state machine           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  Key Properties:                                           ‚îÇ
‚îÇ  ‚Ä¢ Only leader handles writes                              ‚îÇ
‚îÇ  ‚Ä¢ Entries committed when majority replicated             ‚îÇ
‚îÇ  ‚Ä¢ Heartbeats prevent new elections                       ‚îÇ
‚îÇ  ‚Ä¢ Log matching: same index+term = same command           ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìê Replication Strategies

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              REPLICATION STRATEGIES                         ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ LEADER-FOLLOWER (Single-Leader)                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     Writes ‚îÄ‚îÄ‚ñ∂ [Leader] ‚îÄ‚îÄreplicate‚îÄ‚îÄ‚ñ∂ [Follower]  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                   ‚îÇ                     [Follower]  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                   ‚îÇ                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ     Reads ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Simple, consistent writes                       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚ùå Leader bottleneck, failover complexity          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Examples: PostgreSQL, MySQL, MongoDB               ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ MULTI-LEADER                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    [Leader A] ‚óÄ‚îÄ‚îÄconflicts‚îÄ‚îÄ‚ñ∂ [Leader B]           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ        ‚îÇ                           ‚îÇ                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    [Follower]                 [Follower]            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚úÖ Geographic distribution, no single point         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚ùå Conflict resolution complexity                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Examples: CouchDB, Dynamo, multi-region DBs        ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ LEADERLESS (Quorum-based)                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    Client ‚îÄ‚îÄwrite‚îÄ‚îÄ‚ñ∂ [Node 1]                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ            ‚îÇ          [Node 2] (quorum W)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ            ‚îÇ          [Node 3]                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ            ‚îÇ                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ            ‚îî‚îÄ‚îÄread‚îÄ‚îÄ‚îÄ‚ñ∂ [Node 2]                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                        [Node 3] (quorum R)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ R + W > N for strong consistency                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚úÖ High availability, no leader election           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚ùå Complex, potential stale reads                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Examples: Cassandra, Riak, DynamoDB                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìù Quorum Mathematics

```python
# quorum_calculator.py
# üî• Entendiendo quorums para sistemas distribuidos

def analyze_quorum(n: int, w: int, r: int) -> dict:
    """
    Analiza configuraci√≥n de quorum
    
    Args:
        n: N√∫mero total de nodos
        w: Write quorum (nodos que deben confirmar write)
        r: Read quorum (nodos que deben responder read)
    
    Returns:
        An√°lisis de la configuraci√≥n
    """
    
    # Consistencia fuerte: R + W > N
    # Esto garantiza que cualquier read ve el write m√°s reciente
    strong_consistency = (r + w) > n
    
    # Tolerancia a fallas de escritura
    # Podemos perder (N - W) nodos y seguir escribiendo
    write_fault_tolerance = n - w
    
    # Tolerancia a fallas de lectura
    read_fault_tolerance = n - r
    
    # Ejemplos de configuraciones comunes
    configs = {
        "strong_consistency": strong_consistency,
        "write_fault_tolerance": write_fault_tolerance,
        "read_fault_tolerance": read_fault_tolerance,
        "latency_impact": "Higher W = slower writes, Higher R = slower reads"
    }
    
    return configs

# Ejemplos comunes
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# QUORUM (balanceado)
# N=3, W=2, R=2 ‚Üí R+W=4 > 3 ‚úì Strong consistency
# Tolera 1 falla para writes y reads
analyze_quorum(n=3, w=2, r=2)

# READ-HEAVY (optimizado para reads)  
# N=3, W=3, R=1 ‚Üí R+W=4 > 3 ‚úì Strong consistency
# Writes lentos (todos deben confirmar), reads r√°pidos
analyze_quorum(n=3, w=3, r=1)

# WRITE-HEAVY (optimizado para writes)
# N=3, W=1, R=3 ‚Üí R+W=4 > 3 ‚úì Strong consistency
# Writes r√°pidos, reads lentos
analyze_quorum(n=3, w=1, r=3)

# EVENTUAL CONSISTENCY
# N=3, W=1, R=1 ‚Üí R+W=2 < 3 ‚úó Eventual consistency
# Muy r√°pido pero puede leer datos stale
analyze_quorum(n=3, w=1, r=1)
```

### üìù Conflict Resolution Strategies

```python
# conflict_resolution.py
# üî• Estrategias para resolver conflictos en multi-leader/leaderless

from dataclasses import dataclass
from datetime import datetime
from typing import Any

@dataclass
class VersionedValue:
    value: Any
    timestamp: datetime
    node_id: str
    vector_clock: dict  # {node_id: counter}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ESTRATEGIA 1: Last Write Wins (LWW)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def resolve_lww(values: list[VersionedValue]) -> VersionedValue:
    """
    Last Write Wins - El timestamp m√°s reciente gana.
    
    ‚ö†Ô∏è PROBLEMA: Puede perder writes concurrentes.
    Usado por: Cassandra (con timestamps del cliente)
    """
    return max(values, key=lambda v: v.timestamp)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ESTRATEGIA 2: Vector Clocks + Application Resolution
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def detect_conflicts_vector_clock(a: dict, b: dict) -> str:
    """
    Detecta si hay conflicto usando vector clocks.
    
    Returns: 'a_newer', 'b_newer', o 'conflict'
    """
    a_newer = False
    b_newer = False
    
    all_keys = set(a.keys()) | set(b.keys())
    
    for key in all_keys:
        a_val = a.get(key, 0)
        b_val = b.get(key, 0)
        
        if a_val > b_val:
            a_newer = True
        elif b_val > a_val:
            b_newer = True
    
    if a_newer and b_newer:
        return 'conflict'  # Escrituras concurrentes
    elif a_newer:
        return 'a_newer'
    elif b_newer:
        return 'b_newer'
    else:
        return 'equal'

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ESTRATEGIA 3: CRDTs (Conflict-free Replicated Data Types)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class GCounter:
    """
    Grow-only Counter CRDT.
    Cada nodo incrementa su propio contador.
    El valor total es la suma.
    
    ‚úÖ VENTAJA: Merge siempre converge, sin conflictos.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.counts = {}  # {node_id: count}
    
    def increment(self, amount: int = 1):
        current = self.counts.get(self.node_id, 0)
        self.counts[self.node_id] = current + amount
    
    def value(self) -> int:
        return sum(self.counts.values())
    
    def merge(self, other: 'GCounter'):
        """Merge toma el m√°ximo de cada nodo - siempre converge"""
        all_keys = set(self.counts.keys()) | set(other.counts.keys())
        for key in all_keys:
            self.counts[key] = max(
                self.counts.get(key, 0),
                other.counts.get(key, 0)
            )

# Ejemplo de uso
# Node A: 5 incrementos
# Node B: 3 incrementos
# Despu√©s de merge: ambos ven 8
```

### ‚ö†Ô∏è Gotchas de Nivel Senior

> [!WARNING]
> **Gotcha #1: Clock Skew en LWW**
> 
> Relojes desincronizados causan p√©rdida de datos.
> 
> ```python
> # ‚ùå Si Node A tiene reloj 5 segundos adelante:
> # - Write de Node B (timestamp T) 
> # - Write de Node A (timestamp T+5) sobrescribe
> # - Datos de Node B perdidos aunque fue "despu√©s"
> 
> # ‚úÖ Usar Hybrid Logical Clocks (HLC) o TrueTime (Spanner)
> ```

> [!WARNING]
> **Gotcha #2: Split Brain en Leader Election**
> 
> Sin quorum, puedes tener dos leaders.
> 
> ```python
> # ‚ùå Network partition divide cluster en dos
> # Cada partici√≥n elige su propio leader
> # Writes conflictivos cuando se reconectan
> 
> # ‚úÖ Requiere mayor√≠a (quorum) para elegir leader
> # Partition sin mayor√≠a rechaza writes
> ```

> [!WARNING]
> **Gotcha #3: Linearizability vs Serializability**
> 
> Conceptos diferentes que se confunden.
> 
> ```python
> # Linearizability: Real-time ordering de operaciones
> # - Si A completa antes de que B empiece, todos ven A antes de B
> 
> # Serializability: Transacciones aparentan ejecutar en serie
> # - No garantiza orden real-time
> 
> # Strict Serializability = Linearizable + Serializable
> # (Lo que Spanner ofrece con TrueTime)
> ```

> [!WARNING]
> **Gotcha #4: Read-your-writes no es autom√°tico**
> 
> En sistemas eventualmente consistentes, puedes no ver tu propio write.
> 
> ```python
> # ‚ùå Usuario hace update, refresh, ve datos viejos
> write_to_leader(data)
> read_from_follower()  # ‚Üê Puede no tener el write a√∫n
> 
> # ‚úÖ Estrategias:
> # 1. Read-after-write: leer del leader despu√©s de write
> # 2. Session consistency: sticky sessions al mismo nodo
> # 3. Monotonic reads: siempre leer de nodo >= √∫ltimo le√≠do
> ```

---

## üìä Comparativa de Sistemas

| Sistema | Consensus | Consistency Model | Use Case |
|---------|-----------|-------------------|----------|
| **etcd** | Raft | Linearizable | Kubernetes coordination |
| **ZooKeeper** | ZAB | Linearizable | Distributed coordination |
| **Kafka** | ISR | Eventually consistent | Streaming |
| **Cassandra** | Quorum | Tunable (eventual to strong) | High-write workloads |
| **CockroachDB** | Raft | Serializable | Distributed SQL |
| **Spanner** | Paxos + TrueTime | External consistency | Global transactions |

---

## üìö Bibliograf√≠a Acad√©mica y Profesional

### üìñ Libros Seminales

| Libro | Autor | Por qu√© leerlo |
|-------|-------|----------------|
| **Designing Data-Intensive Applications** | Martin Kleppmann | LA biblia de sistemas distribuidos. Obligatorio. |
| **Distributed Systems** | Maarten van Steen | Textbook acad√©mico comprehensivo. |
| **Database Internals** | Alex Petrov | Deep dive en implementaci√≥n de DBs distribuidas. |

### üìÑ Papers Cl√°sicos

1. **"In Search of an Understandable Consensus Algorithm" (Raft)**
   - üîó [Raft Paper](https://raft.github.io/raft.pdf)
   - üí° **Insight clave**: Raft fue dise√±ado para ser entendible, a diferencia de Paxos.

2. **"The Part-Time Parliament" (Paxos)**
   - üîó [Lamport's Paxos](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)
   - üí° **Insight clave**: El paper original de Paxos, famosamente dif√≠cil de entender.

3. **"Dynamo: Amazon's Highly Available Key-value Store"**
   - üîó [Dynamo Paper](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)
   - üí° **Insight clave**: Dise√±o de DynamoDB, eventually consistent, quorum-based.

4. **"Spanner: Google's Globally-Distributed Database"**
   - üîó [Spanner Paper](https://research.google/pubs/pub39966/)
   - üí° **Insight clave**: TrueTime para external consistency global.

5. **"Time, Clocks, and the Ordering of Events"**
   - üîó [Lamport Clocks](https://lamport.azurewebsites.net/pubs/time-clocks.pdf)
   - üí° **Insight clave**: Fundamentos de ordenamiento en sistemas distribuidos.

---

## ‚úÖ Checklist de Dominio

Antes de avanzar, verifica que puedes:

- [ ] Explicar CAP theorem y sus implicaciones pr√°cticas
- [ ] Describir c√≥mo funciona Raft (leader election + log replication)
- [ ] Calcular quorums para diferentes guarantees (R + W > N)
- [ ] Diferenciar entre strong y eventual consistency
- [ ] Explicar vector clocks y c√≥mo detectan conflictos
- [ ] Describir estrategias de conflict resolution (LWW, CRDTs)
- [ ] Diferenciar entre linearizability y serializability
- [ ] Explicar por qu√© partition tolerance es obligatorio
- [ ] Identificar consistency model de sistemas como Kafka, Cassandra, Spanner
- [ ] Dise√±ar sistemas considerando failure modes

---

*√öltima actualizaci√≥n: Enero 2026 | Versi√≥n: 1.0.0*

