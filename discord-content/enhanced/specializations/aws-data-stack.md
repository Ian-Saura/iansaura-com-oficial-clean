---
id: "spec-aws-data-stack"
version: "1.0.0"
lastUpdated: "2026-01-05"

title:
  es: "AWS Data Stack: S3, Glue y Redshift"
  en: "AWS Data Stack: S3, Glue and Redshift"
  pt: "AWS Data Stack: S3, Glue e Redshift"

subtitle:
  es: "Arquitectura serverless de datos en AWS y patrones de costo-eficiencia"
  en: "Serverless data architecture in AWS and cost-efficiency patterns"
  pt: "Arquitetura serverless de dados em AWS e padrÃµes de custo-eficiÃªncia"

level: "specialization"
phase: "spec-aws"
estimatedTime: "35-45 horas"

prerequisites:
  - "l2-cloud-architecture"
  - "l2-spark-distributed"

tags:
  - "aws"
  - "s3"
  - "glue"
  - "redshift"
  - "athena"
  - "serverless"

theoreticalFoundations:
  - "Serverless architecture"
  - "Data lake patterns"
  - "ETL vs ELT"
  - "Columnar storage"
---

<!-- 
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ðŸ“š BLOQUE: AWS DATA STACK                                   â•‘
â•‘  EspecializaciÃ³n: AWS Data Engineering                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-->

# â˜ï¸ AWS Data Stack: S3, Glue y Redshift

> **Objetivo**: Dominar el stack de datos de AWS. S3 como data lake, Glue para ETL, Athena para queries ad-hoc, y Redshift como warehouse.

---

## ðŸ§  Mapa Conceptual

```mermaid
mindmap
  root((AWS Data
    Stack))
    ðŸ”¬ Storage Layer
      S3
        Object storage
        Partitioning
        File formats
        Lifecycle
      S3 Glacier
        Archive
        Retrieval tiers
    ðŸ“– Processing Layer
      AWS Glue
        ETL Jobs
        Crawlers
        Data Catalog
        Spark runtime
      EMR
        Managed Hadoop
        Spark clusters
        Presto
      Lambda
        Serverless
        Event-driven
        Lightweight ETL
    âš¡ Analytics Layer
      Athena
        Serverless SQL
        Presto engine
        Pay per query
      Redshift
        Data warehouse
        Columnar
        RA3 nodes
      QuickSight
        BI Tool
        SPICE engine
        Dashboards
    ðŸ—ï¸ Orchestration
      Step Functions
        State machines
        Workflow
        Error handling
      MWAA
        Managed Airflow
        DAGs
        Operators
      EventBridge
        Event routing
        Schedules
        Cross-account
```

---

## ðŸ”— First Principles: De la TeorÃ­a a la PrÃ¡ctica

| Concepto | QuÃ© significa | Servicio AWS |
|----------|---------------|--------------|
| **Object Storage** | Almacenamiento por objetos, no archivos | S3: Durabilidad 99.999999999%, infinitely scalable. |
| **Serverless ETL** | Ejecuta transformaciones sin manejar servers | Glue Jobs: Pagas solo por DPU-hora de ejecuciÃ³n. |
| **Serverless SQL** | Query sin provisionar cluster | Athena: $5/TB escaneado. Ideal para exploraciÃ³n. |
| **Columnar Warehouse** | Optimizado para analytics (SELECT columns) | Redshift: CompresiÃ³n automÃ¡tica, distribuciÃ³n paralela. |
| **Data Catalog** | Metadata centralizado de todos los datasets | Glue Catalog: Schema discovery automÃ¡tico, integra con Athena/Redshift. |

> [!IMPORTANT]
> ðŸ§  **First Principle clave**: AWS cobra por **lo que usas**. Optimizar para costo = optimizar formatos de archivo, particionar datos, y elegir el servicio correcto para cada workload.

---

## ðŸ“‹ Technical Cheat Sheet

### ðŸ–¥ï¸ AWS CLI - S3 Operations

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# S3 DATA LAKE OPERATIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Crear bucket con configuraciÃ³n Ã³ptima
aws s3api create-bucket \
  --bucket my-data-lake-prod \
  --region us-east-1 \
  --create-bucket-configuration LocationConstraint=us-east-1

# Habilitar versioning (recomendado para producciÃ³n)
aws s3api put-bucket-versioning \
  --bucket my-data-lake-prod \
  --versioning-configuration Status=Enabled

# Sync con exclusiones
aws s3 sync ./data s3://my-bucket/raw/ \
  --exclude "*.tmp" \
  --exclude "*.log" \
  --storage-class STANDARD_IA

# Copiar con metadata
aws s3 cp data.parquet s3://my-bucket/curated/ \
  --metadata "source=etl-job-123,processed_at=2026-01-05"

# AnÃ¡lisis de storage por prefijo
aws s3 ls s3://my-bucket/raw/ --recursive --human-readable --summarize

# Configurar lifecycle (via JSON)
aws s3api put-bucket-lifecycle-configuration \
  --bucket my-data-lake-prod \
  --lifecycle-configuration file://lifecycle.json
```

### ðŸ“ S3 Partitioning Strategy

```
# ðŸ”¥ BEST PRACTICE: Partitioning Hive-style

s3://my-data-lake/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ orders/
â”‚       â”œâ”€â”€ year=2026/
â”‚       â”‚   â”œâ”€â”€ month=01/
â”‚       â”‚   â”‚   â”œâ”€â”€ day=01/
â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ orders_001.json.gz
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ orders_002.json.gz
â”‚       â”‚   â”‚   â””â”€â”€ day=02/
â”‚       â”‚   â”‚       â””â”€â”€ orders_001.json.gz
â”‚       â”‚   â””â”€â”€ month=02/
â”‚       â”‚       â””â”€â”€ ...
â”‚       â””â”€â”€ year=2025/
â”‚           â””â”€â”€ ...
â”‚
â””â”€â”€ curated/
    â””â”€â”€ orders/
        â”œâ”€â”€ year=2026/
        â”‚   â””â”€â”€ month=01/
        â”‚       â””â”€â”€ orders.parquet  # Archivos mÃ¡s grandes, menos archivos
        â””â”€â”€ ...

# âš ï¸ EVITAR:
# - Demasiadas particiones pequeÃ±as (>10K particiones)
# - Archivos muy pequeÃ±os (<128MB en Parquet)
# - Particiones por columnas de alta cardinalidad (user_id)

# âœ… RECOMENDADO:
# - Particionar por date (year/month/day)
# - Archivos de 128MB-1GB
# - Parquet con compresiÃ³n Snappy
```

### ðŸ“ AWS Glue ETL Job

```python
# glue_etl_job.py
# ðŸ”¥ BEST PRACTICE: Glue job optimizado

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql.functions import col, year, month, dayofmonth, current_timestamp

# InicializaciÃ³n
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'source_path', 'target_path'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXTRACT - Leer desde S3
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# OpciÃ³n 1: DynamicFrame (Glue native)
raw_dyf = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={
        "paths": [args['source_path']],
        "recurse": True,
        "groupFiles": "inPartition",  # Combinar archivos pequeÃ±os
        "groupSize": "134217728"       # 128MB
    },
    format="json",
    format_options={"multiLine": False}
)

# OpciÃ³n 2: Desde Glue Catalog
raw_dyf = glueContext.create_dynamic_frame.from_catalog(
    database="raw_db",
    table_name="orders",
    transformation_ctx="raw_orders"
)

# Convertir a Spark DataFrame para transformaciones complejas
df = raw_dyf.toDF()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSFORM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Limpieza y normalizaciÃ³n
df_clean = df \
    .dropDuplicates(["order_id"]) \
    .filter(col("order_id").isNotNull()) \
    .withColumn("order_amount", col("order_amount").cast("decimal(18,2)")) \
    .withColumn("order_status", col("order_status").lower()) \
    .withColumn("_processed_at", current_timestamp())

# Agregar columnas de particiÃ³n
df_partitioned = df_clean \
    .withColumn("year", year(col("order_date"))) \
    .withColumn("month", month(col("order_date"))) \
    .withColumn("day", dayofmonth(col("order_date")))

# Convertir de vuelta a DynamicFrame
output_dyf = DynamicFrame.fromDF(df_partitioned, glueContext, "output")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD - Escribir a S3 en Parquet particionado
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

glueContext.write_dynamic_frame.from_options(
    frame=output_dyf,
    connection_type="s3",
    connection_options={
        "path": args['target_path'],
        "partitionKeys": ["year", "month", "day"]
    },
    format="parquet",
    format_options={
        "compression": "snappy"
    }
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UPDATE CATALOG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# OpciÃ³n: Escribir directamente al catalog
glueContext.write_dynamic_frame.from_catalog(
    frame=output_dyf,
    database="curated_db",
    table_name="orders_curated",
    additional_options={
        "enableUpdateCatalog": True,
        "updateBehavior": "UPDATE_IN_DATABASE",
        "partitionKeys": ["year", "month", "day"]
    }
)

job.commit()
```

### ðŸ“ Athena Queries Optimizadas

```sql
-- ðŸ”¥ BEST PRACTICE: Queries que minimizan costo

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SIEMPRE: Usar particiones en WHERE
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- âŒ CARO - Escanea TODO el dataset ($$$)
SELECT * FROM orders WHERE order_status = 'completed';

-- âœ… BARATO - Solo escanea particiones necesarias
SELECT * FROM orders 
WHERE year = 2026 AND month = 1  -- Partition pruning
  AND order_status = 'completed';

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- SIEMPRE: Seleccionar solo columnas necesarias
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- âŒ CARO - Lee todas las columnas
SELECT * FROM orders WHERE year = 2026;

-- âœ… BARATO - Solo lee columnas seleccionadas (Parquet columnar)
SELECT order_id, customer_id, order_amount 
FROM orders 
WHERE year = 2026;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- OPTIMIZACIÃ“N: CTAS para materializar
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Crear tabla materializada con formato optimizado
CREATE TABLE curated_db.orders_aggregated
WITH (
    format = 'PARQUET',
    parquet_compression = 'SNAPPY',
    partitioned_by = ARRAY['year', 'month'],
    external_location = 's3://my-bucket/curated/orders_agg/'
) AS
SELECT 
    customer_id,
    date_trunc('day', order_date) as order_day,
    COUNT(*) as order_count,
    SUM(order_amount) as total_amount,
    year(order_date) as year,
    month(order_date) as month
FROM raw_db.orders
GROUP BY 1, 2, year(order_date), month(order_date);

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- UNLOAD: Exportar resultados
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

UNLOAD (
    SELECT * FROM orders WHERE year = 2026 AND month = 1
)
TO 's3://my-bucket/exports/orders_jan/'
WITH (format = 'PARQUET', compression = 'SNAPPY');
```

### ðŸ“ Redshift Best Practices

```sql
-- ðŸ”¥ BEST PRACTICE: DiseÃ±o de tablas Redshift

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- DISTRIBUTION STYLES
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- EVEN: Default, distribuye uniformemente
-- KEY: Co-localiza rows con mismo key (para JOINs)
-- ALL: Copia completa en cada nodo (tablas pequeÃ±as)

-- Fact table: Distribution por key de JOIN frecuente
CREATE TABLE fct_orders (
    order_id BIGINT ENCODE az64,
    customer_id BIGINT ENCODE az64,
    product_id BIGINT ENCODE az64,
    order_date DATE ENCODE az64,
    order_amount DECIMAL(18,2) ENCODE az64,
    order_status VARCHAR(20) ENCODE bytedict
)
DISTKEY (customer_id)    -- Distribuir por customer para JOINs
SORTKEY (order_date)     -- Ordenar por fecha para range queries
;

-- Dimension pequeÃ±a: ALL distribution
CREATE TABLE dim_products (
    product_id BIGINT ENCODE az64,
    product_name VARCHAR(200) ENCODE lzo,
    category VARCHAR(100) ENCODE bytedict,
    brand VARCHAR(100) ENCODE bytedict
)
DISTSTYLE ALL            -- Copia en cada nodo
SORTKEY (product_id)
;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- COPY FROM S3
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COPY fct_orders
FROM 's3://my-bucket/curated/orders/'
IAM_ROLE 'arn:aws:iam::123456789:role/RedshiftLoadRole'
FORMAT PARQUET
;

-- Con manifest (lista explÃ­cita de archivos)
COPY fct_orders
FROM 's3://my-bucket/manifests/orders_manifest.json'
IAM_ROLE 'arn:aws:iam::123456789:role/RedshiftLoadRole'
FORMAT PARQUET
MANIFEST
;

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- VACUUM & ANALYZE
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- DespuÃ©s de DELETEs/UPDATEs: recuperar espacio
VACUUM FULL fct_orders;

-- DespuÃ©s de cargas: actualizar estadÃ­sticas
ANALYZE fct_orders;

-- Ver estado de vacuum
SELECT * FROM svv_table_info WHERE "table" = 'fct_orders';

-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-- QUERY MONITORING
-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

-- Queries lentas
SELECT 
    query,
    substring(querytxt, 1, 100) as query_preview,
    execution_time / 1000000.0 as seconds,
    queue_time / 1000000.0 as queue_seconds
FROM stl_query
WHERE execution_time > 60000000  -- > 60 segundos
ORDER BY execution_time DESC
LIMIT 20;

-- Espacio por tabla
SELECT 
    "schema" || '.' || "table" as tablename,
    size as size_mb,
    tbl_rows as rows
FROM svv_table_info
ORDER BY size DESC
LIMIT 20;
```

### âš ï¸ Gotchas de Nivel Senior

> [!WARNING]
> **Gotcha #1: S3 request costs**
> 
> Demasiados archivos pequeÃ±os = muchas requests = $$$
> 
> ```bash
> # âŒ CARO - 1 millÃ³n de archivos de 1KB
> # GET request = $0.0004 por 1000 â†’ $400 solo en reads
> 
> # âœ… BARATO - Consolidar en archivos de 128MB-1GB
> # Usar Glue groupFiles o compactaciÃ³n
> ```

> [!WARNING]
> **Gotcha #2: Athena query timeout**
> 
> Queries muy complejas pueden timeout (30 min default).
> 
> ```sql
> -- âŒ Query enorme que escanea todo
> SELECT * FROM huge_table WHERE col LIKE '%pattern%'
> 
> -- âœ… Dividir en queries mÃ¡s pequeÃ±as
> -- âœ… Pre-materializar con CTAS
> -- âœ… Mover a Redshift si es recurrente
> ```

> [!WARNING]
> **Gotcha #3: Glue Crawler schema conflicts**
> 
> Crawlers pueden detectar schemas incorrectos si datos son inconsistentes.
> 
> ```python
> # âŒ Crawler detecta "order_amount" como STRING en un archivo
> # y como DOUBLE en otro â†’ conflict
> 
> # âœ… Validar schema ANTES de landing zone
> # âœ… Usar SchemaChangePolicy especÃ­fica
> # âœ… Definir schema manualmente en casos crÃ­ticos
> ```

> [!WARNING]
> **Gotcha #4: Redshift DISTKEY mal elegido**
> 
> Skew de datos causa que un nodo haga todo el trabajo.
> 
> ```sql
> -- âŒ DISTKEY en columna con 90% de valores iguales
> -- Un nodo tiene 90% de los datos
>
> -- âœ… Elegir DISTKEY con distribuciÃ³n uniforme
> -- âœ… O usar EVEN distribution si no hay JOINs frecuentes
>
> -- Verificar skew
> SELECT slice, COUNT(*) 
> FROM stv_blocklist 
> WHERE tbl = (SELECT id FROM stv_tbl_perm WHERE name = 'fct_orders')
> GROUP BY slice;
> ```

---

## ðŸ“Š Comparativa de Servicios AWS Data

| Aspecto | Athena | Redshift | EMR | Glue |
|---------|--------|----------|-----|------|
| **Modelo** | Serverless | Provisioned | Provisioned | Serverless |
| **Costo** | $5/TB scan | $/hora nodo | $/hora cluster | DPU-hora |
| **Latencia** | Segundos | Sub-segundo | Variable | Minutos |
| **Caso de uso** | Ad-hoc, exploraciÃ³n | BI, dashboards | Big Data, ML | ETL batch |
| **Concurrencia** | Alta | Media (WLM) | Baja | Alta |

---

## ðŸ“š BibliografÃ­a AcadÃ©mica y Profesional

### ðŸ“– Recursos Seminales

| Recurso | Autor | Por quÃ© consumirlo |
|---------|-------|-------------------|
| **AWS Documentation** | AWS | DocumentaciÃ³n oficial, siempre actualizada. |
| **Data Engineering on AWS** | Gareth Eagar | Libro comprehensivo de O'Reilly. |
| **AWS Well-Architected Framework** | AWS | Best practices oficiales. |

### ðŸ“„ Whitepapers Clave

1. **AWS Well-Architected - Data Analytics Lens**
   - ðŸ”— [AWS Docs](https://docs.aws.amazon.com/wellarchitected/latest/analytics-lens/)
   - ðŸ’¡ **Insight clave**: Framework oficial para arquitecturas de datos.

2. **Best Practices for Amazon Redshift**
   - ðŸ”— [Redshift Best Practices](https://docs.aws.amazon.com/redshift/latest/dg/best-practices.html)
   - ðŸ’¡ **Insight clave**: DiseÃ±o de tablas, loading, querying.

3. **AWS Big Data Blog**
   - ðŸ”— [AWS Blog](https://aws.amazon.com/blogs/big-data/)
   - ðŸ’¡ **Insight clave**: Casos de uso y patrones actualizados.

---

## âœ… Checklist de Dominio

Antes de avanzar, verifica que puedes:

- [ ] DiseÃ±ar estructura de data lake en S3 con partitioning
- [ ] Configurar lifecycle policies para optimizaciÃ³n de costos
- [ ] Crear Glue ETL jobs con Spark
- [ ] Configurar Glue Crawlers y Data Catalog
- [ ] Escribir queries Athena optimizadas (partition pruning, columnar)
- [ ] DiseÃ±ar tablas Redshift con DISTKEY/SORTKEY correctos
- [ ] Cargar datos a Redshift con COPY
- [ ] Monitorear costos con Cost Explorer y budgets
- [ ] Orquestar pipelines con Step Functions
- [ ] Elegir entre Athena vs Redshift segÃºn caso de uso

---

*Ãšltima actualizaciÃ³n: Enero 2026 | VersiÃ³n: 1.0.0*

