---
id: "spec-aws-lambda-serverless"
version: "1.0.0"
lastUpdated: "2026-02-08"

title:
  es: "AWS Lambda para Data Engineering: Deep Dive Completo"
  en: "AWS Lambda for Data Engineering: Complete Deep Dive"
  pt: "AWS Lambda para Data Engineering: Deep Dive Completo"

subtitle:
  es: "Procesamiento serverless de datos con Lambda, S3 triggers y patrones ETL"
  en: "Serverless data processing with Lambda, S3 triggers and ETL patterns"
  pt: "Processamento serverless de dados com Lambda, S3 triggers e padrÃµes ETL"

level: "specialization"
phase: "spec-aws-lambda"
estimatedTime: "20-30 horas"

prerequisites:
  - "spec-aws-data-stack"
  - "l2-cloud-architecture"

tags:
  - "aws"
  - "lambda"
  - "serverless"
  - "s3-triggers"
  - "etl"
  - "data-engineering"

theoreticalFoundations:
  - "Serverless computing model"
  - "Event-driven architecture"
  - "ETL patterns"
  - "Cold start optimization"
---

<!-- 
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“š BLOQUE: AWS LAMBDA PARA DATA ENGINEERING               â•‘
â•‘  EspecializaciÃ³n: AWS Data Engineering                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
-->

# âš¡ AWS Lambda para Data Engineering: Deep Dive Completo

> **Objetivo**: Dominar AWS Lambda como motor de ingesta y transformaciÃ³n de datos. Desde S3 triggers hasta patrones ETL de producciÃ³n con manejo de errores y monitoreo.

---

## 1. Â¿Por quÃ© Lambda es el Motor de Ingesta #1?

Lambda es el servicio mÃ¡s utilizado para **ingesta event-driven** en pipelines de datos modernos. Cada vez que un archivo llega a S3, Lambda puede procesarlo automÃ¡ticamente sin necesidad de servidores.

### Arquitectura Event-Driven de Ingesta

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚         DATA PIPELINE               â”‚
                        â”‚         (Event-Driven)              â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    S3 Event     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Write      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚          â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚          â”‚
  â”‚  S3 Raw  â”‚   (ObjectCreated)â”‚  Lambda  â”‚  (Parquet)     â”‚ S3 Clean â”‚
  â”‚  Bucket  â”‚                 â”‚ Function â”‚                â”‚  Bucket  â”‚
  â”‚          â”‚                 â”‚          â”‚                â”‚          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â–²                            â”‚                           â”‚
       â”‚                            â”‚ Log / Error               â”‚ Query
   Upload                           â–¼                           â–¼
   (CSV, JSON)               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚CloudWatchâ”‚              â”‚    Athena    â”‚
                              â”‚  Logs    â”‚              â”‚   / Redshift â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ventajas Clave para Data Engineering

| CaracterÃ­stica         | Beneficio para Data                          |
|------------------------|----------------------------------------------|
| **Auto-scaling**       | 1 archivo o 10,000 â†’ Lambda escala solo      |
| **Pago por uso**       | $0 cuando no hay datos que procesar          |
| **Event-driven**       | Reacciona en segundos a nuevos archivos      |
| **Sin mantenimiento**  | No hay servidores, OS, ni parches            |
| **IntegraciÃ³n nativa** | S3, SQS, Kinesis, DynamoDB triggers          |

> **Dato clave**: El 90% de los pipelines de ingesta en startups y empresas medianas usan Lambda + S3 como primer paso.

---

## 2. Modelo de EjecuciÃ³n

### Ciclo de Vida de una InvocaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   COLD START                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Download â”‚  â”‚  Init Runtimeâ”‚  â”‚  Init Handler      â”‚    â”‚
â”‚  â”‚ Code     â”‚â”€â”€â”‚  (Python 3.x)â”‚â”€â”€â”‚  (imports, clients)â”‚    â”‚
â”‚  â”‚ (~50ms)  â”‚  â”‚  (~200ms)    â”‚  â”‚  (~100-500ms)      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   WARM INVOCATION                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚  handler(event, context)     â”‚  â† Solo esto se repite   â”‚
â”‚  â”‚  (~10-50ms overhead)         â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cold Start vs Warm Start

```python
# âœ… BUENA PRÃCTICA: Inicializar FUERA del handler
# Esto se ejecuta SOLO en cold start
import boto3
import pandas as pd

s3_client = boto3.client('s3')
glue_client = boto3.client('glue')

# ReutilizaciÃ³n de conexiÃ³n entre invocaciones
print("Cold start: clientes inicializados")


def handler(event, context):
    """
    Esto se ejecuta en CADA invocaciÃ³n (warm o cold).
    Los clientes de arriba se REUTILIZAN.
    """
    # Procesar evento...
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']

    return process_file(bucket, key)
```

### Provisioned Concurrency (Eliminar Cold Starts)

```python
# Para pipelines crÃ­ticos: configurar provisioned concurrency
# AWS CLI:
# aws lambda put-provisioned-concurrency-config \
#   --function-name data-ingestion \
#   --qualifier prod \
#   --provisioned-concurrent-executions 5
```

> **Tip**: Para Data Engineering, cold starts de ~1-2 segundos son aceptables. Usa Provisioned Concurrency solo si el SLA es < 500ms.

---

## 3. S3 Trigger: El PatrÃ³n MÃ¡s Importante

### Estructura del Evento S3

```json
{
  "Records": [
    {
      "eventVersion": "2.1",
      "eventSource": "aws:s3",
      "eventName": "ObjectCreated:Put",
      "eventTime": "2026-02-08T10:30:00.000Z",
      "s3": {
        "bucket": {
          "name": "raw-data-lake",
          "arn": "arn:aws:s3:::raw-data-lake"
        },
        "object": {
          "key": "incoming/ventas/2026/02/08/ventas_diarias.csv",
          "size": 15728640,
          "eTag": "a1b2c3d4e5f6"
        }
      }
    }
  ]
}
```

### Filtros de Prefijo y Sufijo

```
ConfiguraciÃ³n del trigger:
  â”œâ”€â”€ Prefix: "incoming/"      â†’ Solo archivos en incoming/
  â”œâ”€â”€ Suffix: ".csv"           â†’ Solo archivos CSV
  â””â”€â”€ Events: ObjectCreated    â†’ Solo creaciÃ³n, no borrado
```

> **Regla de oro**: SIEMPRE usar prefix y suffix para evitar invocaciones innecesarias.

### Evitar Loops Infinitos (Error ClÃ¡sico #1)

```
âŒ LOOP INFINITO:
S3 (raw/) â†’ Lambda â†’ Escribe en S3 (raw/) â†’ Trigger de nuevo â†’ Lambda â†’ ...

âœ… SOLUCIÃ“N 1: Buckets separados
S3 (raw-bucket) â†’ Lambda â†’ S3 (clean-bucket)   â† SIN trigger en clean

âœ… SOLUCIÃ“N 2: Prefijos diferentes
S3 (incoming/) â†’ Lambda â†’ S3 (processed/)
   â””â”€â”€ Trigger solo en incoming/

âœ… SOLUCIÃ“N 3: Sufijos diferentes
S3 (*.csv) â†’ Lambda â†’ S3 (*.parquet)
   â””â”€â”€ Trigger solo en .csv
```

```python
def handler(event, context):
    """Handler con protecciÃ³n anti-loop."""
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']

    # ProtecciÃ³n anti-loop: verificar prefijo
    if not key.startswith('incoming/'):
        print(f"SKIP: {key} no estÃ¡ en incoming/")
        return {'statusCode': 200, 'body': 'Skipped'}

    # ProtecciÃ³n adicional: verificar extensiÃ³n
    if not key.endswith('.csv'):
        print(f"SKIP: {key} no es CSV")
        return {'statusCode': 200, 'body': 'Skipped'}

    return process_file(bucket, key)
```

---

## 4. Handler Pattern para ETL (CSV â†’ Parquet)

### PatrÃ³n Completo de TransformaciÃ³n

```python
import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from io import BytesIO
from urllib.parse import unquote_plus
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3_client = boto3.client('s3')

CLEAN_BUCKET = 'clean-data-lake'


def handler(event, context):
    """
    ETL Lambda: CSV â†’ Parquet con validaciÃ³n.

    Trigger: S3 ObjectCreated en raw-data-lake/incoming/*.csv
    Output:  clean-data-lake/processed/YYYY/MM/DD/filename.parquet
    """
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        # Decodificar caracteres especiales en el key
        key = unquote_plus(record['s3']['object']['key'])
        size_mb = record['s3']['object']['size'] / (1024 * 1024)

        logger.info(f"Procesando: s3://{bucket}/{key} ({size_mb:.1f} MB)")

        try:
            # 1. EXTRACT: Leer CSV desde S3
            response = s3_client.get_object(Bucket=bucket, Key=key)
            df = pd.read_csv(response['Body'], encoding='utf-8')
            logger.info(f"Filas leÃ­das: {len(df)}, Columnas: {list(df.columns)}")

            # 2. TRANSFORM: Limpieza y validaciÃ³n
            df = transform_data(df)

            # 3. LOAD: Escribir Parquet a bucket limpio
            output_key = generate_output_key(key)
            write_parquet_to_s3(df, CLEAN_BUCKET, output_key)

            logger.info(f"Escrito: s3://{CLEAN_BUCKET}/{output_key}")

        except Exception as e:
            logger.error(f"Error procesando {key}: {str(e)}")
            raise  # Re-raise para que Lambda marque como fallo

    return {
        'statusCode': 200,
        'body': f'Procesados {len(event["Records"])} archivos'
    }


def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """Transformaciones de limpieza estÃ¡ndar."""
    # Normalizar nombres de columnas
    df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]

    # Eliminar duplicados
    initial_rows = len(df)
    df = df.drop_duplicates()
    logger.info(f"Duplicados eliminados: {initial_rows - len(df)}")

    # Convertir fechas
    date_columns = [col for col in df.columns if 'fecha' in col or 'date' in col]
    for col in date_columns:
        df[col] = pd.to_datetime(df[col], errors='coerce')

    # Eliminar filas completamente nulas
    df = df.dropna(how='all')

    # Agregar metadata de procesamiento
    df['_processed_at'] = pd.Timestamp.now()
    df['_source_file'] = 'lambda_etl'

    return df


def generate_output_key(input_key: str) -> str:
    """
    incoming/ventas/archivo.csv â†’ processed/ventas/2026/02/08/archivo.parquet
    """
    from datetime import datetime
    today = datetime.now()

    filename = input_key.split('/')[-1].replace('.csv', '.parquet')
    path_parts = input_key.replace('incoming/', '').rsplit('/', 1)[0]

    return f"processed/{path_parts}/{today.strftime('%Y/%m/%d')}/{filename}"


def write_parquet_to_s3(df: pd.DataFrame, bucket: str, key: str):
    """Escribir DataFrame como Parquet a S3."""
    buffer = BytesIO()
    table = pa.Table.from_pandas(df)
    pq.write_table(
        table,
        buffer,
        compression='snappy',  # Balance entre tamaÃ±o y velocidad
        use_dictionary=True,
        write_statistics=True
    )
    buffer.seek(0)

    s3_client.put_object(
        Bucket=bucket,
        Key=key,
        Body=buffer.getvalue(),
        ContentType='application/octet-stream'
    )
```

---

## 5. Secrets Manager Integration

### PatrÃ³n con Caching (Evitar llamadas repetidas)

```python
import boto3
import json
from functools import lru_cache

secrets_client = boto3.client('secretsmanager')


@lru_cache(maxsize=1)
def get_database_credentials(secret_name: str = 'prod/data-pipeline/redshift') -> dict:
    """
    Obtener credenciales con caching en memoria.
    En warm invocations, se reutiliza el cache (0 costo adicional).
    """
    response = secrets_client.get_secret_value(SecretId=secret_name)
    return json.loads(response['SecretString'])


def handler(event, context):
    """Handler que usa credenciales cacheadas."""
    # Primera invocaciÃ³n: llama a Secrets Manager (~50ms)
    # Siguientes invocaciones: usa cache (~0ms)
    creds = get_database_credentials()

    connection_string = (
        f"postgresql://{creds['username']}:{creds['password']}"
        f"@{creds['host']}:{creds['port']}/{creds['database']}"
    )

    # Usar la conexiÃ³n para cargar datos...
    load_to_redshift(connection_string, event)
```

### Uso de Lambda Extensions para Secrets (Avanzado)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Lambda Execution Env           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Tu CÃ³digo    â”‚  â”‚  Secrets Manager   â”‚  â”‚
â”‚  â”‚   (Handler)    â”‚â”€â”€â”‚  Extension         â”‚  â”‚
â”‚  â”‚                â”‚  â”‚  (cache local)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ventaja: Cache automÃ¡tico de secretos con TTL configurable.
ARN Layer: arn:aws:lambda:<region>:177933569100:layer:
           AWS-Parameters-and-Secrets-Lambda-Extension:11
```

---

## 6. Lambda Layers para Dependencias

### El Problema

Lambda viene con un runtime mÃ­nimo de Python. LibrerÃ­as como `pandas`, `pyarrow` o `numpy` **no estÃ¡n incluidas** y pesan > 50 MB.

### Crear un Layer con pandas + pyarrow

```bash
# 1. Crear directorio con estructura requerida
mkdir -p lambda-layer/python

# 2. Instalar dependencias para Amazon Linux 2 (arquitectura Lambda)
pip install pandas pyarrow \
  --target lambda-layer/python \
  --platform manylinux2014_x86_64 \
  --only-binary=:all:

# 3. Comprimir (mÃ¡ximo 250 MB descomprimido)
cd lambda-layer
zip -r9 pandas-pyarrow-layer.zip python/

# 4. Publicar layer
aws lambda publish-layer-version \
  --layer-name pandas-pyarrow \
  --description "pandas 2.x + pyarrow para ETL" \
  --zip-file fileb://pandas-pyarrow-layer.zip \
  --compatible-runtimes python3.11 python3.12
```

### Layers PÃºblicos Recomendados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer                  â”‚ TamaÃ±o  â”‚ Uso                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ AWSSDKPandas-Python312  â”‚ ~90 MB  â”‚ pandas + numpy        â”‚
â”‚  (AWS Data Wrangler)    â”‚         â”‚ + awswrangler         â”‚
â”‚ pyarrow                 â”‚ ~80 MB  â”‚ Lectura/escritura     â”‚
â”‚                         â”‚         â”‚ Parquet               â”‚
â”‚ psycopg2                â”‚ ~15 MB  â”‚ PostgreSQL/Redshift   â”‚
â”‚ requests                â”‚ ~2 MB   â”‚ APIs HTTP             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **AWS Data Wrangler** (awswrangler): Layer oficial de AWS que incluye pandas + integraciÃ³n nativa con S3, Glue, Athena, Redshift. Ideal para Data Engineering.

```python
# Con AWS Data Wrangler layer, tu cÃ³digo se simplifica:
import awswrangler as wr

def handler(event, context):
    # Leer CSV desde S3
    df = wr.s3.read_csv('s3://raw-bucket/incoming/data.csv')

    # Escribir Parquet directamente (con Glue Catalog opcional)
    wr.s3.to_parquet(
        df=df,
        path='s3://clean-bucket/processed/',
        dataset=True,
        database='my_database',     # Registrar en Glue Catalog
        table='my_table',
        mode='append',
        partition_cols=['year', 'month']
    )
```

---

## 7. Error Handling: DLQ, Destinations y SNS

### Estrategia de Errores para Pipelines de Datos

```
                    InvocaciÃ³n Lambda
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                    â”‚ Â¿Ã‰xito?   â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                   SÃ­/    â”‚    \No
                  /       â”‚     \
                 â–¼        â”‚      â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Success  â”‚    â”‚  â”‚ Retry x2     â”‚ â† AWS reintenta automÃ¡ticamente
          â”‚Destinationâ”‚   â”‚  â”‚ (async)      â”‚   en invocaciones asÃ­ncronas
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚          â”‚         â”‚
               â–¼          â”‚    Â¿Sigue fallando?
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚         â”‚
          â”‚ SNS/SQS  â”‚    â”‚         â–¼
          â”‚ Notificarâ”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Ã‰xito    â”‚    â”‚  â”‚ DLQ (SQS)    â”‚ â† Mensaje va a Dead Letter Queue
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ o Failure    â”‚
                          â”‚  â”‚ Destination  â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚         â”‚
                          â”‚         â–¼
                          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  â”‚ Alerta SNS   â”‚ â†’ Email/Slack/PagerDuty
                          â”‚  â”‚ + Reprocesar â”‚
                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configurar DLQ con CloudFormation/SAM

```yaml
# template.yaml (AWS SAM)
Resources:
  DataIngestionFunction:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: data-ingestion-etl
      Runtime: python3.12
      Handler: app.handler
      Timeout: 300
      MemorySize: 1024
      # Dead Letter Queue
      DeadLetterQueue:
        Type: SQS
        TargetArn: !GetAtt IngestionDLQ.Arn
      # Destinations (mÃ¡s modernas que DLQ)
      EventInvokeConfig:
        MaximumRetryAttempts: 2
        OnSuccess:
          Type: SNS
          Destination: !Ref SuccessTopic
        OnFailure:
          Type: SQS
          Destination: !GetAtt IngestionDLQ.Arn

  IngestionDLQ:
    Type: AWS::SQS::Queue
    Properties:
      QueueName: data-ingestion-dlq
      MessageRetentionPeriod: 1209600  # 14 dÃ­as

  SuccessTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: data-ingestion-success
```

### Manejo de Errores en CÃ³digo

```python
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)


class DataValidationError(Exception):
    """Error de validaciÃ³n que NO debe reintentarse."""
    pass


class TransientError(Exception):
    """Error transitorio que SÃ debe reintentarse."""
    pass


def handler(event, context):
    try:
        result = process_event(event)
        return {'statusCode': 200, 'body': result}

    except DataValidationError as e:
        # Error de datos â†’ no reintentar, enviar a DLQ directamente
        logger.error(f"ValidaciÃ³n fallida (no retry): {e}")
        # Guardar el archivo problemÃ¡tico en bucket de errores
        save_to_error_bucket(event, str(e))
        return {'statusCode': 400, 'body': str(e)}

    except TransientError as e:
        # Error transitorio â†’ dejar que Lambda reintente
        logger.warning(f"Error transitorio (retry): {e}")
        raise  # Re-raise = Lambda reintenta

    except Exception as e:
        # Error inesperado â†’ log detallado + raise
        logger.error(f"Error inesperado: {e}", exc_info=True)
        raise
```

---

## 8. Testing y Debugging

### Eventos de Test (Simular S3 Trigger)

```json
{
  "Records": [
    {
      "eventVersion": "2.1",
      "eventSource": "aws:s3",
      "eventName": "ObjectCreated:Put",
      "s3": {
        "bucket": {
          "name": "test-raw-bucket"
        },
        "object": {
          "key": "incoming/ventas/test_data.csv",
          "size": 1024
        }
      }
    }
  ]
}
```

### Unit Testing Local

```python
# test_handler.py
import pytest
import json
from unittest.mock import patch, MagicMock
from io import BytesIO
import pandas as pd


@pytest.fixture
def s3_event():
    """Evento S3 de prueba."""
    return {
        "Records": [{
            "s3": {
                "bucket": {"name": "test-bucket"},
                "object": {"key": "incoming/test.csv", "size": 100}
            }
        }]
    }


@pytest.fixture
def sample_csv():
    """CSV de prueba como bytes."""
    df = pd.DataFrame({
        'nombre': ['Ana', 'Luis', 'Ana'],
        'fecha': ['2026-01-01', '2026-01-02', '2026-01-01'],
        'monto': [100.5, 200.0, 100.5]
    })
    buffer = BytesIO()
    df.to_csv(buffer, index=False)
    buffer.seek(0)
    return buffer


@patch('app.s3_client')
def test_handler_csv_to_parquet(mock_s3, s3_event, sample_csv):
    """Verificar que CSV se convierte a Parquet correctamente."""
    # Arrange
    mock_s3.get_object.return_value = {'Body': sample_csv}

    # Act
    from app import handler
    result = handler(s3_event, None)

    # Assert
    assert result['statusCode'] == 200
    mock_s3.put_object.assert_called_once()
    call_args = mock_s3.put_object.call_args
    assert 'processed/' in call_args.kwargs['Key']
    assert call_args.kwargs['Key'].endswith('.parquet')
```

### CloudWatch Logs Insights Queries

```sql
-- Encontrar errores en las Ãºltimas 24 horas
fields @timestamp, @message
| filter @message like /ERROR/
| sort @timestamp desc
| limit 50

-- DuraciÃ³n promedio por funciÃ³n
fields @timestamp, @duration, @billedDuration, @memorySize, @maxMemoryUsed
| stats avg(@duration) as avg_duration,
        max(@duration) as max_duration,
        count(*) as invocations
| filter @type = "REPORT"

-- Cold starts vs warm starts
fields @timestamp, @initDuration
| stats count(*) as total,
        count(@initDuration) as cold_starts,
        avg(@initDuration) as avg_cold_start_ms
| filter @type = "REPORT"

-- Archivos procesados por hora
fields @timestamp, @message
| filter @message like /Procesando: s3/
| stats count(*) as archivos_por_hora by bin(1h)

-- Top 10 archivos mÃ¡s lentos
fields @timestamp, @duration, @message
| filter @message like /Procesando/
| sort @duration desc
| limit 10
```

### X-Ray Tracing (Trazabilidad End-to-End)

```python
# Activar X-Ray en la configuraciÃ³n de Lambda
# Y agregar el SDK:
from aws_xray_sdk.core import xray_recorder, patch_all

# Instrumentar automÃ¡ticamente boto3, requests, etc.
patch_all()


@xray_recorder.capture('process_csv')
def process_csv(bucket, key):
    """Esta funciÃ³n aparecerÃ¡ como subsegmento en X-Ray."""
    # ... procesamiento
    pass
```

---

## 9. LÃ­mites y CuÃ¡ndo Usar Fargate

### LÃ­mites de Lambda (2026)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Recurso                â”‚ LÃ­mite                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Timeout mÃ¡ximo         â”‚ 15 minutos              â”‚
â”‚  Memoria mÃ¡xima         â”‚ 10,240 MB (10 GB)       â”‚
â”‚  Almacenamiento /tmp    â”‚ 10,240 MB (10 GB)       â”‚
â”‚  Payload (sync)         â”‚ 6 MB                    â”‚
â”‚  Payload (async)        â”‚ 256 KB                  â”‚
â”‚  Concurrencia por cuentaâ”‚ 1,000 (default)         â”‚
â”‚  Package size (zip)     â”‚ 50 MB (250 MB descomp.) â”‚
â”‚  Container image        â”‚ 10 GB                   â”‚
â”‚  vCPUs (proporcional)   â”‚ 6 vCPUs (a 10 GB RAM)   â”‚
â”‚  Ephemeral storage      â”‚ 10 GB /tmp              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ãrbol de DecisiÃ³n: Lambda vs Fargate

```
Â¿Tu tarea necesita mÃ¡s de 15 minutos?
â”œâ”€â”€ SÃ â†’ Fargate
â””â”€â”€ NO
    â”œâ”€â”€ Â¿Necesita mÃ¡s de 10 GB RAM?
    â”‚   â”œâ”€â”€ SÃ â†’ Fargate
    â”‚   â””â”€â”€ NO
    â”‚       â”œâ”€â”€ Â¿El archivo pesa mÃ¡s de 10 GB?
    â”‚       â”‚   â”œâ”€â”€ SÃ â†’ Fargate (streaming con EFS o /tmp no alcanza)
    â”‚       â”‚   â””â”€â”€ NO
    â”‚       â”‚       â”œâ”€â”€ Â¿Necesita GPU?
    â”‚       â”‚       â”‚   â”œâ”€â”€ SÃ â†’ Fargate o EC2
    â”‚       â”‚       â”‚   â””â”€â”€ NO â†’ âœ… LAMBDA
    â”‚       â”‚       â””â”€â”€
    â”‚       â””â”€â”€
    â””â”€â”€
```

> **Regla prÃ¡ctica**: Si tu archivo cabe en memoria y se procesa en < 10 minutos â†’ **Lambda**. De lo contrario â†’ **Fargate**.

---

## 10. Preguntas de Entrevista

### Pregunta 1: Cold Start Optimization
**P**: Tu Lambda de ingesta tiene cold starts de 8 segundos que afectan el SLA. Â¿CÃ³mo lo resuelves?

**R**: MÃºltiples estrategias en orden de impacto:
1. **Reducir el tamaÃ±o del package**: Eliminar dependencias innecesarias, usar Layers.
2. **Provisioned Concurrency**: Mantener N instancias warm (costo fijo).
3. **Mover inicializaciÃ³n fuera del handler**: Clientes boto3, conexiones DB.
4. **Usar SnapStart** (Java) o **container images** optimizadas.
5. **Reducir memoria no ayuda**: MÃ¡s memoria = mÃ¡s CPU = cold start mÃ¡s rÃ¡pido.

---

### Pregunta 2: Loop Infinito en S3
**P**: Un junior configurÃ³ un Lambda que lee de S3 y escribe en el mismo bucket. El costo se disparÃ³ a $5,000 en una hora. Â¿QuÃ© pasÃ³ y cÃ³mo lo previenes?

**R**: Se creÃ³ un **loop infinito**: Lambda escribe â†’ S3 trigger â†’ Lambda se invoca de nuevo â†’ escribe â†’ trigger â†’ ... Soluciones:
1. **Buckets separados** para raw y processed (mejor opciÃ³n).
2. **Prefijos distintos** con filter en el trigger (`incoming/` â†’ `processed/`).
3. **Sufijos distintos** (`.csv` trigger â†’ escribe `.parquet`).
4. **Concurrency limit = 1** como freno de emergencia temporal.
5. Activar **billing alarm** en CloudWatch como red de seguridad.

---

### Pregunta 3: Archivos Grandes
**P**: Necesitas procesar un CSV de 8 GB en Lambda. Â¿Es posible? Â¿CÃ³mo?

**R**: SÃ­, es posible con la configuraciÃ³n adecuada:
1. **Lambda con 10 GB de memoria** y 10 GB de `/tmp`.
2. **Streaming desde S3** con `boto3` y lectura por chunks con `pd.read_csv(chunksize=...)`.
3. Escribir cada chunk procesado a S3 inmediatamente.
4. **Alternativa**: Si el procesamiento toma > 15 min, migrar a **Fargate**.

---

### Pregunta 4: Idempotencia
**P**: S3 puede enviar el mismo evento dos veces a Lambda. Â¿CÃ³mo garantizas que no proceses un archivo duplicado?

**R**: Implementar **idempotencia**:
1. Usar **DynamoDB** como registro: antes de procesar, verificar si el `s3_key + etag` ya existe.
2. Usar **S3 object metadata** para marcar archivos como procesados.
3. Escribir outputs con **nombre determinÃ­stico** (no timestamp random) â†’ sobreescritura es idempotente.
4. **Conditional writes** en DynamoDB con `ConditionExpression`.

---

### Pregunta 5: Monitoreo de Pipeline
**P**: Â¿CÃ³mo monitoreas un pipeline Lambda que procesa 50,000 archivos/dÃ­a?

**R**: Stack de monitoreo completo:
1. **CloudWatch Metrics**: Invocations, Errors, Duration, Throttles, ConcurrentExecutions.
2. **CloudWatch Alarms**: Error rate > 5%, Duration > 80% del timeout, Throttles > 0.
3. **CloudWatch Logs Insights**: Queries customizadas para patrones de error.
4. **X-Ray**: Trazabilidad end-to-end, bottlenecks en S3/Redshift calls.
5. **Custom Metrics**: Archivos procesados, filas transformadas, bytes escritos.
6. **Dashboard**: VisualizaciÃ³n unificada con mÃ©tricas de negocio.

---

## 11. Enlaces Oficiales

| Recurso | URL |
|---------|-----|
| Lambda Developer Guide | https://docs.aws.amazon.com/lambda/latest/dg/ |
| Lambda Best Practices | https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html |
| S3 Event Notifications | https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html |
| Lambda Layers | https://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html |
| Lambda Destinations | https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html |
| AWS Data Wrangler | https://aws-sdk-pandas.readthedocs.io/ |
| CloudWatch Logs Insights | https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html |
| Lambda Quotas | https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html |
| X-Ray SDK Python | https://docs.aws.amazon.com/xray/latest/devguide/xray-sdk-python.html |

---

> **Siguiente paso**: Una vez domines Lambda, avanza a **[ECS Fargate para Data Engineering](./aws-fargate-containers.md)** para procesar cargas que superan los lÃ­mites de Lambda.
