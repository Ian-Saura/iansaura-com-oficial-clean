/**
 * FASE 4: Spark Avanzado
 * 9 pasos para dominar t√©cnicas avanzadas de Spark
 */

import { DatabricksPhase } from '../types';

export const PHASE_4_SPARK_ADVANCED: DatabricksPhase = {
  id: 'db-phase-4',
  number: 4,
  title: { es: 'Spark Avanzado', en: 'Advanced Spark', pt: 'Spark Avan√ßado' },
  subtitle: { es: 'Optimizaci√≥n y t√©cnicas avanzadas', en: 'Optimization and advanced techniques', pt: 'Otimiza√ß√£o e t√©cnicas avan√ßadas' },
  description: { es: 'Llev√° tu conocimiento de Spark al siguiente nivel con UDFs, broadcast, particionamiento y debugging.', en: 'Take your Spark knowledge to the next level with UDFs, broadcast, partitioning and debugging.', pt: 'Leve seu conhecimento de Spark ao pr√≥ximo n√≠vel com UDFs, broadcast, particionamento e debugging.' },
  icon: 'üî•',
  color: 'red',
  estimatedDays: '5-6 d√≠as',
  steps: [
    {
      id: 'db-4-1',
      title: { es: 'User Defined Functions (UDFs)', en: 'User Defined Functions (UDFs)', pt: 'User Defined Functions (UDFs)' },
      description: { es: 'Cre√° funciones personalizadas para transformaciones complejas.', en: 'Create custom functions for complex transformations.', pt: 'Crie fun√ß√µes personalizadas para transforma√ß√µes complexas.' },
      theory: { es: `## UDFs en Spark\n\n\`\`\`python\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, IntegerType\n\n# UDF simple\n@udf(returnType=StringType())\ndef limpiar_texto(texto):\n    if texto is None:\n        return None\n    return texto.strip().lower()\n\ndf.withColumn("nombre_limpio", limpiar_texto("nombre"))\n\n# UDF con registro\ndef calcular_categoria(edad):\n    if edad < 18: return "menor"\n    elif edad < 65: return "adulto"\n    else: return "senior"\n\ncategorizar = udf(calcular_categoria, StringType())\ndf.withColumn("categoria", categorizar("edad"))\n\n# Pandas UDF (m√°s eficiente)\nfrom pyspark.sql.functions import pandas_udf\nimport pandas as pd\n\n@pandas_udf(StringType())\ndef upper_pandas(s: pd.Series) -> pd.Series:\n    return s.str.upper()\n\ndf.withColumn("nombre_upper", upper_pandas("nombre"))\n\`\`\`\n\n‚ö†Ô∏è Las UDFs son lentas. Prefer√≠ funciones built-in cuando sea posible.`, en: `## UDFs in Spark\n\n\`\`\`python\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, IntegerType\n\n# Simple UDF\n@udf(returnType=StringType())\ndef clean_text(text):\n    if text is None:\n        return None\n    return text.strip().lower()\n\ndf.withColumn("clean_name", clean_text("name"))\n\n# UDF with registration\ndef calculate_category(age):\n    if age < 18: return "minor"\n    elif age < 65: return "adult"\n    else: return "senior"\n\ncategorize = udf(calculate_category, StringType())\ndf.withColumn("category", categorize("age"))\n\n# Pandas UDF (more efficient)\nfrom pyspark.sql.functions import pandas_udf\nimport pandas as pd\n\n@pandas_udf(StringType())\ndef upper_pandas(s: pd.Series) -> pd.Series:\n    return s.str.upper()\n\ndf.withColumn("name_upper", upper_pandas("name"))\n\`\`\`\n\n‚ö†Ô∏è UDFs are slow. Prefer built-in functions when possible.`, pt: `## UDFs no Spark\n\n\`\`\`python\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, IntegerType\n\n# UDF simples\n@udf(returnType=StringType())\ndef limpar_texto(texto):\n    if texto is None:\n        return None\n    return texto.strip().lower()\n\ndf.withColumn("nome_limpo", limpar_texto("nome"))\n\n# UDF com registro\ndef calcular_categoria(idade):\n    if idade < 18: return "menor"\n    elif idade < 65: return "adulto"\n    else: return "senior"\n\ncategorizar = udf(calcular_categoria, StringType())\ndf.withColumn("categoria", categorizar("idade"))\n\n# Pandas UDF (mais eficiente)\nfrom pyspark.sql.functions import pandas_udf\nimport pandas as pd\n\n@pandas_udf(StringType())\ndef upper_pandas(s: pd.Series) -> pd.Series:\n    return s.str.upper()\n\ndf.withColumn("nome_upper", upper_pandas("nome"))\n\`\`\`\n\n‚ö†Ô∏è UDFs s√£o lentas. Prefira fun√ß√µes built-in quando poss√≠vel.` },
      practicalTips: [{ es: '‚ö° Pandas UDFs son 10-100x m√°s r√°pidas que UDFs normales.', en: '‚ö° Pandas UDFs are 10-100x faster than regular UDFs.', pt: '‚ö° Pandas UDFs s√£o 10-100x mais r√°pidas que UDFs normais.' }],
      externalLinks: [{ title: 'UDF Guide', url: 'https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øCreaste una Pandas UDF?', en: '‚úÖ Did you create a Pandas UDF?', pt: '‚úÖ Voc√™ criou uma Pandas UDF?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-4-2',
      title: { es: 'Broadcast Variables y Joins', en: 'Broadcast Variables and Joins', pt: 'Broadcast Variables e Joins' },
      description: { es: 'Optimiz√° joins con tablas peque√±as usando broadcast.', en: 'Optimize joins with small tables using broadcast.', pt: 'Otimize joins com tabelas pequenas usando broadcast.' },
      theory: { es: `## Broadcast en Spark\n\n\`\`\`python\nfrom pyspark.sql.functions import broadcast\n\n# Sin broadcast (shuffle de ambas tablas)\ndf_grande.join(df_peque√±o, "id")\n\n# Con broadcast (df_peque√±o se copia a todos los workers)\ndf_grande.join(broadcast(df_peque√±o), "id")\n\`\`\`\n\n### ¬øCu√°ndo usar?\n- Tabla peque√±a < 10MB (configuraci√≥n default)\n- Se puede aumentar: \`spark.sql.autoBroadcastJoinThreshold\`\n\n### Broadcast variables manuales:\n\`\`\`python\n# Para diccionarios o lookups\nlookup_dict = {"A": 1, "B": 2}\nbroadcast_lookup = spark.sparkContext.broadcast(lookup_dict)\n\n@udf(IntegerType())\ndef get_value(key):\n    return broadcast_lookup.value.get(key, 0)\n\`\`\``, en: `## Broadcast in Spark\n\n\`\`\`python\nfrom pyspark.sql.functions import broadcast\n\n# Without broadcast (shuffles both tables)\ndf_large.join(df_small, "id")\n\n# With broadcast (df_small is copied to all workers)\ndf_large.join(broadcast(df_small), "id")\n\`\`\`\n\n### When to use?\n- Small table < 10MB (default config)\n- Can increase: \`spark.sql.autoBroadcastJoinThreshold\`\n\n### Manual broadcast variables:\n\`\`\`python\n# For dictionaries or lookups\nlookup_dict = {"A": 1, "B": 2}\nbroadcast_lookup = spark.sparkContext.broadcast(lookup_dict)\n\n@udf(IntegerType())\ndef get_value(key):\n    return broadcast_lookup.value.get(key, 0)\n\`\`\``, pt: `## Broadcast no Spark\n\n\`\`\`python\nfrom pyspark.sql.functions import broadcast\n\n# Sem broadcast (shuffle de ambas tabelas)\ndf_grande.join(df_pequeno, "id")\n\n# Com broadcast (df_pequeno √© copiado para todos os workers)\ndf_grande.join(broadcast(df_pequeno), "id")\n\`\`\`\n\n### Quando usar?\n- Tabela pequena < 10MB (config default)\n- Pode aumentar: \`spark.sql.autoBroadcastJoinThreshold\`\n\n### Broadcast variables manuais:\n\`\`\`python\n# Para dicion√°rios ou lookups\nlookup_dict = {"A": 1, "B": 2}\nbroadcast_lookup = spark.sparkContext.broadcast(lookup_dict)\n\n@udf(IntegerType())\ndef get_value(key):\n    return broadcast_lookup.value.get(key, 0)\n\`\`\`` },
      practicalTips: [{ es: 'üí° Revis√° el plan de ejecuci√≥n con df.explain() para verificar que se use broadcast.', en: 'üí° Check the execution plan with df.explain() to verify broadcast is used.', pt: 'üí° Verifique o plano de execu√ß√£o com df.explain() para verificar que broadcast √© usado.' }],
      externalLinks: [{ title: 'Broadcast Joins', url: 'https://spark.apache.org/docs/latest/sql-performance-tuning.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øVerificaste con explain() que el join usa broadcast?', en: '‚úÖ Did you verify with explain() that the join uses broadcast?', pt: '‚úÖ Voc√™ verificou com explain() que o join usa broadcast?' },
      xpReward: 25,
      estimatedMinutes: 25
    },
    {
      id: 'db-4-3',
      title: { es: 'Particionamiento de Datos', en: 'Data Partitioning', pt: 'Particionamento de Dados' },
      description: { es: 'El particionamiento correcto puede acelerar tus queries 10x o m√°s.', en: 'Correct partitioning can speed up your queries 10x or more.', pt: 'O particionamento correto pode acelerar suas queries 10x ou mais.' },
      theory: { es: `## Particionamiento\n\n### Al escribir:\n\`\`\`python\n# Particionar por columnas\ndf.write.partitionBy("a√±o", "mes").parquet("path")\n\n# Resultado en disco:\n# path/a√±o=2024/mes=01/part-00000.parquet\n# path/a√±o=2024/mes=02/part-00000.parquet\n\`\`\`\n\n### Al leer (partition pruning):\n\`\`\`python\n# Solo lee la partici√≥n necesaria\ndf = spark.read.parquet("path").filter("a√±o = 2024 AND mes = 1")\n\`\`\`\n\n### Repartition vs Coalesce:\n\`\`\`python\n# Repartition: redistribuye datos (shuffle)\ndf.repartition(100)  # Aumentar particiones\ndf.repartition("columna")  # Por columna\n\n# Coalesce: reduce particiones (sin shuffle)\ndf.coalesce(10)  # Solo para reducir\n\`\`\`\n\n### Tips:\n- Particionar por columnas de filtro frecuente\n- Evitar muchas particiones peque√±as\n- Tama√±o ideal: 128MB-1GB por partici√≥n`, en: `## Partitioning\n\n### When writing:\n\`\`\`python\n# Partition by columns\ndf.write.partitionBy("year", "month").parquet("path")\n\n# Result on disk:\n# path/year=2024/month=01/part-00000.parquet\n# path/year=2024/month=02/part-00000.parquet\n\`\`\`\n\n### When reading (partition pruning):\n\`\`\`python\n# Only reads needed partition\ndf = spark.read.parquet("path").filter("year = 2024 AND month = 1")\n\`\`\`\n\n### Repartition vs Coalesce:\n\`\`\`python\n# Repartition: redistributes data (shuffle)\ndf.repartition(100)  # Increase partitions\ndf.repartition("column")  # By column\n\n# Coalesce: reduces partitions (no shuffle)\ndf.coalesce(10)  # Only to reduce\n\`\`\`\n\n### Tips:\n- Partition by frequently filtered columns\n- Avoid many small partitions\n- Ideal size: 128MB-1GB per partition`, pt: `## Particionamento\n\n### Ao escrever:\n\`\`\`python\n# Particionar por colunas\ndf.write.partitionBy("ano", "mes").parquet("path")\n\n# Resultado no disco:\n# path/ano=2024/mes=01/part-00000.parquet\n# path/ano=2024/mes=02/part-00000.parquet\n\`\`\`\n\n### Ao ler (partition pruning):\n\`\`\`python\n# S√≥ l√™ a parti√ß√£o necess√°ria\ndf = spark.read.parquet("path").filter("ano = 2024 AND mes = 1")\n\`\`\`\n\n### Repartition vs Coalesce:\n\`\`\`python\n# Repartition: redistribui dados (shuffle)\ndf.repartition(100)  # Aumentar parti√ß√µes\ndf.repartition("coluna")  # Por coluna\n\n# Coalesce: reduz parti√ß√µes (sem shuffle)\ndf.coalesce(10)  # S√≥ para reduzir\n\`\`\`\n\n### Dicas:\n- Particionar por colunas de filtro frequente\n- Evitar muitas parti√ß√µes pequenas\n- Tamanho ideal: 128MB-1GB por parti√ß√£o` },
      practicalTips: [{ es: '‚ö†Ô∏è Demasiadas particiones = archivos peque√±os = performance pobre. Encontr√° el balance.', en: '‚ö†Ô∏è Too many partitions = small files = poor performance. Find the balance.', pt: '‚ö†Ô∏è Muitas parti√ß√µes = arquivos pequenos = performance ruim. Encontre o equil√≠brio.' }],
      externalLinks: [{ title: 'Partition Pruning', url: 'https://docs.databricks.com/optimizations/dynamic-partition-pruning.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øGuardaste datos particionados y verificaste el partition pruning?', en: '‚úÖ Did you save partitioned data and verify partition pruning?', pt: '‚úÖ Voc√™ salvou dados particionados e verificou o partition pruning?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-4-4',
      title: { es: 'Caching y Persistencia', en: 'Caching and Persistence', pt: 'Caching e Persist√™ncia' },
      description: { es: 'Guard√° DataFrames en memoria para reutilizarlos eficientemente.', en: 'Store DataFrames in memory to reuse them efficiently.', pt: 'Guarde DataFrames em mem√≥ria para reutiliz√°-los eficientemente.' },
      theory: { es: `## Caching en Spark\n\n\`\`\`python\n# Cache en memoria\ndf.cache()  # Equivalente a persist(MEMORY_ONLY)\n\n# Diferentes niveles de persistencia\nfrom pyspark import StorageLevel\n\ndf.persist(StorageLevel.MEMORY_ONLY)      # Solo RAM\ndf.persist(StorageLevel.MEMORY_AND_DISK)  # RAM + disco si no entra\ndf.persist(StorageLevel.DISK_ONLY)        # Solo disco\ndf.persist(StorageLevel.MEMORY_ONLY_SER)  # RAM serializado (menos espacio)\n\n# Liberar cache\ndf.unpersist()\n\n# Ver qu√© est√° cacheado\nspark.catalog.clearCache()  # Limpiar todo\n\`\`\`\n\n### ¬øCu√°ndo usar cache?\n‚úÖ DataFrame usado m√∫ltiples veces\n‚úÖ Despu√©s de transformaciones costosas\n‚ùå DataFrames usados una sola vez\n‚ùå DataFrames muy grandes que no entran en memoria`, en: `## Caching in Spark\n\n\`\`\`python\n# Cache in memory\ndf.cache()  # Equivalent to persist(MEMORY_ONLY)\n\n# Different persistence levels\nfrom pyspark import StorageLevel\n\ndf.persist(StorageLevel.MEMORY_ONLY)      # RAM only\ndf.persist(StorageLevel.MEMORY_AND_DISK)  # RAM + disk if doesn't fit\ndf.persist(StorageLevel.DISK_ONLY)        # Disk only\ndf.persist(StorageLevel.MEMORY_ONLY_SER)  # Serialized RAM (less space)\n\n# Release cache\ndf.unpersist()\n\n# See what's cached\nspark.catalog.clearCache()  # Clear all\n\`\`\`\n\n### When to use cache?\n‚úÖ DataFrame used multiple times\n‚úÖ After expensive transformations\n‚ùå DataFrames used only once\n‚ùå Very large DataFrames that don't fit in memory`, pt: `## Caching no Spark\n\n\`\`\`python\n# Cache em mem√≥ria\ndf.cache()  # Equivalente a persist(MEMORY_ONLY)\n\n# Diferentes n√≠veis de persist√™ncia\nfrom pyspark import StorageLevel\n\ndf.persist(StorageLevel.MEMORY_ONLY)      # S√≥ RAM\ndf.persist(StorageLevel.MEMORY_AND_DISK)  # RAM + disco se n√£o couber\ndf.persist(StorageLevel.DISK_ONLY)        # S√≥ disco\ndf.persist(StorageLevel.MEMORY_ONLY_SER)  # RAM serializado (menos espa√ßo)\n\n# Liberar cache\ndf.unpersist()\n\n# Ver o que est√° cacheado\nspark.catalog.clearCache()  # Limpar tudo\n\`\`\`\n\n### Quando usar cache?\n‚úÖ DataFrame usado m√∫ltiplas vezes\n‚úÖ Depois de transforma√ß√µes custosas\n‚ùå DataFrames usados uma √∫nica vez\n‚ùå DataFrames muito grandes que n√£o cabem em mem√≥ria` },
      practicalTips: [{ es: 'üí° cache() es lazy - el cache real ocurre en la primera acci√≥n.', en: 'üí° cache() is lazy - actual caching happens on first action.', pt: 'üí° cache() √© lazy - o cache real acontece na primeira a√ß√£o.' }],
      externalLinks: [{ title: 'Caching', url: 'https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øCacheaste un DataFrame y verificaste la mejora de performance?', en: '‚úÖ Did you cache a DataFrame and verify the performance improvement?', pt: '‚úÖ Voc√™ cacheou um DataFrame e verificou a melhora de performance?' },
      xpReward: 25,
      estimatedMinutes: 20
    },
    {
      id: 'db-4-5',
      title: { es: 'Spark UI y Debugging', en: 'Spark UI and Debugging', pt: 'Spark UI e Debugging' },
      description: { es: 'Aprend√© a usar Spark UI para entender y optimizar tus jobs.', en: 'Learn to use Spark UI to understand and optimize your jobs.', pt: 'Aprenda a usar Spark UI para entender e otimizar seus jobs.' },
      theory: { es: `## Spark UI\n\nAcced√© desde el cluster en Databricks: Clusters > Tu cluster > Spark UI\n\n### Secciones principales:\n\n**Jobs**: Lista de jobs ejecutados\n- Tiempo de ejecuci√≥n\n- Stages completados\n- Tasks exitosos/fallidos\n\n**Stages**: Detalle de cada stage\n- Shuffle read/write\n- Tiempo por task\n- Skew de datos\n\n**Storage**: DataFrames cacheados\n- Tama√±o en memoria\n- Particiones\n\n**SQL**: Planes de ejecuci√≥n SQL\n- DAG visual\n- Tiempos por operaci√≥n\n\n### Qu√© buscar:\n- üî¥ Stages muy largos\n- üî¥ Shuffle excesivo\n- üî¥ Skew (tasks mucho m√°s lentas que otras)\n- üî¥ Spill a disco`, en: `## Spark UI\n\nAccess from cluster in Databricks: Clusters > Your cluster > Spark UI\n\n### Main sections:\n\n**Jobs**: List of executed jobs\n- Execution time\n- Completed stages\n- Successful/failed tasks\n\n**Stages**: Detail of each stage\n- Shuffle read/write\n- Time per task\n- Data skew\n\n**Storage**: Cached DataFrames\n- Size in memory\n- Partitions\n\n**SQL**: SQL execution plans\n- Visual DAG\n- Time per operation\n\n### What to look for:\n- üî¥ Very long stages\n- üî¥ Excessive shuffle\n- üî¥ Skew (tasks much slower than others)\n- üî¥ Spill to disk`, pt: `## Spark UI\n\nAcesse do cluster no Databricks: Clusters > Seu cluster > Spark UI\n\n### Se√ß√µes principais:\n\n**Jobs**: Lista de jobs executados\n- Tempo de execu√ß√£o\n- Stages completados\n- Tasks bem-sucedidas/falhadas\n\n**Stages**: Detalhe de cada stage\n- Shuffle read/write\n- Tempo por task\n- Skew de dados\n\n**Storage**: DataFrames cacheados\n- Tamanho em mem√≥ria\n- Parti√ß√µes\n\n**SQL**: Planos de execu√ß√£o SQL\n- DAG visual\n- Tempos por opera√ß√£o\n\n### O que procurar:\n- üî¥ Stages muito longos\n- üî¥ Shuffle excessivo\n- üî¥ Skew (tasks muito mais lentas que outras)\n- üî¥ Spill para disco` },
      practicalTips: [{ es: 'üîç El 80% de los problemas de performance se ven en Spark UI.', en: 'üîç 80% of performance issues are visible in Spark UI.', pt: 'üîç 80% dos problemas de performance s√£o vis√≠veis no Spark UI.' }],
      externalLinks: [{ title: 'Spark UI', url: 'https://spark.apache.org/docs/latest/web-ui.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øIdentificaste un cuello de botella en Spark UI?', en: '‚úÖ Did you identify a bottleneck in Spark UI?', pt: '‚úÖ Voc√™ identificou um gargalo no Spark UI?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-4-6',
      title: { es: 'Manejo de Skew de Datos', en: 'Handling Data Skew', pt: 'Lidando com Skew de Dados' },
      description: { es: 'El skew es uno de los problemas m√°s comunes. Aprend√© a detectarlo y solucionarlo.', en: 'Skew is one of the most common problems. Learn to detect and fix it.', pt: 'O skew √© um dos problemas mais comuns. Aprenda a detect√°-lo e corrigi-lo.' },
      theory: { es: `## Data Skew\n\nEl skew ocurre cuando algunos particiones tienen muchos m√°s datos que otras.\n\n### Detectar skew:\n\`\`\`python\n# Ver distribuci√≥n\ndf.groupBy("columna_join").count().orderBy("count", ascending=False).show()\n\`\`\`\n\n### Soluciones:\n\n**1. Salting (agregar ruido)**\n\`\`\`python\nfrom pyspark.sql.functions import rand, concat, lit\n\n# Agregar salt a la key\ndf_salted = df.withColumn("key_salted", \n    concat("key", lit("_"), (rand() * 10).cast("int")))\n\`\`\`\n\n**2. Broadcast si una tabla es peque√±a**\n\`\`\`python\ndf_grande.join(broadcast(df_peque√±o), "key")\n\`\`\`\n\n**3. AQE (Adaptive Query Execution)**\n\`\`\`python\nspark.conf.set("spark.sql.adaptive.enabled", "true")\nspark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")\n\`\`\``, en: `## Data Skew\n\nSkew occurs when some partitions have much more data than others.\n\n### Detect skew:\n\`\`\`python\n# View distribution\ndf.groupBy("join_column").count().orderBy("count", ascending=False).show()\n\`\`\`\n\n### Solutions:\n\n**1. Salting (add noise)**\n\`\`\`python\nfrom pyspark.sql.functions import rand, concat, lit\n\n# Add salt to key\ndf_salted = df.withColumn("key_salted", \n    concat("key", lit("_"), (rand() * 10).cast("int")))\n\`\`\`\n\n**2. Broadcast if one table is small**\n\`\`\`python\ndf_large.join(broadcast(df_small), "key")\n\`\`\`\n\n**3. AQE (Adaptive Query Execution)**\n\`\`\`python\nspark.conf.set("spark.sql.adaptive.enabled", "true")\nspark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")\n\`\`\``, pt: `## Data Skew\n\nO skew ocorre quando algumas parti√ß√µes t√™m muito mais dados que outras.\n\n### Detectar skew:\n\`\`\`python\n# Ver distribui√ß√£o\ndf.groupBy("coluna_join").count().orderBy("count", ascending=False).show()\n\`\`\`\n\n### Solu√ß√µes:\n\n**1. Salting (adicionar ru√≠do)**\n\`\`\`python\nfrom pyspark.sql.functions import rand, concat, lit\n\n# Adicionar salt √† key\ndf_salted = df.withColumn("key_salted", \n    concat("key", lit("_"), (rand() * 10).cast("int")))\n\`\`\`\n\n**2. Broadcast se uma tabela √© pequena**\n\`\`\`python\ndf_grande.join(broadcast(df_pequeno), "key")\n\`\`\`\n\n**3. AQE (Adaptive Query Execution)**\n\`\`\`python\nspark.conf.set("spark.sql.adaptive.enabled", "true")\nspark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")\n\`\`\`` },
      practicalTips: [{ es: 'üí° AQE est√° habilitado por default en Databricks Runtime 7.3+', en: 'üí° AQE is enabled by default in Databricks Runtime 7.3+', pt: 'üí° AQE est√° habilitado por padr√£o no Databricks Runtime 7.3+' }],
      externalLinks: [{ title: 'Skew Handling', url: 'https://docs.databricks.com/optimizations/skew-join.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øDetectaste skew y aplicaste una soluci√≥n?', en: '‚úÖ Did you detect skew and apply a solution?', pt: '‚úÖ Voc√™ detectou skew e aplicou uma solu√ß√£o?' },
      xpReward: 35,
      estimatedMinutes: 30
    },
    {
      id: 'db-4-7',
      title: { es: 'Structured Streaming B√°sico', en: 'Basic Structured Streaming', pt: 'Structured Streaming B√°sico' },
      description: { es: 'Introducci√≥n al procesamiento de datos en tiempo real con Spark.', en: 'Introduction to real-time data processing with Spark.', pt: 'Introdu√ß√£o ao processamento de dados em tempo real com Spark.' },
      theory: { es: `## Structured Streaming\n\n\`\`\`python\n# Leer stream (ejemplo con archivo)\ndf_stream = spark.readStream \\\n    .format("cloudFiles") \\\n    .option("cloudFiles.format", "json") \\\n    .schema(schema) \\\n    .load("path/to/input")\n\n# Transformaciones (igual que batch!)\ndf_transformed = df_stream \\\n    .filter(df_stream.value > 100) \\\n    .groupBy("category").count()\n\n# Escribir stream\nquery = df_transformed.writeStream \\\n    .format("delta") \\\n    .outputMode("complete") \\\n    .option("checkpointLocation", "/checkpoint") \\\n    .start("path/to/output")\n\n# Output modes:\n# - append: solo nuevas filas\n# - complete: toda la tabla (para agregaciones)\n# - update: filas modificadas\n\nquery.awaitTermination()\n\`\`\``, en: `## Structured Streaming\n\n\`\`\`python\n# Read stream (file example)\ndf_stream = spark.readStream \\\n    .format("cloudFiles") \\\n    .option("cloudFiles.format", "json") \\\n    .schema(schema) \\\n    .load("path/to/input")\n\n# Transformations (same as batch!)\ndf_transformed = df_stream \\\n    .filter(df_stream.value > 100) \\\n    .groupBy("category").count()\n\n# Write stream\nquery = df_transformed.writeStream \\\n    .format("delta") \\\n    .outputMode("complete") \\\n    .option("checkpointLocation", "/checkpoint") \\\n    .start("path/to/output")\n\n# Output modes:\n# - append: only new rows\n# - complete: entire table (for aggregations)\n# - update: modified rows\n\nquery.awaitTermination()\n\`\`\``, pt: `## Structured Streaming\n\n\`\`\`python\n# Ler stream (exemplo com arquivo)\ndf_stream = spark.readStream \\\n    .format("cloudFiles") \\\n    .option("cloudFiles.format", "json") \\\n    .schema(schema) \\\n    .load("path/to/input")\n\n# Transforma√ß√µes (igual que batch!)\ndf_transformed = df_stream \\\n    .filter(df_stream.value > 100) \\\n    .groupBy("category").count()\n\n# Escrever stream\nquery = df_transformed.writeStream \\\n    .format("delta") \\\n    .outputMode("complete") \\\n    .option("checkpointLocation", "/checkpoint") \\\n    .start("path/to/output")\n\n# Output modes:\n# - append: s√≥ novas linhas\n# - complete: toda a tabela (para agrega√ß√µes)\n# - update: linhas modificadas\n\nquery.awaitTermination()\n\`\`\`` },
      practicalTips: [{ es: 'üí° El c√≥digo de streaming es casi id√©ntico al de batch. Esa es la magia de Structured Streaming.', en: 'üí° Streaming code is almost identical to batch. That\'s the magic of Structured Streaming.', pt: 'üí° O c√≥digo de streaming √© quase id√™ntico ao de batch. Essa √© a magia do Structured Streaming.' }],
      externalLinks: [{ title: 'Structured Streaming', url: 'https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øEjecutaste un stream b√°sico y verificaste la salida?', en: '‚úÖ Did you run a basic stream and verify the output?', pt: '‚úÖ Voc√™ executou um stream b√°sico e verificou a sa√≠da?' },
      xpReward: 35,
      estimatedMinutes: 35
    },
    {
      id: 'db-4-8',
      title: { es: 'Configuraci√≥n de Spark', en: 'Spark Configuration', pt: 'Configura√ß√£o do Spark' },
      description: { es: 'Las configuraciones correctas pueden mejorar dr√°sticamente la performance.', en: 'Correct configurations can dramatically improve performance.', pt: 'As configura√ß√µes corretas podem melhorar drasticamente a performance.' },
      theory: { es: `## Configuraciones Importantes\n\n\`\`\`python\n# Ver configuraci√≥n actual\nspark.conf.get("spark.sql.shuffle.partitions")\n\n# Cambiar en runtime\nspark.conf.set("spark.sql.shuffle.partitions", "200")\n\n# Configuraciones clave:\n\n# Particiones de shuffle (default 200, ajustar seg√∫n datos)\nspark.conf.set("spark.sql.shuffle.partitions", "auto")\n\n# Adaptive Query Execution\nspark.conf.set("spark.sql.adaptive.enabled", "true")\n\n# Broadcast threshold (bytes)\nspark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10MB")\n\n# Memoria del driver\nspark.conf.set("spark.driver.memory", "4g")\n\n# Memoria del executor\nspark.conf.set("spark.executor.memory", "8g")\n\`\`\`\n\n### En Databricks:\nMuchas configuraciones se manejan al crear el cluster en la UI.`, en: `## Important Configurations\n\n\`\`\`python\n# View current configuration\nspark.conf.get("spark.sql.shuffle.partitions")\n\n# Change at runtime\nspark.conf.set("spark.sql.shuffle.partitions", "200")\n\n# Key configurations:\n\n# Shuffle partitions (default 200, adjust based on data)\nspark.conf.set("spark.sql.shuffle.partitions", "auto")\n\n# Adaptive Query Execution\nspark.conf.set("spark.sql.adaptive.enabled", "true")\n\n# Broadcast threshold (bytes)\nspark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10MB")\n\n# Driver memory\nspark.conf.set("spark.driver.memory", "4g")\n\n# Executor memory\nspark.conf.set("spark.executor.memory", "8g")\n\`\`\`\n\n### In Databricks:\nMany configurations are managed when creating the cluster in the UI.`, pt: `## Configura√ß√µes Importantes\n\n\`\`\`python\n# Ver configura√ß√£o atual\nspark.conf.get("spark.sql.shuffle.partitions")\n\n# Mudar em runtime\nspark.conf.set("spark.sql.shuffle.partitions", "200")\n\n# Configura√ß√µes chave:\n\n# Parti√ß√µes de shuffle (default 200, ajustar conforme dados)\nspark.conf.set("spark.sql.shuffle.partitions", "auto")\n\n# Adaptive Query Execution\nspark.conf.set("spark.sql.adaptive.enabled", "true")\n\n# Broadcast threshold (bytes)\nspark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10MB")\n\n# Mem√≥ria do driver\nspark.conf.set("spark.driver.memory", "4g")\n\n# Mem√≥ria do executor\nspark.conf.set("spark.executor.memory", "8g")\n\`\`\`\n\n### No Databricks:\nMuitas configura√ß√µes s√£o gerenciadas ao criar o cluster na UI.` },
      practicalTips: [{ es: '‚ö° spark.sql.shuffle.partitions="auto" + AQE es la mejor configuraci√≥n default.', en: '‚ö° spark.sql.shuffle.partitions="auto" + AQE is the best default config.', pt: '‚ö° spark.sql.shuffle.partitions="auto" + AQE √© a melhor config padr√£o.' }],
      externalLinks: [{ title: 'Configuration', url: 'https://spark.apache.org/docs/latest/configuration.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øExperimentaste con diferentes valores de shuffle.partitions?', en: '‚úÖ Did you experiment with different shuffle.partitions values?', pt: '‚úÖ Voc√™ experimentou com diferentes valores de shuffle.partitions?' },
      xpReward: 25,
      estimatedMinutes: 25
    },
    {
      id: 'db-4-9',
      title: { es: 'Schema Types: StructType y StructField', en: 'Schema Types: StructType and StructField', pt: 'Schema Types: StructType e StructField' },
      description: { es: 'Defin√≠ schemas expl√≠citos para mejor control y performance.', en: 'Define explicit schemas for better control and performance.', pt: 'Defina schemas expl√≠citos para melhor controle e performance.' },
      theory: { es: `## Definir Schemas Expl√≠citos\n\n\`\`\`python\nfrom pyspark.sql.types import *\n\n# Definir schema\nschema = StructType([\n    StructField("id", IntegerType(), nullable=False),\n    StructField("nombre", StringType(), nullable=True),\n    StructField("precio", DoubleType(), nullable=True),\n    StructField("fecha", DateType(), nullable=True),\n    StructField("activo", BooleanType(), nullable=True)\n])\n\n# Usar al leer\ndf = spark.read.schema(schema).csv("path")\n\n# Schema anidado (JSON)\nschema_complejo = StructType([\n    StructField("usuario", StructType([\n        StructField("id", IntegerType()),\n        StructField("nombre", StringType())\n    ])),\n    StructField("items", ArrayType(StructType([\n        StructField("producto", StringType()),\n        StructField("cantidad", IntegerType())\n    ])))\n])\n\`\`\`\n\n### Tipos disponibles:\n- StringType, IntegerType, LongType, DoubleType, FloatType\n- BooleanType, DateType, TimestampType\n- ArrayType, MapType, StructType\n\n### ¬øPor qu√© usar schema expl√≠cito?\n‚úÖ M√°s r√°pido (no inferSchema)\n‚úÖ M√°s control sobre tipos\n‚úÖ Detecta errores temprano\n‚úÖ Documentaci√≥n del dato`, en: `## Define Explicit Schemas\n\n\`\`\`python\nfrom pyspark.sql.types import *\n\n# Define schema\nschema = StructType([\n    StructField("id", IntegerType(), nullable=False),\n    StructField("name", StringType(), nullable=True),\n    StructField("price", DoubleType(), nullable=True),\n    StructField("date", DateType(), nullable=True),\n    StructField("active", BooleanType(), nullable=True)\n])\n\n# Use when reading\ndf = spark.read.schema(schema).csv("path")\n\n# Nested schema (JSON)\ncomplex_schema = StructType([\n    StructField("user", StructType([\n        StructField("id", IntegerType()),\n        StructField("name", StringType())\n    ])),\n    StructField("items", ArrayType(StructType([\n        StructField("product", StringType()),\n        StructField("quantity", IntegerType())\n    ])))\n])\n\`\`\`\n\n### Available types:\n- StringType, IntegerType, LongType, DoubleType, FloatType\n- BooleanType, DateType, TimestampType\n- ArrayType, MapType, StructType\n\n### Why use explicit schema?\n‚úÖ Faster (no inferSchema)\n‚úÖ More control over types\n‚úÖ Detect errors early\n‚úÖ Data documentation`, pt: `## Definir Schemas Expl√≠citos\n\n\`\`\`python\nfrom pyspark.sql.types import *\n\n# Definir schema\nschema = StructType([\n    StructField("id", IntegerType(), nullable=False),\n    StructField("nome", StringType(), nullable=True),\n    StructField("preco", DoubleType(), nullable=True),\n    StructField("data", DateType(), nullable=True),\n    StructField("ativo", BooleanType(), nullable=True)\n])\n\n# Usar ao ler\ndf = spark.read.schema(schema).csv("path")\n\n# Schema aninhado (JSON)\nschema_complexo = StructType([\n    StructField("usuario", StructType([\n        StructField("id", IntegerType()),\n        StructField("nome", StringType())\n    ])),\n    StructField("items", ArrayType(StructType([\n        StructField("produto", StringType()),\n        StructField("quantidade", IntegerType())\n    ])))\n])\n\`\`\`\n\n### Tipos dispon√≠veis:\n- StringType, IntegerType, LongType, DoubleType, FloatType\n- BooleanType, DateType, TimestampType\n- ArrayType, MapType, StructType\n\n### Por que usar schema expl√≠cito?\n‚úÖ Mais r√°pido (sem inferSchema)\n‚úÖ Mais controle sobre tipos\n‚úÖ Detecta erros cedo\n‚úÖ Documenta√ß√£o do dado` },
      practicalTips: [{ es: '‚ö° inferSchema=True lee el archivo 2 veces. Con schema expl√≠cito solo 1 vez.', en: '‚ö° inferSchema=True reads the file twice. With explicit schema only once.', pt: '‚ö° inferSchema=True l√™ o arquivo 2 vezes. Com schema expl√≠cito s√≥ 1 vez.' }],
      externalLinks: [{ title: 'Data Types', url: 'https://spark.apache.org/docs/latest/sql-ref-datatypes.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øDefiniste un schema con StructType y lo usaste para leer datos?', en: '‚úÖ Did you define a schema with StructType and use it to read data?', pt: '‚úÖ Voc√™ definiu um schema com StructType e usou para ler dados?' },
      xpReward: 25,
      estimatedMinutes: 20
    },
    {
      id: 'db-4-10',
      title: { es: 'Higher-Order Functions', en: 'Higher-Order Functions', pt: 'Higher-Order Functions' },
      description: { es: 'Funciones avanzadas para trabajar con arrays y estructuras complejas.', en: 'Advanced functions for working with arrays and complex structures.', pt: 'Fun√ß√µes avan√ßadas para trabalhar com arrays e estruturas complexas.' },
      theory: { es: `## Higher-Order Functions\n\nFunciones que operan sobre arrays y maps.\n\n### TRANSFORM - Aplicar funci√≥n a cada elemento:\n\`\`\`python\nfrom pyspark.sql.functions import transform, col\n\n# Duplicar cada elemento del array\ndf.select(transform("numeros", lambda x: x * 2).alias("dobles"))\n\n# En SQL:\n# SELECT transform(numeros, x -> x * 2) FROM tabla\n\`\`\`\n\n### FILTER - Filtrar elementos del array:\n\`\`\`python\nfrom pyspark.sql.functions import filter\n\n# Solo n√∫meros positivos\ndf.select(filter("numeros", lambda x: x > 0).alias("positivos"))\n\n# En SQL:\n# SELECT filter(numeros, x -> x > 0) FROM tabla\n\`\`\`\n\n### AGGREGATE - Reducir array a un valor:\n\`\`\`python\nfrom pyspark.sql.functions import aggregate\n\n# Sumar todos los elementos\ndf.select(aggregate(\n    "numeros",\n    lit(0),  # valor inicial\n    lambda acc, x: acc + x  # funci√≥n de acumulaci√≥n\n).alias("suma"))\n\`\`\`\n\n### EXISTS - Verificar si existe elemento:\n\`\`\`python\nfrom pyspark.sql.functions import exists\n\n# ¬øHay alg√∫n negativo?\ndf.select(exists("numeros", lambda x: x < 0).alias("tiene_negativo"))\n\`\`\`\n\n### FORALL - Verificar todos los elementos:\n\`\`\`python\nfrom pyspark.sql.functions import forall\n\n# ¬øTodos son positivos?\ndf.select(forall("numeros", lambda x: x > 0).alias("todos_positivos"))\n\`\`\``, en: `## Higher-Order Functions\n\nFunctions that operate on arrays and maps.\n\n### TRANSFORM - Apply function to each element:\n\`\`\`python\nfrom pyspark.sql.functions import transform, col\n\n# Double each array element\ndf.select(transform("numbers", lambda x: x * 2).alias("doubles"))\n\n# In SQL:\n# SELECT transform(numbers, x -> x * 2) FROM table\n\`\`\`\n\n### FILTER - Filter array elements:\n\`\`\`python\nfrom pyspark.sql.functions import filter\n\n# Only positive numbers\ndf.select(filter("numbers", lambda x: x > 0).alias("positives"))\n\n# In SQL:\n# SELECT filter(numbers, x -> x > 0) FROM table\n\`\`\`\n\n### AGGREGATE - Reduce array to a value:\n\`\`\`python\nfrom pyspark.sql.functions import aggregate\n\n# Sum all elements\ndf.select(aggregate(\n    "numbers",\n    lit(0),  # initial value\n    lambda acc, x: acc + x  # accumulation function\n).alias("sum"))\n\`\`\`\n\n### EXISTS - Check if element exists:\n\`\`\`python\nfrom pyspark.sql.functions import exists\n\n# Any negative?\ndf.select(exists("numbers", lambda x: x < 0).alias("has_negative"))\n\`\`\`\n\n### FORALL - Check all elements:\n\`\`\`python\nfrom pyspark.sql.functions import forall\n\n# All positive?\ndf.select(forall("numbers", lambda x: x > 0).alias("all_positive"))\n\`\`\``, pt: `## Higher-Order Functions\n\nFun√ß√µes que operam em arrays e maps.\n\n### TRANSFORM - Aplicar fun√ß√£o a cada elemento:\n\`\`\`python\nfrom pyspark.sql.functions import transform, col\n\n# Duplicar cada elemento do array\ndf.select(transform("numeros", lambda x: x * 2).alias("dobros"))\n\n# Em SQL:\n# SELECT transform(numeros, x -> x * 2) FROM tabela\n\`\`\`\n\n### FILTER - Filtrar elementos do array:\n\`\`\`python\nfrom pyspark.sql.functions import filter\n\n# S√≥ n√∫meros positivos\ndf.select(filter("numeros", lambda x: x > 0).alias("positivos"))\n\n# Em SQL:\n# SELECT filter(numeros, x -> x > 0) FROM tabela\n\`\`\`\n\n### AGGREGATE - Reduzir array a um valor:\n\`\`\`python\nfrom pyspark.sql.functions import aggregate\n\n# Somar todos os elementos\ndf.select(aggregate(\n    "numeros",\n    lit(0),  # valor inicial\n    lambda acc, x: acc + x  # fun√ß√£o de acumula√ß√£o\n).alias("soma"))\n\`\`\`\n\n### EXISTS - Verificar se existe elemento:\n\`\`\`python\nfrom pyspark.sql.functions import exists\n\n# H√° algum negativo?\ndf.select(exists("numeros", lambda x: x < 0).alias("tem_negativo"))\n\`\`\`\n\n### FORALL - Verificar todos os elementos:\n\`\`\`python\nfrom pyspark.sql.functions import forall\n\n# Todos s√£o positivos?\ndf.select(forall("numeros", lambda x: x > 0).alias("todos_positivos"))\n\`\`\`` },
      practicalTips: [{ es: '‚≠ê Estas funciones son pregunta frecuente en el examen de certificaci√≥n.', en: '‚≠ê These functions are frequently asked in the certification exam.', pt: '‚≠ê Estas fun√ß√µes s√£o pergunta frequente no exame de certifica√ß√£o.' }],
      externalLinks: [{ title: 'Higher-Order Functions', url: 'https://docs.databricks.com/sql/language-manual/functions/transform.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øUsaste transform y filter en un array?', en: '‚úÖ Did you use transform and filter on an array?', pt: '‚úÖ Voc√™ usou transform e filter em um array?' },
      xpReward: 30,
      estimatedMinutes: 25
    },
    {
      id: 'db-4-11',
      title: { es: 'Spark SQL vs DataFrame API', en: 'Spark SQL vs DataFrame API', pt: 'Spark SQL vs DataFrame API' },
      description: { es: 'Cu√°ndo usar SQL y cu√°ndo usar DataFrame API.', en: 'When to use SQL and when to use DataFrame API.', pt: 'Quando usar SQL e quando usar DataFrame API.' },
      theory: { es: `## Spark SQL vs DataFrame API\n\nAmbos generan el mismo plan de ejecuci√≥n.\n\n### Misma operaci√≥n, dos formas:\n\n**DataFrame API:**\n\`\`\`python\nfrom pyspark.sql.functions import *\n\nresult = (df\n    .filter(col("estado") == "activo")\n    .groupBy("categoria")\n    .agg(\n        sum("ventas").alias("total"),\n        avg("precio").alias("promedio")\n    )\n    .orderBy(desc("total"))\n)\n\`\`\`\n\n**Spark SQL:**\n\`\`\`python\ndf.createOrReplaceTempView("ventas")\n\nresult = spark.sql("""\n    SELECT \n        categoria,\n        SUM(ventas) as total,\n        AVG(precio) as promedio\n    FROM ventas\n    WHERE estado = 'activo'\n    GROUP BY categoria\n    ORDER BY total DESC\n""")\n\`\`\`\n\n### ¬øCu√°ndo usar cada uno?\n\n| Caso | Recomendaci√≥n |\n|------|---------------|\n| Queries ad-hoc | SQL ‚úÖ |\n| Pipelines de ETL | DataFrame ‚úÖ |\n| L√≥gica condicional compleja | DataFrame ‚úÖ |\n| Reportes/dashboards | SQL ‚úÖ |\n| C√≥digo reutilizable | DataFrame ‚úÖ |\n| Usuarios de negocio | SQL ‚úÖ |\n\n### Mezclar ambos:\n\`\`\`python\n# Crear view del DataFrame\ndf.createOrReplaceTempView("tabla")\n\n# Usar SQL\nresult = spark.sql("SELECT * FROM tabla WHERE x > 10")\n\n# Volver a DataFrame\nresult.filter(col("y") < 100)\n\`\`\``, en: `## Spark SQL vs DataFrame API\n\nBoth generate the same execution plan.\n\n### Same operation, two ways:\n\n**DataFrame API:**\n\`\`\`python\nfrom pyspark.sql.functions import *\n\nresult = (df\n    .filter(col("status") == "active")\n    .groupBy("category")\n    .agg(\n        sum("sales").alias("total"),\n        avg("price").alias("average")\n    )\n    .orderBy(desc("total"))\n)\n\`\`\`\n\n**Spark SQL:**\n\`\`\`python\ndf.createOrReplaceTempView("sales")\n\nresult = spark.sql("""\n    SELECT \n        category,\n        SUM(sales) as total,\n        AVG(price) as average\n    FROM sales\n    WHERE status = 'active'\n    GROUP BY category\n    ORDER BY total DESC\n""")\n\`\`\`\n\n### When to use each?\n\n| Case | Recommendation |\n|------|---------------|\n| Ad-hoc queries | SQL ‚úÖ |\n| ETL pipelines | DataFrame ‚úÖ |\n| Complex conditional logic | DataFrame ‚úÖ |\n| Reports/dashboards | SQL ‚úÖ |\n| Reusable code | DataFrame ‚úÖ |\n| Business users | SQL ‚úÖ |\n\n### Mix both:\n\`\`\`python\n# Create view from DataFrame\ndf.createOrReplaceTempView("table")\n\n# Use SQL\nresult = spark.sql("SELECT * FROM table WHERE x > 10")\n\n# Back to DataFrame\nresult.filter(col("y") < 100)\n\`\`\``, pt: `## Spark SQL vs DataFrame API\n\nAmbos geram o mesmo plano de execu√ß√£o.\n\n### Mesma opera√ß√£o, duas formas:\n\n**DataFrame API:**\n\`\`\`python\nfrom pyspark.sql.functions import *\n\nresult = (df\n    .filter(col("status") == "ativo")\n    .groupBy("categoria")\n    .agg(\n        sum("vendas").alias("total"),\n        avg("preco").alias("media")\n    )\n    .orderBy(desc("total"))\n)\n\`\`\`\n\n**Spark SQL:**\n\`\`\`python\ndf.createOrReplaceTempView("vendas")\n\nresult = spark.sql("""\n    SELECT \n        categoria,\n        SUM(vendas) as total,\n        AVG(preco) as media\n    FROM vendas\n    WHERE status = 'ativo'\n    GROUP BY categoria\n    ORDER BY total DESC\n""")\n\`\`\`\n\n### Quando usar cada um?\n\n| Caso | Recomenda√ß√£o |\n|------|---------------|\n| Queries ad-hoc | SQL ‚úÖ |\n| Pipelines de ETL | DataFrame ‚úÖ |\n| L√≥gica condicional complexa | DataFrame ‚úÖ |\n| Relat√≥rios/dashboards | SQL ‚úÖ |\n| C√≥digo reutiliz√°vel | DataFrame ‚úÖ |\n| Usu√°rios de neg√≥cio | SQL ‚úÖ |\n\n### Misturar ambos:\n\`\`\`python\n# Criar view do DataFrame\ndf.createOrReplaceTempView("tabela")\n\n# Usar SQL\nresult = spark.sql("SELECT * FROM tabela WHERE x > 10")\n\n# Voltar para DataFrame\nresult.filter(col("y") < 100)\n\`\`\`` },
      practicalTips: [{ es: 'üí° En Databricks, los notebooks SQL son perfectos para exploraci√≥n. Python para ETL.', en: 'üí° In Databricks, SQL notebooks are perfect for exploration. Python for ETL.', pt: 'üí° No Databricks, notebooks SQL s√£o perfeitos para explora√ß√£o. Python para ETL.' }],
      externalLinks: [{ title: 'Spark SQL Guide', url: 'https://spark.apache.org/docs/latest/sql-programming-guide.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øEscribiste la misma query en SQL y DataFrame API?', en: '‚úÖ Did you write the same query in SQL and DataFrame API?', pt: '‚úÖ Voc√™ escreveu a mesma query em SQL e DataFrame API?' },
      xpReward: 20,
      estimatedMinutes: 20
    },
    {
      id: 'db-4-12',
      title: { es: 'Quiz y Proyecto: Optimizaci√≥n', en: 'Quiz and Project: Optimization', pt: 'Quiz e Projeto: Otimiza√ß√£o' },
      description: { es: 'Pon√© a prueba todo lo aprendido sobre Spark avanzado.', en: 'Test everything learned about advanced Spark.', pt: 'Teste tudo o que aprendeu sobre Spark avan√ßado.' },
      theory: { es: `## Proyecto: Optimizar un Pipeline\n\nTom√° el ETL de la fase anterior y optimizalo:\n\n### Checklist:\n- [ ] Identificar cuellos de botella en Spark UI\n- [ ] Aplicar broadcast join donde sea apropiado\n- [ ] Verificar y corregir skew si existe\n- [ ] Cachear DataFrames reutilizados\n- [ ] Particionar la salida correctamente\n- [ ] Configurar AQE\n\n### M√©tricas a mejorar:\n- Tiempo de ejecuci√≥n total\n- Shuffle bytes\n- Spill a disco\n\n### Quiz:\n1. ¬øCu√°l es m√°s eficiente: UDF o Pandas UDF?\n2. ¬øCu√°ndo usar cache vs persist?\n3. ¬øC√≥mo detect√°s skew en Spark UI?\n4. ¬øQu√© hace broadcast()?\n5. ¬øCu√°l es la diferencia entre repartition y coalesce?`, en: `## Project: Optimize a Pipeline\n\nTake the ETL from the previous phase and optimize it:\n\n### Checklist:\n- [ ] Identify bottlenecks in Spark UI\n- [ ] Apply broadcast join where appropriate\n- [ ] Verify and fix skew if exists\n- [ ] Cache reused DataFrames\n- [ ] Partition output correctly\n- [ ] Configure AQE\n\n### Metrics to improve:\n- Total execution time\n- Shuffle bytes\n- Spill to disk\n\n### Quiz:\n1. Which is more efficient: UDF or Pandas UDF?\n2. When to use cache vs persist?\n3. How do you detect skew in Spark UI?\n4. What does broadcast() do?\n5. What's the difference between repartition and coalesce?`, pt: `## Projeto: Otimizar um Pipeline\n\nPegue o ETL da fase anterior e otimize-o:\n\n### Checklist:\n- [ ] Identificar gargalos no Spark UI\n- [ ] Aplicar broadcast join onde apropriado\n- [ ] Verificar e corrigir skew se existir\n- [ ] Cachear DataFrames reutilizados\n- [ ] Particionar a sa√≠da corretamente\n- [ ] Configurar AQE\n\n### M√©tricas a melhorar:\n- Tempo de execu√ß√£o total\n- Shuffle bytes\n- Spill para disco\n\n### Quiz:\n1. Qual √© mais eficiente: UDF ou Pandas UDF?\n2. Quando usar cache vs persist?\n3. Como voc√™ detecta skew no Spark UI?\n4. O que broadcast() faz?\n5. Qual √© a diferen√ßa entre repartition e coalesce?` },
      practicalTips: [{ es: 'üèÜ Un pipeline bien optimizado puede ser 10x m√°s r√°pido que uno sin optimizar.', en: 'üèÜ A well-optimized pipeline can be 10x faster than an unoptimized one.', pt: 'üèÜ Um pipeline bem otimizado pode ser 10x mais r√°pido que um sem otimiza√ß√£o.' }],
      externalLinks: [{ title: 'Performance Tuning', url: 'https://spark.apache.org/docs/latest/sql-performance-tuning.html', type: 'docs' }],
      checkpoint: { es: 'üèÜ ¬øMejoraste el tiempo de ejecuci√≥n en al menos 30%?', en: 'üèÜ Did you improve execution time by at least 30%?', pt: 'üèÜ Voc√™ melhorou o tempo de execu√ß√£o em pelo menos 30%?' },
      xpReward: 75,
      estimatedMinutes: 60
    }
  ]
};


