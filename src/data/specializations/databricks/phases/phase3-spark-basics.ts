/**
 * FASE 3: Spark B√°sico en Databricks
 * 10 pasos para dominar los fundamentos de Spark
 */

import { DatabricksPhase } from '../types';

export const PHASE_3_SPARK_BASICS: DatabricksPhase = {
  id: 'db-phase-3',
  number: 3,
  title: {
    es: 'Apache Spark B√°sico',
    en: 'Apache Spark Basics',
    pt: 'Apache Spark B√°sico'
  },
  subtitle: {
    es: 'El coraz√≥n de Databricks',
    en: 'The heart of Databricks',
    pt: 'O cora√ß√£o do Databricks'
  },
  description: {
    es: 'Apache Spark es el motor de procesamiento detr√°s de Databricks. Dominar Spark te permite procesar terabytes de datos de forma eficiente.',
    en: 'Apache Spark is the processing engine behind Databricks. Mastering Spark allows you to process terabytes of data efficiently.',
    pt: 'Apache Spark √© o motor de processamento por tr√°s do Databricks. Dominar Spark permite processar terabytes de dados de forma eficiente.'
  },
  icon: '‚ö°',
  color: 'orange',
  estimatedDays: '5-7 d√≠as',
  steps: [
    {
      id: 'db-3-1',
      title: { es: '¬øQu√© es Apache Spark?', en: 'What is Apache Spark?', pt: 'O que √© Apache Spark?' },
      description: { es: 'Historia, arquitectura y por qu√© Spark revolucion√≥ el procesamiento de Big Data.', en: 'History, architecture and why Spark revolutionized Big Data processing.', pt: 'Hist√≥ria, arquitetura e por que Spark revolucionou o processamento de Big Data.' },
      theory: {
        es: `## Apache Spark: El Motor de Big Data m√°s Poderoso

Apache Spark es una **plataforma de procesamiento de datos distribuido** que ha revolucionado la forma en que las empresas manejan Big Data. Creado por los mismos fundadores de Databricks, Spark se ha convertido en el est√°ndar de la industria para procesar grandes vol√∫menes de datos.

### Historia y Evoluci√≥n

**2009 - El Nacimiento:**
Spark fue creado en el AMPLab de UC Berkeley por Matei Zaharia. El objetivo era superar las limitaciones de MapReduce, el framework dominante en ese momento.

**2010 - Open Source:**
El proyecto se libera como c√≥digo abierto, permitiendo que la comunidad contribuya y lo mejore.

**2013 - Apache Foundation:**
Spark se convierte en proyecto de Apache, ganando credibilidad empresarial.

**2014 - R√©cord Mundial:**
Spark estableci√≥ el r√©cord mundial de ordenamiento de datos, procesando 100TB en 23 minutos (3x m√°s r√°pido que el r√©cord anterior con Hadoop).

**2023 - Dominancia Total:**
M√°s del 80% de las empresas Fortune 500 usan Spark. Es el framework #1 en Gartner Magic Quadrant para Data Science y ML.

### ¬øPor qu√© Spark es 100x m√°s R√°pido que MapReduce?

El secreto est√° en el **procesamiento en memoria (in-memory computing)**.

**MapReduce (Hadoop) - El Problema:**
\`\`\`
   PASO 1          PASO 2          PASO 3
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Leer   ‚îÇ ‚Üí  ‚îÇ Escribir‚îÇ ‚Üí  ‚îÇ  Leer   ‚îÇ ‚Üí  ...
‚îÇ  Disco  ‚îÇ    ‚îÇ  Disco  ‚îÇ    ‚îÇ  Disco  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì              ‚Üì              ‚Üì
   LENTO         LENTO          LENTO
   (I/O)         (I/O)          (I/O)

Cada paso intermedio escribe a disco = MUY LENTO
\`\`\`

**Spark - La Soluci√≥n:**
\`\`\`
   PASO 1          PASO 2          PASO 3
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Leer   ‚îÇ ‚Üí  ‚îÇ Procesar‚îÇ ‚Üí  ‚îÇ Procesar‚îÇ ‚Üí Resultado
‚îÇ  Disco  ‚îÇ    ‚îÇ MEMORIA ‚îÇ    ‚îÇ MEMORIA ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì              ‚Üì              ‚Üì
   1 vez       SUPER R√ÅPIDO   SUPER R√ÅPIDO
              (RAM es 100x    (Sin I/O de
               m√°s r√°pida)     disco)

Solo lee del disco 1 vez, todo lo dem√°s en RAM
\`\`\`

### Los 5 Componentes de Spark

1. **Spark Core** - El motor base
   - Gesti√≥n de memoria y disco
   - Scheduling de tareas
   - Recuperaci√≥n ante fallos
   - API de RDDs (Resilient Distributed Datasets)

2. **Spark SQL** - Consultas estructuradas
   - DataFrames y Datasets API
   - Optimizador Catalyst
   - Soporte JDBC/ODBC
   - Compatible con Hive

3. **Spark Streaming** - Datos en tiempo real
   - Micro-batch processing
   - Structured Streaming
   - Integraci√≥n con Kafka, Kinesis
   - Exactly-once semantics

4. **MLlib** - Machine Learning
   - Algoritmos distribuidos
   - Feature engineering
   - Model selection
   - Pipelines de ML

5. **GraphX** - Procesamiento de grafos
   - Algoritmos de grafos (PageRank, etc.)
   - Graph-parallel computation
   - Integraci√≥n con GraphFrames

### Arquitectura de Spark en Detalle

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TU APLICACI√ìN SPARK                       ‚îÇ
‚îÇ  (PySpark, Scala, Java, R, SQL)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      DRIVER PROGRAM                          ‚îÇ
‚îÇ  ‚Ä¢ SparkContext / SparkSession                              ‚îÇ
‚îÇ  ‚Ä¢ Planifica tareas (DAG)                                   ‚îÇ
‚îÇ  ‚Ä¢ Distribuye c√≥digo a los workers                          ‚îÇ
‚îÇ  ‚Ä¢ Recolecta resultados                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CLUSTER MANAGER                           ‚îÇ
‚îÇ  (Standalone, YARN, Mesos, Kubernetes)                      ‚îÇ
‚îÇ  ‚Ä¢ Asigna recursos (CPU, memoria)                           ‚îÇ
‚îÇ  ‚Ä¢ Monitorea workers                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚ñº                 ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   WORKER NODE 1   ‚îÇ ‚îÇ   WORKER NODE 2   ‚îÇ ‚îÇ   WORKER NODE N   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Executor   ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  Executor   ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  Executor   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Task  ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ Task  ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ Task  ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Task  ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ Task  ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ Task  ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Cache ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ Cache ‚îÇ  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îÇ Cache ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Empresas que usan Spark (Casos Reales)

- **Netflix**: Procesa 500 mil millones de eventos por d√≠a para recomendaciones
- **Uber**: ETL de 100+ petabytes de datos de viajes
- **Airbnb**: ML para pricing din√°mico y detecci√≥n de fraude
- **Spotify**: An√°lisis de 100 millones de usuarios activos
- **Pinterest**: Procesamiento de 1000+ billones de pins

### ¬øPor qu√© aprender Spark te hace m√°s empleable?

1. **Salarios m√°s altos**: Data Engineers con Spark ganan 20-30% m√°s
2. **Alta demanda**: 70% de las ofertas de DE requieren Spark
3. **Skill transferible**: Funciona en AWS, Azure, GCP, on-premise
4. **Base para Databricks**: Databricks ES Spark optimizado`,
        en: `## Apache Spark: The Most Powerful Big Data Engine

Apache Spark is a **distributed data processing platform** that has revolutionized how companies handle Big Data. Created by the same founders of Databricks, Spark has become the industry standard for processing large volumes of data.

### History and Evolution

**2009 - The Birth:**
Spark was created at UC Berkeley's AMPLab by Matei Zaharia. The goal was to overcome the limitations of MapReduce, the dominant framework at the time.

**2010 - Open Source:**
The project was released as open source, allowing the community to contribute and improve it.

**2013 - Apache Foundation:**
Spark becomes an Apache project, gaining enterprise credibility.

**2014 - World Record:**
Spark set the world record for data sorting, processing 100TB in 23 minutes (3x faster than the previous Hadoop record).

**2023 - Total Dominance:**
More than 80% of Fortune 500 companies use Spark. It's the #1 framework in Gartner Magic Quadrant for Data Science and ML.

### Why is Spark 100x Faster than MapReduce?

The secret is **in-memory computing**.

**MapReduce (Hadoop) - The Problem:**
\`\`\`
   STEP 1          STEP 2          STEP 3
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Read   ‚îÇ ‚Üí  ‚îÇ  Write  ‚îÇ ‚Üí  ‚îÇ  Read   ‚îÇ ‚Üí  ...
‚îÇ  Disk   ‚îÇ    ‚îÇ  Disk   ‚îÇ    ‚îÇ  Disk   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì              ‚Üì              ‚Üì
   SLOW          SLOW           SLOW
   (I/O)         (I/O)          (I/O)

Each intermediate step writes to disk = VERY SLOW
\`\`\`

**Spark - The Solution:**
\`\`\`
   STEP 1          STEP 2          STEP 3
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Read   ‚îÇ ‚Üí  ‚îÇ Process ‚îÇ ‚Üí  ‚îÇ Process ‚îÇ ‚Üí Result
‚îÇ  Disk   ‚îÇ    ‚îÇ MEMORY  ‚îÇ    ‚îÇ MEMORY  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì              ‚Üì              ‚Üì
   1 time      SUPER FAST     SUPER FAST
              (RAM is 100x    (No disk
               faster)         I/O)

Only reads from disk once, everything else in RAM
\`\`\`

### The 5 Components of Spark

1. **Spark Core** - The base engine
2. **Spark SQL** - Structured queries
3. **Spark Streaming** - Real-time data
4. **MLlib** - Machine Learning
5. **GraphX** - Graph processing

### Companies using Spark (Real Cases)

- **Netflix**: Processes 500 billion events per day
- **Uber**: ETL of 100+ petabytes of trip data
- **Airbnb**: ML for dynamic pricing and fraud detection
- **Spotify**: Analysis of 100 million active users`,
        pt: `## Apache Spark: O Motor de Big Data mais Poderoso

Apache Spark √© uma **plataforma de processamento de dados distribu√≠do** que revolucionou a forma como as empresas lidam com Big Data.

### Hist√≥ria e Evolu√ß√£o

**2009 - O Nascimento:**
Spark foi criado no AMPLab da UC Berkeley por Matei Zaharia.

**2014 - Recorde Mundial:**
Spark estabeleceu o recorde mundial de ordena√ß√£o de dados, processando 100TB em 23 minutos.

**2023 - Domin√¢ncia Total:**
Mais de 80% das empresas Fortune 500 usam Spark.

### Por que Spark √© 100x mais R√°pido que MapReduce?

O segredo est√° no **processamento em mem√≥ria**.

### Os 5 Componentes do Spark

1. **Spark Core** - O motor base
2. **Spark SQL** - Consultas estruturadas
3. **Spark Streaming** - Dados em tempo real
4. **MLlib** - Machine Learning
5. **GraphX** - Processamento de grafos`
      },
      practicalTips: [
        { es: 'üí° Spark procesa en memoria RAM, que es 100,000x m√°s r√°pida que el disco duro. Por eso puede ser hasta 100x m√°s r√°pido que Hadoop.', en: 'üí° Spark processes in RAM memory, which is 100,000x faster than hard disk. That\'s why it can be up to 100x faster than Hadoop.', pt: 'üí° Spark processa em mem√≥ria RAM, que √© 100.000x mais r√°pida que o disco. Por isso pode ser at√© 100x mais r√°pido que Hadoop.' },
        { es: 'üí° En Databricks, Spark ya viene pre-configurado y optimizado. No necesitas instalar nada.', en: 'üí° In Databricks, Spark comes pre-configured and optimized. You don\'t need to install anything.', pt: 'üí° No Databricks, Spark j√° vem pr√©-configurado e otimizado. N√£o precisa instalar nada.' },
        { es: 'üí° El 80% de las empresas Fortune 500 usan Spark. Aprenderlo te abre muchas puertas.', en: 'üí° 80% of Fortune 500 companies use Spark. Learning it opens many doors.', pt: 'üí° 80% das empresas Fortune 500 usam Spark. Aprend√™-lo abre muitas portas.' }
      ],
      externalLinks: [
        { title: 'Apache Spark Official', url: 'https://spark.apache.org/', type: 'docs' },
        { title: 'Spark: The Definitive Guide (Free Chapter)', url: 'https://pages.databricks.com/rs/094-YMS-629/images/Apache-Spark-The-Definitive-Guide-Excerpts-R1.pdf', type: 'article' },
        { title: 'Databricks Spark Docs', url: 'https://docs.databricks.com/spark/index.html', type: 'docs' }
      ],
      checkpoint: { es: 'ü§î Explica con tus palabras: ¬øPor qu√© procesar datos en memoria RAM es m√°s r√°pido que hacerlo desde disco? ¬øCu√°l es la diferencia de velocidad aproximada?', en: 'ü§î Explain in your own words: Why is processing data in RAM memory faster than from disk? What is the approximate speed difference?', pt: 'ü§î Explique com suas palavras: Por que processar dados em mem√≥ria RAM √© mais r√°pido que do disco? Qual √© a diferen√ßa de velocidade aproximada?' },
      xpReward: 25,
      estimatedMinutes: 30
    },
    {
      id: 'db-3-2',
      title: { es: 'SparkSession: Tu Punto de Entrada', en: 'SparkSession: Your Entry Point', pt: 'SparkSession: Seu Ponto de Entrada' },
      description: { es: 'SparkSession es el objeto principal para interactuar con Spark.', en: 'SparkSession is the main object to interact with Spark.', pt: 'SparkSession √© o objeto principal para interagir com Spark.' },
      theory: {
        es: `## SparkSession

En Databricks, \`spark\` ya viene pre-configurado.

\`\`\`python
# Ya disponible autom√°ticamente
spark  # SparkSession object

# Ver configuraci√≥n
spark.version
spark.sparkContext.getConf().getAll()

# Crear DataFrame
df = spark.createDataFrame([
    (1, "Ana"),
    (2, "Bob")
], ["id", "nombre"])

# Leer datos
df = spark.read.csv("path/to/file.csv", header=True)
df = spark.read.parquet("path/to/file.parquet")
df = spark.read.json("path/to/file.json")

# Ejecutar SQL
spark.sql("SELECT * FROM mi_tabla")
\`\`\`

### M√©todos principales:
- \`spark.read\` - Leer datos
- \`spark.sql()\` - Ejecutar SQL
- \`spark.createDataFrame()\` - Crear DF
- \`spark.table()\` - Acceder a tabla`,
        en: `## SparkSession

In Databricks, \`spark\` is already pre-configured.

\`\`\`python
# Already available automatically
spark  # SparkSession object

# View configuration
spark.version
spark.sparkContext.getConf().getAll()

# Create DataFrame
df = spark.createDataFrame([
    (1, "Ana"),
    (2, "Bob")
], ["id", "name"])

# Read data
df = spark.read.csv("path/to/file.csv", header=True)
df = spark.read.parquet("path/to/file.parquet")
df = spark.read.json("path/to/file.json")

# Execute SQL
spark.sql("SELECT * FROM my_table")
\`\`\`

### Main methods:
- \`spark.read\` - Read data
- \`spark.sql()\` - Execute SQL
- \`spark.createDataFrame()\` - Create DF
- \`spark.table()\` - Access table`,
        pt: `## SparkSession

No Databricks, \`spark\` j√° vem pr√©-configurado.

\`\`\`python
# J√° dispon√≠vel automaticamente
spark  # SparkSession object

# Ver configura√ß√£o
spark.version
spark.sparkContext.getConf().getAll()

# Criar DataFrame
df = spark.createDataFrame([
    (1, "Ana"),
    (2, "Bob")
], ["id", "nome"])

# Ler dados
df = spark.read.csv("path/to/file.csv", header=True)
df = spark.read.parquet("path/to/file.parquet")
df = spark.read.json("path/to/file.json")

# Executar SQL
spark.sql("SELECT * FROM minha_tabela")
\`\`\`

### M√©todos principais:
- \`spark.read\` - Ler dados
- \`spark.sql()\` - Executar SQL
- \`spark.createDataFrame()\` - Criar DF
- \`spark.table()\` - Acessar tabela`
      },
      practicalTips: [{ es: '‚ö° En Databricks nunca ten√©s que crear SparkSession manualmente.', en: '‚ö° In Databricks you never have to create SparkSession manually.', pt: '‚ö° No Databricks voc√™ nunca precisa criar SparkSession manualmente.' }],
      externalLinks: [{ title: 'SparkSession API', url: 'https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øPudiste ejecutar spark.version en tu notebook?', en: '‚úÖ Could you run spark.version in your notebook?', pt: '‚úÖ Voc√™ conseguiu executar spark.version no seu notebook?' },
      xpReward: 15,
      estimatedMinutes: 15
    },
    {
      id: 'db-3-3',
      title: { es: 'DataFrames: La Estructura Principal', en: 'DataFrames: The Main Structure', pt: 'DataFrames: A Estrutura Principal' },
      description: { es: 'Los DataFrames son tablas distribuidas. Aprend√© a crearlos y manipularlos.', en: 'DataFrames are distributed tables. Learn to create and manipulate them.', pt: 'DataFrames s√£o tabelas distribu√≠das. Aprenda a cri√°-los e manipul√°-los.' },
      theory: {
        es: `## Spark DataFrames

Un DataFrame es una colecci√≥n distribuida de datos organizados en columnas.

\`\`\`python
# Crear DataFrame de lista
data = [(1, "Ana", 25), (2, "Bob", 30)]
df = spark.createDataFrame(data, ["id", "nombre", "edad"])

# Ver datos
df.show()           # Tabla de texto
display(df)         # Tabla interactiva (Databricks)

# Ver esquema
df.printSchema()
df.dtypes

# Informaci√≥n b√°sica
df.count()          # N√∫mero de filas
df.columns          # Lista de columnas
df.describe()       # Estad√≠sticas

# Seleccionar columnas
df.select("nombre", "edad")
df.select(df.nombre, df.edad)

# Filtrar
df.filter(df.edad > 25)
df.where("edad > 25")

# Ordenar
df.orderBy("edad")
df.orderBy(df.edad.desc())
\`\`\``,
        en: `## Spark DataFrames

A DataFrame is a distributed collection of data organized in columns.

\`\`\`python
# Create DataFrame from list
data = [(1, "Ana", 25), (2, "Bob", 30)]
df = spark.createDataFrame(data, ["id", "name", "age"])

# View data
df.show()           # Text table
display(df)         # Interactive table (Databricks)

# View schema
df.printSchema()
df.dtypes

# Basic info
df.count()          # Number of rows
df.columns          # List of columns
df.describe()       # Statistics

# Select columns
df.select("name", "age")
df.select(df.name, df.age)

# Filter
df.filter(df.age > 25)
df.where("age > 25")

# Sort
df.orderBy("age")
df.orderBy(df.age.desc())
\`\`\``,
        pt: `## Spark DataFrames

Um DataFrame √© uma cole√ß√£o distribu√≠da de dados organizados em colunas.

\`\`\`python
# Criar DataFrame de lista
data = [(1, "Ana", 25), (2, "Bob", 30)]
df = spark.createDataFrame(data, ["id", "nome", "idade"])

# Ver dados
df.show()           # Tabela de texto
display(df)         # Tabela interativa (Databricks)

# Ver esquema
df.printSchema()
df.dtypes

# Informa√ß√£o b√°sica
df.count()          # N√∫mero de linhas
df.columns          # Lista de colunas
df.describe()       # Estat√≠sticas

# Selecionar colunas
df.select("nome", "idade")
df.select(df.nome, df.idade)

# Filtrar
df.filter(df.idade > 25)
df.where("idade > 25")

# Ordenar
df.orderBy("idade")
df.orderBy(df.idade.desc())
\`\`\``
      },
      practicalTips: [{ es: 'üí° display() es mejor que show() en Databricks - tiene visualizaciones interactivas.', en: 'üí° display() is better than show() in Databricks - it has interactive visualizations.', pt: 'üí° display() √© melhor que show() no Databricks - tem visualiza√ß√µes interativas.' }],
      externalLinks: [{ title: 'DataFrame API', url: 'https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øCreaste un DataFrame y usaste filter y select?', en: '‚úÖ Did you create a DataFrame and use filter and select?', pt: '‚úÖ Voc√™ criou um DataFrame e usou filter e select?' },
      xpReward: 25,
      estimatedMinutes: 25
    },
    {
      id: 'db-3-4',
      title: { es: 'Transformaciones vs Acciones', en: 'Transformations vs Actions', pt: 'Transforma√ß√µes vs A√ß√µes' },
      description: { es: 'Entender la diferencia es clave para escribir c√≥digo Spark eficiente.', en: 'Understanding the difference is key to writing efficient Spark code.', pt: 'Entender a diferen√ßa √© chave para escrever c√≥digo Spark eficiente.' },
      theory: {
        es: `## Lazy Evaluation en Spark

Spark usa "evaluaci√≥n perezosa": no ejecuta nada hasta que es necesario.

### Transformaciones (Lazy)
No ejecutan inmediatamente, solo crean un plan:
\`\`\`python
# Ninguna de estas ejecuta nada todav√≠a
df2 = df.filter(df.edad > 25)      # Transformaci√≥n
df3 = df2.select("nombre")          # Transformaci√≥n
df4 = df3.withColumn("x", lit(1))   # Transformaci√≥n
\`\`\`

### Acciones (Eager)
Disparan la ejecuci√≥n del plan:
\`\`\`python
df4.show()      # ACCI√ìN - ejecuta todo
df4.count()     # ACCI√ìN
df4.collect()   # ACCI√ìN
df4.write...    # ACCI√ìN
\`\`\`

### ¬øPor qu√© importa?
Spark optimiza todas las transformaciones juntas antes de ejecutar.

\`\`\`
Plan l√≥gico:
filter ‚Üí select ‚Üí withColumn

Plan f√≠sico optimizado:
(Spark combina operaciones, reordena, etc.)
\`\`\`

### Lista de transformaciones comunes:
select, filter, where, groupBy, orderBy, join, withColumn, drop

### Lista de acciones comunes:
show, count, collect, take, first, write, save`,
        en: `## Lazy Evaluation in Spark

Spark uses "lazy evaluation": it doesn't execute anything until necessary.

### Transformations (Lazy)
Don't execute immediately, just create a plan:
\`\`\`python
# None of these execute anything yet
df2 = df.filter(df.age > 25)        # Transformation
df3 = df2.select("name")            # Transformation
df4 = df3.withColumn("x", lit(1))   # Transformation
\`\`\`

### Actions (Eager)
Trigger execution of the plan:
\`\`\`python
df4.show()      # ACTION - executes everything
df4.count()     # ACTION
df4.collect()   # ACTION
df4.write...    # ACTION
\`\`\`

### Why does it matter?
Spark optimizes all transformations together before executing.

\`\`\`
Logical plan:
filter ‚Üí select ‚Üí withColumn

Optimized physical plan:
(Spark combines operations, reorders, etc.)
\`\`\`

### Common transformations:
select, filter, where, groupBy, orderBy, join, withColumn, drop

### Common actions:
show, count, collect, take, first, write, save`,
        pt: `## Lazy Evaluation no Spark

Spark usa "avalia√ß√£o pregui√ßosa": n√£o executa nada at√© ser necess√°rio.

### Transforma√ß√µes (Lazy)
N√£o executam imediatamente, s√≥ criam um plano:
\`\`\`python
# Nenhuma dessas executa nada ainda
df2 = df.filter(df.idade > 25)      # Transforma√ß√£o
df3 = df2.select("nome")            # Transforma√ß√£o
df4 = df3.withColumn("x", lit(1))   # Transforma√ß√£o
\`\`\`

### A√ß√µes (Eager)
Disparam a execu√ß√£o do plano:
\`\`\`python
df4.show()      # A√á√ÉO - executa tudo
df4.count()     # A√á√ÉO
df4.collect()   # A√á√ÉO
df4.write...    # A√á√ÉO
\`\`\`

### Por que importa?
Spark otimiza todas as transforma√ß√µes juntas antes de executar.

\`\`\`
Plano l√≥gico:
filter ‚Üí select ‚Üí withColumn

Plano f√≠sico otimizado:
(Spark combina opera√ß√µes, reordena, etc.)
\`\`\`

### Transforma√ß√µes comuns:
select, filter, where, groupBy, orderBy, join, withColumn, drop

### A√ß√µes comuns:
show, count, collect, take, first, write, save`
      },
      practicalTips: [{ es: '‚ö†Ô∏è collect() trae todos los datos al driver. Puede crashear con datasets grandes!', en: '‚ö†Ô∏è collect() brings all data to driver. Can crash with large datasets!', pt: '‚ö†Ô∏è collect() traz todos os dados para o driver. Pode crashar com datasets grandes!' }],
      externalLinks: [{ title: 'RDD Programming Guide', url: 'https://spark.apache.org/docs/latest/rdd-programming-guide.html', type: 'docs' }],
      checkpoint: { es: 'ü§î ¬øfilter() es una transformaci√≥n o una acci√≥n?', en: 'ü§î Is filter() a transformation or an action?', pt: 'ü§î filter() √© uma transforma√ß√£o ou uma a√ß√£o?' },
      xpReward: 25,
      estimatedMinutes: 20
    },
    {
      id: 'db-3-5',
      title: { es: 'Funciones de Columna (pyspark.sql.functions)', en: 'Column Functions (pyspark.sql.functions)', pt: 'Fun√ß√µes de Coluna (pyspark.sql.functions)' },
      description: { es: 'Las funciones built-in de Spark para transformar datos.', en: 'Spark built-in functions to transform data.', pt: 'As fun√ß√µes built-in do Spark para transformar dados.' },
      theory: {
        es: `## pyspark.sql.functions

\`\`\`python
from pyspark.sql.functions import *

# Funciones de string
df.select(upper("nombre"), lower("nombre"), trim("nombre"))
df.select(concat("nombre", lit(" "), "apellido"))
df.select(substring("texto", 1, 5))

# Funciones num√©ricas
df.select(round("precio", 2), floor("precio"), ceil("precio"))
df.select(abs("valor"), sqrt("valor"))

# Funciones de fecha
df.select(current_date(), current_timestamp())
df.select(year("fecha"), month("fecha"), dayofweek("fecha"))
df.select(datediff("fecha_fin", "fecha_inicio"))
df.select(date_add("fecha", 7))

# Funciones de agregaci√≥n
df.groupBy("categoria").agg(
    sum("ventas"),
    avg("precio"),
    count("*"),
    max("fecha")
)

# Condicionales
df.select(when(col("edad") > 18, "adulto").otherwise("menor"))
df.select(coalesce("valor1", "valor2", lit(0)))

# Crear columnas
df.withColumn("nuevo", col("precio") * 1.21)
df.withColumn("constante", lit(100))
\`\`\``,
        en: `## pyspark.sql.functions

\`\`\`python
from pyspark.sql.functions import *

# String functions
df.select(upper("name"), lower("name"), trim("name"))
df.select(concat("first_name", lit(" "), "last_name"))
df.select(substring("text", 1, 5))

# Numeric functions
df.select(round("price", 2), floor("price"), ceil("price"))
df.select(abs("value"), sqrt("value"))

# Date functions
df.select(current_date(), current_timestamp())
df.select(year("date"), month("date"), dayofweek("date"))
df.select(datediff("end_date", "start_date"))
df.select(date_add("date", 7))

# Aggregation functions
df.groupBy("category").agg(
    sum("sales"),
    avg("price"),
    count("*"),
    max("date")
)

# Conditionals
df.select(when(col("age") > 18, "adult").otherwise("minor"))
df.select(coalesce("value1", "value2", lit(0)))

# Create columns
df.withColumn("new", col("price") * 1.21)
df.withColumn("constant", lit(100))
\`\`\``,
        pt: `## pyspark.sql.functions

\`\`\`python
from pyspark.sql.functions import *

# Fun√ß√µes de string
df.select(upper("nome"), lower("nome"), trim("nome"))
df.select(concat("nome", lit(" "), "sobrenome"))
df.select(substring("texto", 1, 5))

# Fun√ß√µes num√©ricas
df.select(round("preco", 2), floor("preco"), ceil("preco"))
df.select(abs("valor"), sqrt("valor"))

# Fun√ß√µes de data
df.select(current_date(), current_timestamp())
df.select(year("data"), month("data"), dayofweek("data"))
df.select(datediff("data_fim", "data_inicio"))
df.select(date_add("data", 7))

# Fun√ß√µes de agrega√ß√£o
df.groupBy("categoria").agg(
    sum("vendas"),
    avg("preco"),
    count("*"),
    max("data")
)

# Condicionais
df.select(when(col("idade") > 18, "adulto").otherwise("menor"))
df.select(coalesce("valor1", "valor2", lit(0)))

# Criar colunas
df.withColumn("novo", col("preco") * 1.21)
df.withColumn("constante", lit(100))
\`\`\``
      },
      practicalTips: [{ es: 'üí° Siempre import√° con "from pyspark.sql.functions import *" para tener todas las funciones disponibles.', en: 'üí° Always import with "from pyspark.sql.functions import *" to have all functions available.', pt: 'üí° Sempre importe com "from pyspark.sql.functions import *" para ter todas as fun√ß√µes dispon√≠veis.' }],
      externalLinks: [{ title: 'Functions Reference', url: 'https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øUsaste when/otherwise para crear una columna condicional?', en: '‚úÖ Did you use when/otherwise to create a conditional column?', pt: '‚úÖ Voc√™ usou when/otherwise para criar uma coluna condicional?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-3-6',
      title: { es: 'Joins en Spark', en: 'Joins in Spark', pt: 'Joins no Spark' },
      description: { es: 'Combinar DataFrames es esencial. Aprend√© todos los tipos de joins.', en: 'Combining DataFrames is essential. Learn all join types.', pt: 'Combinar DataFrames √© essencial. Aprenda todos os tipos de joins.' },
      theory: {
        es: `## Joins en Spark

\`\`\`python
# Datos de ejemplo
clientes = spark.createDataFrame([
    (1, "Ana"), (2, "Bob"), (3, "Carlos")
], ["id", "nombre"])

pedidos = spark.createDataFrame([
    (1, 100), (1, 200), (2, 150), (4, 300)
], ["cliente_id", "monto"])

# INNER JOIN (solo matches)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "inner")

# LEFT JOIN (todos los de la izquierda)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "left")

# RIGHT JOIN (todos los de la derecha)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "right")

# FULL OUTER JOIN (todos)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "outer")

# CROSS JOIN (producto cartesiano)
clientes.crossJoin(pedidos)

# LEFT ANTI (los que NO tienen match)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "left_anti")

# LEFT SEMI (los que S√ç tienen match, solo columnas izquierda)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "left_semi")
\`\`\`

### Tips de performance:
- Broadcast join para tablas peque√±as
- Evitar cross joins cuando sea posible`,
        en: `## Joins in Spark

\`\`\`python
# Sample data
customers = spark.createDataFrame([
    (1, "Ana"), (2, "Bob"), (3, "Carlos")
], ["id", "name"])

orders = spark.createDataFrame([
    (1, 100), (1, 200), (2, 150), (4, 300)
], ["customer_id", "amount"])

# INNER JOIN (only matches)
customers.join(orders, customers.id == orders.customer_id, "inner")

# LEFT JOIN (all from left)
customers.join(orders, customers.id == orders.customer_id, "left")

# RIGHT JOIN (all from right)
customers.join(orders, customers.id == orders.customer_id, "right")

# FULL OUTER JOIN (all)
customers.join(orders, customers.id == orders.customer_id, "outer")

# CROSS JOIN (cartesian product)
customers.crossJoin(orders)

# LEFT ANTI (those without match)
customers.join(orders, customers.id == orders.customer_id, "left_anti")

# LEFT SEMI (those with match, only left columns)
customers.join(orders, customers.id == orders.customer_id, "left_semi")
\`\`\`

### Performance tips:
- Broadcast join for small tables
- Avoid cross joins when possible`,
        pt: `## Joins no Spark

\`\`\`python
# Dados de exemplo
clientes = spark.createDataFrame([
    (1, "Ana"), (2, "Bob"), (3, "Carlos")
], ["id", "nome"])

pedidos = spark.createDataFrame([
    (1, 100), (1, 200), (2, 150), (4, 300)
], ["cliente_id", "valor"])

# INNER JOIN (s√≥ matches)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "inner")

# LEFT JOIN (todos da esquerda)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "left")

# RIGHT JOIN (todos da direita)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "right")

# FULL OUTER JOIN (todos)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "outer")

# CROSS JOIN (produto cartesiano)
clientes.crossJoin(pedidos)

# LEFT ANTI (os que N√ÉO t√™m match)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "left_anti")

# LEFT SEMI (os que SIM t√™m match, s√≥ colunas esquerda)
clientes.join(pedidos, clientes.id == pedidos.cliente_id, "left_semi")
\`\`\`

### Dicas de performance:
- Broadcast join para tabelas pequenas
- Evitar cross joins quando poss√≠vel`
      },
      practicalTips: [{ es: '‚ö° Us√° broadcast() para tablas < 10MB: from pyspark.sql.functions import broadcast', en: '‚ö° Use broadcast() for tables < 10MB: from pyspark.sql.functions import broadcast', pt: '‚ö° Use broadcast() para tabelas < 10MB: from pyspark.sql.functions import broadcast' }],
      externalLinks: [{ title: 'Join Types', url: 'https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øProbaste left_anti para encontrar clientes sin pedidos?', en: '‚úÖ Did you try left_anti to find customers without orders?', pt: '‚úÖ Voc√™ testou left_anti para encontrar clientes sem pedidos?' },
      xpReward: 30,
      estimatedMinutes: 25
    },
    {
      id: 'db-3-7',
      title: { es: 'GroupBy y Agregaciones', en: 'GroupBy and Aggregations', pt: 'GroupBy e Agrega√ß√µes' },
      description: { es: 'Agrupar datos y calcular m√©tricas es fundamental para analytics.', en: 'Grouping data and calculating metrics is fundamental for analytics.', pt: 'Agrupar dados e calcular m√©tricas √© fundamental para analytics.' },
      theory: {
        es: `## GroupBy en Spark

\`\`\`python
from pyspark.sql.functions import *

# GroupBy simple
df.groupBy("categoria").count()

# M√∫ltiples agregaciones
df.groupBy("categoria").agg(
    count("*").alias("total"),
    sum("ventas").alias("ventas_totales"),
    avg("precio").alias("precio_promedio"),
    min("fecha").alias("primera_venta"),
    max("fecha").alias("ultima_venta")
)

# GroupBy m√∫ltiples columnas
df.groupBy("a√±o", "mes").agg(sum("ventas"))

# Agregaci√≥n sin grupo (toda la tabla)
df.agg(sum("ventas"), avg("precio"))

# Pivot (transponer)
df.groupBy("a√±o").pivot("mes").sum("ventas")

# Rollup (subtotales jer√°rquicos)
df.rollup("a√±o", "mes").sum("ventas")

# Cube (todas las combinaciones)
df.cube("a√±o", "mes").sum("ventas")
\`\`\``,
        en: `## GroupBy in Spark

\`\`\`python
from pyspark.sql.functions import *

# Simple GroupBy
df.groupBy("category").count()

# Multiple aggregations
df.groupBy("category").agg(
    count("*").alias("total"),
    sum("sales").alias("total_sales"),
    avg("price").alias("avg_price"),
    min("date").alias("first_sale"),
    max("date").alias("last_sale")
)

# GroupBy multiple columns
df.groupBy("year", "month").agg(sum("sales"))

# Aggregation without group (whole table)
df.agg(sum("sales"), avg("price"))

# Pivot (transpose)
df.groupBy("year").pivot("month").sum("sales")

# Rollup (hierarchical subtotals)
df.rollup("year", "month").sum("sales")

# Cube (all combinations)
df.cube("year", "month").sum("sales")
\`\`\``,
        pt: `## GroupBy no Spark

\`\`\`python
from pyspark.sql.functions import *

# GroupBy simples
df.groupBy("categoria").count()

# M√∫ltiplas agrega√ß√µes
df.groupBy("categoria").agg(
    count("*").alias("total"),
    sum("vendas").alias("vendas_totais"),
    avg("preco").alias("preco_medio"),
    min("data").alias("primeira_venda"),
    max("data").alias("ultima_venda")
)

# GroupBy m√∫ltiplas colunas
df.groupBy("ano", "mes").agg(sum("vendas"))

# Agrega√ß√£o sem grupo (toda a tabela)
df.agg(sum("vendas"), avg("preco"))

# Pivot (transpor)
df.groupBy("ano").pivot("mes").sum("vendas")

# Rollup (subtotais hier√°rquicos)
df.rollup("ano", "mes").sum("vendas")

# Cube (todas as combina√ß√µes)
df.cube("ano", "mes").sum("vendas")
\`\`\``
      },
      practicalTips: [{ es: 'üí° Us√° .alias() para dar nombres legibles a las columnas agregadas.', en: 'üí° Use .alias() to give readable names to aggregated columns.', pt: 'üí° Use .alias() para dar nomes leg√≠veis √†s colunas agregadas.' }],
      externalLinks: [{ title: 'GroupBy API', url: 'https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øCreaste un reporte con m√∫ltiples agregaciones usando agg()?', en: '‚úÖ Did you create a report with multiple aggregations using agg()?', pt: '‚úÖ Voc√™ criou um relat√≥rio com m√∫ltiplas agrega√ß√µes usando agg()?' },
      xpReward: 25,
      estimatedMinutes: 25
    },
    {
      id: 'db-3-8',
      title: { es: 'Window Functions en Spark', en: 'Window Functions in Spark', pt: 'Window Functions no Spark' },
      description: { es: 'Las Window Functions son poderosas para c√°lculos sobre particiones de datos.', en: 'Window Functions are powerful for calculations over data partitions.', pt: 'Window Functions s√£o poderosas para c√°lculos sobre parti√ß√µes de dados.' },
      theory: {
        es: `## Window Functions

\`\`\`python
from pyspark.sql.window import Window
from pyspark.sql.functions import *

# Definir ventana
ventana = Window.partitionBy("cliente_id").orderBy("fecha")

# ROW_NUMBER
df.withColumn("row_num", row_number().over(ventana))

# RANK y DENSE_RANK
df.withColumn("rank", rank().over(ventana))
df.withColumn("dense_rank", dense_rank().over(ventana))

# LAG y LEAD (valor anterior/siguiente)
df.withColumn("venta_anterior", lag("monto", 1).over(ventana))
df.withColumn("venta_siguiente", lead("monto", 1).over(ventana))

# Agregaciones con ventana
ventana_cliente = Window.partitionBy("cliente_id")
df.withColumn("total_cliente", sum("monto").over(ventana_cliente))
df.withColumn("promedio_cliente", avg("monto").over(ventana_cliente))

# Running totals
df.withColumn("acumulado", sum("monto").over(
    Window.partitionBy("cliente_id")
          .orderBy("fecha")
          .rowsBetween(Window.unboundedPreceding, Window.currentRow)
))
\`\`\``,
        en: `## Window Functions

\`\`\`python
from pyspark.sql.window import Window
from pyspark.sql.functions import *

# Define window
window = Window.partitionBy("customer_id").orderBy("date")

# ROW_NUMBER
df.withColumn("row_num", row_number().over(window))

# RANK and DENSE_RANK
df.withColumn("rank", rank().over(window))
df.withColumn("dense_rank", dense_rank().over(window))

# LAG and LEAD (previous/next value)
df.withColumn("prev_sale", lag("amount", 1).over(window))
df.withColumn("next_sale", lead("amount", 1).over(window))

# Aggregations with window
customer_window = Window.partitionBy("customer_id")
df.withColumn("customer_total", sum("amount").over(customer_window))
df.withColumn("customer_avg", avg("amount").over(customer_window))

# Running totals
df.withColumn("cumulative", sum("amount").over(
    Window.partitionBy("customer_id")
          .orderBy("date")
          .rowsBetween(Window.unboundedPreceding, Window.currentRow)
))
\`\`\``,
        pt: `## Window Functions

\`\`\`python
from pyspark.sql.window import Window
from pyspark.sql.functions import *

# Definir janela
janela = Window.partitionBy("cliente_id").orderBy("data")

# ROW_NUMBER
df.withColumn("row_num", row_number().over(janela))

# RANK e DENSE_RANK
df.withColumn("rank", rank().over(janela))
df.withColumn("dense_rank", dense_rank().over(janela))

# LAG e LEAD (valor anterior/pr√≥ximo)
df.withColumn("venda_anterior", lag("valor", 1).over(janela))
df.withColumn("venda_proxima", lead("valor", 1).over(janela))

# Agrega√ß√µes com janela
janela_cliente = Window.partitionBy("cliente_id")
df.withColumn("total_cliente", sum("valor").over(janela_cliente))
df.withColumn("media_cliente", avg("valor").over(janela_cliente))

# Running totals
df.withColumn("acumulado", sum("valor").over(
    Window.partitionBy("cliente_id")
          .orderBy("data")
          .rowsBetween(Window.unboundedPreceding, Window.currentRow)
))
\`\`\``
      },
      practicalTips: [{ es: '‚≠ê Window Functions son preguntas comunes en entrevistas t√©cnicas.', en: '‚≠ê Window Functions are common interview questions.', pt: '‚≠ê Window Functions s√£o perguntas comuns em entrevistas t√©cnicas.' }],
      externalLinks: [{ title: 'Window Functions', url: 'https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øCalculaste un running total usando Window?', en: '‚úÖ Did you calculate a running total using Window?', pt: '‚úÖ Voc√™ calculou um running total usando Window?' },
      xpReward: 35,
      estimatedMinutes: 30
    },
    {
      id: 'db-3-9',
      title: { es: 'Leer y Escribir Datos', en: 'Reading and Writing Data', pt: 'Ler e Escrever Dados' },
      description: { es: 'Domin√° todos los formatos de entrada/salida de Spark.', en: 'Master all Spark input/output formats.', pt: 'Domine todos os formatos de entrada/sa√≠da do Spark.' },
      theory: {
        es: `## I/O en Spark

### Leer datos:
\`\`\`python
# CSV
df = spark.read.csv("path", header=True, inferSchema=True)
df = spark.read.option("header", "true").csv("path")

# Parquet (recomendado)
df = spark.read.parquet("path")

# JSON
df = spark.read.json("path")

# Delta (Databricks)
df = spark.read.format("delta").load("path")

# Tabla
df = spark.table("database.table")
\`\`\`

### Escribir datos:
\`\`\`python
# Modos: overwrite, append, ignore, error
df.write.mode("overwrite").parquet("path")
df.write.mode("append").parquet("path")

# Particionado (importante para performance!)
df.write.partitionBy("a√±o", "mes").parquet("path")

# Como tabla
df.write.saveAsTable("database.table")

# Delta
df.write.format("delta").save("path")
\`\`\`

### Opciones comunes:
\`\`\`python
spark.read.options(
    header="true",
    inferSchema="true",
    delimiter=";",
    encoding="UTF-8"
).csv("path")
\`\`\``,
        en: `## I/O in Spark

### Read data:
\`\`\`python
# CSV
df = spark.read.csv("path", header=True, inferSchema=True)
df = spark.read.option("header", "true").csv("path")

# Parquet (recommended)
df = spark.read.parquet("path")

# JSON
df = spark.read.json("path")

# Delta (Databricks)
df = spark.read.format("delta").load("path")

# Table
df = spark.table("database.table")
\`\`\`

### Write data:
\`\`\`python
# Modes: overwrite, append, ignore, error
df.write.mode("overwrite").parquet("path")
df.write.mode("append").parquet("path")

# Partitioned (important for performance!)
df.write.partitionBy("year", "month").parquet("path")

# As table
df.write.saveAsTable("database.table")

# Delta
df.write.format("delta").save("path")
\`\`\`

### Common options:
\`\`\`python
spark.read.options(
    header="true",
    inferSchema="true",
    delimiter=";",
    encoding="UTF-8"
).csv("path")
\`\`\``,
        pt: `## I/O no Spark

### Ler dados:
\`\`\`python
# CSV
df = spark.read.csv("path", header=True, inferSchema=True)
df = spark.read.option("header", "true").csv("path")

# Parquet (recomendado)
df = spark.read.parquet("path")

# JSON
df = spark.read.json("path")

# Delta (Databricks)
df = spark.read.format("delta").load("path")

# Tabela
df = spark.table("database.table")
\`\`\`

### Escrever dados:
\`\`\`python
# Modos: overwrite, append, ignore, error
df.write.mode("overwrite").parquet("path")
df.write.mode("append").parquet("path")

# Particionado (importante para performance!)
df.write.partitionBy("ano", "mes").parquet("path")

# Como tabela
df.write.saveAsTable("database.table")

# Delta
df.write.format("delta").save("path")
\`\`\`

### Op√ß√µes comuns:
\`\`\`python
spark.read.options(
    header="true",
    inferSchema="true",
    delimiter=";",
    encoding="UTF-8"
).csv("path")
\`\`\``
      },
      practicalTips: [{ es: 'üí° Siempre us√° Parquet o Delta en producci√≥n. CSV es solo para importar/exportar.', en: 'üí° Always use Parquet or Delta in production. CSV is only for import/export.', pt: 'üí° Sempre use Parquet ou Delta em produ√ß√£o. CSV √© s√≥ para importar/exportar.' }],
      externalLinks: [{ title: 'Data Sources', url: 'https://spark.apache.org/docs/latest/sql-data-sources.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øGuardaste un DataFrame particionado por fecha?', en: '‚úÖ Did you save a DataFrame partitioned by date?', pt: '‚úÖ Voc√™ salvou um DataFrame particionado por data?' },
      xpReward: 25,
      estimatedMinutes: 25
    },
    {
      id: 'db-3-10',
      title: { es: 'Proyecto: ETL Completo con Spark', en: 'Project: Complete ETL with Spark', pt: 'Projeto: ETL Completo com Spark' },
      description: { es: 'Aplic√° todo lo aprendido en un pipeline ETL de principio a fin.', en: 'Apply everything learned in an end-to-end ETL pipeline.', pt: 'Aplique tudo o que aprendeu em um pipeline ETL de ponta a ponta.' },
      theory: {
        es: `## Proyecto: ETL de Ventas

Objetivo: Procesar datos de ventas y generar reportes.

### Estructura del proyecto:
\`\`\`python
# 1. EXTRACT
ventas_raw = spark.read.csv("/databricks-datasets/...")
productos = spark.read.json("...")

# 2. TRANSFORM
# - Limpiar datos
ventas_clean = ventas_raw.dropna()

# - Joins
ventas_enriquecidas = ventas_clean.join(
    productos, "producto_id", "left"
)

# - Agregaciones
resumen_diario = ventas_enriquecidas.groupBy("fecha").agg(
    sum("monto").alias("ventas_totales"),
    countDistinct("cliente_id").alias("clientes_unicos")
)

# - Window functions
from pyspark.sql.window import Window
w = Window.orderBy("fecha")
resumen_diario = resumen_diario.withColumn(
    "ventas_7d", avg("ventas_totales").over(
        w.rowsBetween(-6, 0)
    )
)

# 3. LOAD
resumen_diario.write.mode("overwrite").saveAsTable("analytics.ventas_diarias")
\`\`\`

### Checklist:
- [ ] Cargar al menos 2 fuentes de datos
- [ ] Realizar limpieza de datos
- [ ] Hacer join entre tablas
- [ ] Crear agregaciones
- [ ] Usar al menos 1 window function
- [ ] Guardar resultado como tabla`,
        en: `## Project: Sales ETL

Objective: Process sales data and generate reports.

### Project structure:
\`\`\`python
# 1. EXTRACT
sales_raw = spark.read.csv("/databricks-datasets/...")
products = spark.read.json("...")

# 2. TRANSFORM
# - Clean data
sales_clean = sales_raw.dropna()

# - Joins
enriched_sales = sales_clean.join(
    products, "product_id", "left"
)

# - Aggregations
daily_summary = enriched_sales.groupBy("date").agg(
    sum("amount").alias("total_sales"),
    countDistinct("customer_id").alias("unique_customers")
)

# - Window functions
from pyspark.sql.window import Window
w = Window.orderBy("date")
daily_summary = daily_summary.withColumn(
    "sales_7d", avg("total_sales").over(
        w.rowsBetween(-6, 0)
    )
)

# 3. LOAD
daily_summary.write.mode("overwrite").saveAsTable("analytics.daily_sales")
\`\`\`

### Checklist:
- [ ] Load at least 2 data sources
- [ ] Perform data cleaning
- [ ] Join tables
- [ ] Create aggregations
- [ ] Use at least 1 window function
- [ ] Save result as table`,
        pt: `## Projeto: ETL de Vendas

Objetivo: Processar dados de vendas e gerar relat√≥rios.

### Estrutura do projeto:
\`\`\`python
# 1. EXTRACT
vendas_raw = spark.read.csv("/databricks-datasets/...")
produtos = spark.read.json("...")

# 2. TRANSFORM
# - Limpar dados
vendas_clean = vendas_raw.dropna()

# - Joins
vendas_enriquecidas = vendas_clean.join(
    produtos, "produto_id", "left"
)

# - Agrega√ß√µes
resumo_diario = vendas_enriquecidas.groupBy("data").agg(
    sum("valor").alias("vendas_totais"),
    countDistinct("cliente_id").alias("clientes_unicos")
)

# - Window functions
from pyspark.sql.window import Window
w = Window.orderBy("data")
resumo_diario = resumo_diario.withColumn(
    "vendas_7d", avg("vendas_totais").over(
        w.rowsBetween(-6, 0)
    )
)

# 3. LOAD
resumo_diario.write.mode("overwrite").saveAsTable("analytics.vendas_diarias")
\`\`\`

### Checklist:
- [ ] Carregar pelo menos 2 fontes de dados
- [ ] Realizar limpeza de dados
- [ ] Fazer join entre tabelas
- [ ] Criar agrega√ß√µes
- [ ] Usar pelo menos 1 window function
- [ ] Salvar resultado como tabela`
      },
      practicalTips: [{ es: 'üìì Este proyecto es perfecto para tu portfolio. Documentalo bien!', en: 'üìì This project is perfect for your portfolio. Document it well!', pt: 'üìì Este projeto √© perfeito para seu portf√≥lio. Documente bem!' }],
      externalLinks: [{ title: 'ETL Best Practices', url: 'https://docs.databricks.com/data-engineering/index.html', type: 'docs' }],
      checkpoint: { es: 'üèÜ ¬øCompletaste el ETL con todas las transformaciones?', en: 'üèÜ Did you complete the ETL with all transformations?', pt: 'üèÜ Voc√™ completou o ETL com todas as transforma√ß√µes?' },
      xpReward: 75,
      estimatedMinutes: 60
    }
  ]
};


