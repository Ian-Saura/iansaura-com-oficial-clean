/**
 * FASE 5: Delta Lake
 * 10 pasos para dominar Delta Lake - El formato que revolucion√≥ los Data Lakes
 */

import { DatabricksPhase } from '../types';

export const PHASE_5_DELTA_LAKE: DatabricksPhase = {
  id: 'db-phase-5',
  number: 5,
  title: { es: 'Delta Lake', en: 'Delta Lake', pt: 'Delta Lake' },
  subtitle: { es: 'El formato de datos m√°s poderoso', en: 'The most powerful data format', pt: 'O formato de dados mais poderoso' },
  description: { es: 'Domina Delta Lake: ACID transactions, time travel, schema enforcement, MERGE, OPTIMIZE, y Z-ordering. Delta Lake es la tecnolog√≠a core de Databricks y lo que hace posible el Lakehouse.', en: 'Master Delta Lake: ACID transactions, time travel, schema enforcement, MERGE, OPTIMIZE, and Z-ordering. Delta Lake is the core technology of Databricks and what makes the Lakehouse possible.', pt: 'Domine Delta Lake: transa√ß√µes ACID, time travel, schema enforcement, MERGE, OPTIMIZE e Z-ordering. Delta Lake √© a tecnologia core do Databricks e o que torna poss√≠vel o Lakehouse.' },
  icon: 'üî∑',
  color: 'blue',
  estimatedDays: '5-7 d√≠as',
  steps: [
    {
      id: 'db-5-1',
      title: { es: '¬øQu√© es Delta Lake?', en: 'What is Delta Lake?', pt: 'O que √© Delta Lake?' },
      description: { es: 'Introducci√≥n completa a Delta Lake y por qu√© revolucion√≥ los Data Lakes.', en: 'Complete introduction to Delta Lake and why it revolutionized Data Lakes.', pt: 'Introdu√ß√£o completa ao Delta Lake e por que revolucionou os Data Lakes.' },
      theory: {
        es: `## Delta Lake: El Formato que Cambi√≥ Todo

Delta Lake es un **formato de almacenamiento open-source** creado por Databricks que convierte tu data lake en un lakehouse confiable. Es la tecnolog√≠a que hace posible tener la flexibilidad de un data lake con las garant√≠as de un data warehouse.

### El Problema que Resuelve

**Antes de Delta Lake (Data Lake tradicional):**
\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA LAKE (Parquet)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚ùå No hay transacciones ACID                               ‚îÇ
‚îÇ    ‚Üí Datos corruptos si falla a mitad de escritura         ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚ùå No hay schema enforcement                               ‚îÇ
‚îÇ    ‚Üí Cualquiera puede escribir cualquier cosa              ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚ùå No hay historial de cambios                             ‚îÇ
‚îÇ    ‚Üí "¬øQu√© datos hab√≠a ayer?" ‚Üí No se sabe                 ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚ùå No hay updates ni deletes eficientes                    ‚îÇ
‚îÇ    ‚Üí Para cambiar 1 fila hay que reescribir toda la tabla  ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚ùå Problema de "small files"                               ‚îÇ
‚îÇ    ‚Üí Miles de archivos peque√±os = queries lentas           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

**Con Delta Lake:**
\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      DELTA LAKE                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚úÖ Transacciones ACID                                      ‚îÇ
‚îÇ    ‚Üí Escrituras at√≥micas, todo o nada                      ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚úÖ Schema Enforcement                                      ‚îÇ
‚îÇ    ‚Üí Rechaza datos que no cumplen el schema                ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚úÖ Time Travel (historial de versiones)                    ‚îÇ
‚îÇ    ‚Üí Viaja a cualquier versi√≥n anterior                    ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚úÖ MERGE (upserts eficientes)                              ‚îÇ
‚îÇ    ‚Üí UPDATE, DELETE, INSERT en una operaci√≥n               ‚îÇ
‚îÇ                                                            ‚îÇ
‚îÇ ‚úÖ OPTIMIZE + Z-ORDER                                      ‚îÇ
‚îÇ    ‚Üí Compacta archivos y optimiza queries                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### ¬øC√≥mo funciona Delta Lake?

Delta Lake usa Parquet para almacenar datos + un **transaction log** (carpeta \`_delta_log/\`):

\`\`\`
mi_tabla_delta/
‚îú‚îÄ‚îÄ _delta_log/                    # Transaction Log
‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000000.json  # Versi√≥n 0
‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000001.json  # Versi√≥n 1
‚îÇ   ‚îú‚îÄ‚îÄ 00000000000000000002.json  # Versi√≥n 2
‚îÇ   ‚îî‚îÄ‚îÄ ...                        # M√°s versiones
‚îú‚îÄ‚îÄ part-00000-xxxxx.parquet       # Datos
‚îú‚îÄ‚îÄ part-00001-xxxxx.parquet       # Datos
‚îî‚îÄ‚îÄ part-00002-xxxxx.parquet       # Datos
\`\`\`

El **transaction log** registra:
- Qu√© archivos agregar/eliminar
- Schema de la tabla
- Estad√≠sticas para optimizaci√≥n
- Qui√©n hizo el cambio y cu√°ndo

### Delta Lake vs Parquet vs CSV

| Caracter√≠stica | CSV | Parquet | Delta Lake |
|----------------|-----|---------|------------|
| Transacciones ACID | ‚ùå | ‚ùå | ‚úÖ |
| Schema enforcement | ‚ùå | ‚ö†Ô∏è Parcial | ‚úÖ |
| Time travel | ‚ùå | ‚ùå | ‚úÖ |
| UPDATE/DELETE | ‚ùå | ‚ùå | ‚úÖ |
| Compresi√≥n | ‚ùå | ‚úÖ | ‚úÖ |
| Columnar | ‚ùå | ‚úÖ | ‚úÖ |
| Data skipping | ‚ùå | ‚ö†Ô∏è B√°sico | ‚úÖ Avanzado |

### ¬øPor qu√© Delta Lake es CR√çTICO para tu carrera?

1. **Es el est√°ndar de facto**: Databricks, AWS, Azure, GCP todos soportan Delta
2. **Pregunta de entrevista**: "¬øPor qu√© usar√≠as Delta Lake?" es muy com√∫n
3. **Certificaci√≥n**: 30-40% del examen de Databricks es sobre Delta Lake
4. **Diferenciador**: Saber Delta te separa de otros candidatos`,
        en: `## Delta Lake: The Format that Changed Everything

Delta Lake is an **open-source storage format** created by Databricks that converts your data lake into a reliable lakehouse. It's the technology that makes it possible to have the flexibility of a data lake with the guarantees of a data warehouse.

### The Problem it Solves

**Before Delta Lake (traditional Data Lake):**
- ‚ùå No ACID transactions ‚Üí Corrupt data if write fails mid-way
- ‚ùå No schema enforcement ‚Üí Anyone can write anything
- ‚ùå No change history ‚Üí "What data was there yesterday?" ‚Üí Unknown
- ‚ùå No efficient updates/deletes ‚Üí To change 1 row you rewrite entire table
- ‚ùå "Small files" problem ‚Üí Thousands of small files = slow queries

**With Delta Lake:**
- ‚úÖ ACID Transactions ‚Üí Atomic writes, all or nothing
- ‚úÖ Schema Enforcement ‚Üí Rejects data that doesn't match schema
- ‚úÖ Time Travel ‚Üí Go to any previous version
- ‚úÖ MERGE (efficient upserts) ‚Üí UPDATE, DELETE, INSERT in one operation
- ‚úÖ OPTIMIZE + Z-ORDER ‚Üí Compacts files and optimizes queries

### Why Delta Lake is CRITICAL for your career?

1. **It's the de facto standard**: Databricks, AWS, Azure, GCP all support Delta
2. **Interview question**: "Why would you use Delta Lake?" is very common
3. **Certification**: 30-40% of Databricks exam is about Delta Lake
4. **Differentiator**: Knowing Delta sets you apart from other candidates`,
        pt: `## Delta Lake: O Formato que Mudou Tudo

Delta Lake √© um **formato de armazenamento open-source** criado pelo Databricks que converte seu data lake em um lakehouse confi√°vel.

### O Problema que Resolve

**Antes do Delta Lake:**
- ‚ùå Sem transa√ß√µes ACID ‚Üí Dados corrompidos se falhar no meio
- ‚ùå Sem schema enforcement ‚Üí Qualquer um escreve qualquer coisa
- ‚ùå Sem hist√≥rico ‚Üí "Que dados tinha ontem?" ‚Üí N√£o se sabe
- ‚ùå Sem updates/deletes eficientes
- ‚ùå Problema de "small files"

**Com Delta Lake:**
- ‚úÖ Transa√ß√µes ACID
- ‚úÖ Schema Enforcement
- ‚úÖ Time Travel
- ‚úÖ MERGE (upserts eficientes)
- ‚úÖ OPTIMIZE + Z-ORDER

### Por que Delta Lake √© CR√çTICO para sua carreira?

1. √â o padr√£o de facto
2. Pergunta comum em entrevistas
3. 30-40% do exame de certifica√ß√£o
4. Te diferencia de outros candidatos`
      },
      practicalTips: [
        { es: 'üí° Delta Lake es open source (delta.io), pero Databricks tiene optimizaciones exclusivas como Photon y Liquid Clustering.', en: 'üí° Delta Lake is open source (delta.io), but Databricks has exclusive optimizations like Photon and Liquid Clustering.', pt: 'üí° Delta Lake √© open source (delta.io), mas Databricks tem otimiza√ß√µes exclusivas como Photon e Liquid Clustering.' },
        { es: 'üéØ En Databricks, Delta es el formato DEFAULT. No necesit√°s especificarlo.', en: 'üéØ In Databricks, Delta is the DEFAULT format. You don\'t need to specify it.', pt: 'üéØ No Databricks, Delta √© o formato DEFAULT. Voc√™ n√£o precisa especific√°-lo.' },
        { es: 'üìö Memoriz√° esto: "Delta = Parquet + Transaction Log + ACID"', en: 'üìö Memorize this: "Delta = Parquet + Transaction Log + ACID"', pt: 'üìö Memorize isso: "Delta = Parquet + Transaction Log + ACID"' }
      ],
      externalLinks: [
        { title: 'Delta Lake Official Site', url: 'https://delta.io/', type: 'docs' },
        { title: 'Delta Lake Paper (original)', url: 'https://www.vldb.org/pvldb/vol13/p3411-armbrust.pdf', type: 'article' },
        { title: 'What is Delta Lake? (Databricks)', url: 'https://docs.databricks.com/delta/index.html', type: 'docs' }
      ],
      checkpoint: { es: 'ü§î Explic√° en tus palabras: ¬øPor qu√© Delta Lake es mejor que Parquet para un data warehouse moderno?', en: 'ü§î Explain in your words: Why is Delta Lake better than Parquet for a modern data warehouse?', pt: 'ü§î Explique com suas palavras: Por que Delta Lake √© melhor que Parquet para um data warehouse moderno?' },
      xpReward: 25,
      estimatedMinutes: 30
    },
    {
      id: 'db-5-2',
      title: { es: 'Crear y Escribir Tablas Delta', en: 'Create and Write Delta Tables', pt: 'Criar e Escrever Tabelas Delta' },
      description: { es: 'Todas las formas de crear tablas Delta: desde DataFrames, SQL, y conversi√≥n de Parquet.', en: 'All the ways to create Delta tables: from DataFrames, SQL, and Parquet conversion.', pt: 'Todas as formas de criar tabelas Delta: de DataFrames, SQL e convers√£o de Parquet.' },
      theory: {
        es: `## Crear Tablas Delta

Hay m√∫ltiples formas de crear una tabla Delta. Vamos a ver todas:

### M√©todo 1: Desde un DataFrame (m√°s com√∫n)

\`\`\`python
# Crear un DataFrame
data = [
    (1, "Ana", 1000.50, "2024-01-15"),
    (2, "Bob", 2500.00, "2024-01-16"),
    (3, "Carlos", 1750.25, "2024-01-17")
]
df = spark.createDataFrame(data, ["id", "nombre", "monto", "fecha"])

# M√âTODO 1A: Guardar como tabla Delta (managed table)
df.write.format("delta").saveAsTable("ventas")

# M√âTODO 1B: Guardar en una ubicaci√≥n espec√≠fica (external table)
df.write.format("delta").save("/mnt/data/ventas_delta")

# M√âTODO 1C: Con opciones adicionales
df.write \\
    .format("delta") \\
    .mode("overwrite") \\  # overwrite, append, ignore, error
    .partitionBy("fecha") \\
    .option("overwriteSchema", "true") \\
    .save("/mnt/data/ventas_particionada")
\`\`\`

### M√©todo 2: Usando SQL

\`\`\`sql
-- Crear tabla vac√≠a con schema
CREATE TABLE ventas (
    id INT,
    nombre STRING,
    monto DOUBLE,
    fecha DATE
) USING DELTA
PARTITIONED BY (fecha);

-- Crear tabla desde SELECT
CREATE TABLE ventas_2024 AS
SELECT * FROM ventas WHERE fecha >= '2024-01-01';

-- Crear tabla en ubicaci√≥n espec√≠fica
CREATE TABLE ventas_externa
USING DELTA
LOCATION '/mnt/data/ventas_externa';

-- Insertar datos
INSERT INTO ventas VALUES (4, 'Diana', 3000.00, '2024-01-18');
\`\`\`

### M√©todo 3: Convertir Parquet existente a Delta

\`\`\`python
# Si ya ten√©s datos en Parquet, convertirlos es f√°cil:
from delta.tables import DeltaTable

# Convertir in-place (no copia datos, solo agrega _delta_log)
DeltaTable.convertToDelta(spark, "parquet.\`/mnt/data/mis_datos_parquet\`")

# O con SQL:
# CONVERT TO DELTA parquet.\`/mnt/data/mis_datos_parquet\`
\`\`\`

### Modos de Escritura

| Modo | Comportamiento |
|------|----------------|
| \`overwrite\` | Reemplaza toda la tabla |
| \`append\` | Agrega filas al final |
| \`ignore\` | No hace nada si la tabla existe |
| \`error\` (default) | Error si la tabla existe |

### Verificar que es Delta

\`\`\`python
# Listar archivos - deb√©s ver _delta_log/
dbutils.fs.ls("/mnt/data/ventas_delta")

# Ver historial de versiones
spark.sql("DESCRIBE HISTORY ventas").show()

# Ver detalles de la tabla
spark.sql("DESCRIBE DETAIL ventas").show()
\`\`\`

### Estructura de archivos Delta

\`\`\`
/mnt/data/ventas_delta/
‚îú‚îÄ‚îÄ _delta_log/
‚îÇ   ‚îî‚îÄ‚îÄ 00000000000000000000.json   # Primera versi√≥n (metadata)
‚îú‚îÄ‚îÄ part-00000-xxx.snappy.parquet   # Datos comprimidos
‚îú‚îÄ‚îÄ part-00001-xxx.snappy.parquet
‚îî‚îÄ‚îÄ part-00002-xxx.snappy.parquet
\`\`\``,
        en: `## Create Delta Tables

There are multiple ways to create a Delta table. Let's see them all:

### Method 1: From a DataFrame (most common)

\`\`\`python
# Create a DataFrame
data = [(1, "Ana", 1000.50), (2, "Bob", 2500.00)]
df = spark.createDataFrame(data, ["id", "name", "amount"])

# Save as Delta table (managed)
df.write.format("delta").saveAsTable("sales")

# Save to specific location (external)
df.write.format("delta").save("/mnt/data/sales_delta")

# With additional options
df.write \\
    .format("delta") \\
    .mode("overwrite") \\
    .partitionBy("date") \\
    .save("/mnt/data/sales_partitioned")
\`\`\`

### Method 2: Using SQL

\`\`\`sql
-- Create empty table with schema
CREATE TABLE sales (id INT, name STRING, amount DOUBLE)
USING DELTA;

-- Create from SELECT
CREATE TABLE sales_2024 AS SELECT * FROM sales;
\`\`\`

### Method 3: Convert existing Parquet to Delta

\`\`\`python
from delta.tables import DeltaTable
DeltaTable.convertToDelta(spark, "parquet.\`/path/to/parquet\`")
\`\`\`

### Write Modes

| Mode | Behavior |
|------|----------|
| overwrite | Replace entire table |
| append | Add rows at the end |
| ignore | Do nothing if table exists |
| error | Error if table exists |`,
        pt: `## Criar Tabelas Delta

### M√©todo 1: De um DataFrame

\`\`\`python
df.write.format("delta").saveAsTable("vendas")
\`\`\`

### M√©todo 2: Usando SQL

\`\`\`sql
CREATE TABLE vendas (id INT, nome STRING) USING DELTA;
\`\`\`

### M√©todo 3: Converter Parquet existente

\`\`\`python
DeltaTable.convertToDelta(spark, "parquet.\`/path/to/parquet\`")
\`\`\``
      },
      codeExample: {
        language: 'python',
        code: `# Ejemplo completo: Crear tabla Delta
from pyspark.sql.types import *

# 1. Definir schema
schema = StructType([
    StructField("id", IntegerType(), False),
    StructField("producto", StringType(), True),
    StructField("precio", DoubleType(), True),
    StructField("categoria", StringType(), True),
    StructField("fecha_venta", DateType(), True)
])

# 2. Crear datos de ejemplo
data = [
    (1, "Laptop", 999.99, "Electronics", "2024-01-15"),
    (2, "Mouse", 29.99, "Electronics", "2024-01-15"),
    (3, "Notebook", 5.99, "Office", "2024-01-16"),
]
df = spark.createDataFrame(data, schema)

# 3. Guardar como tabla Delta particionada
df.write \\
    .format("delta") \\
    .mode("overwrite") \\
    .partitionBy("categoria") \\
    .saveAsTable("productos_delta")

# 4. Verificar
display(spark.sql("DESCRIBE DETAIL productos_delta"))
display(spark.sql("DESCRIBE HISTORY productos_delta"))`,
        explanation: { es: 'Este ejemplo crea una tabla Delta particionada por categor√≠a, lo cual optimiza queries que filtran por categor√≠a.', en: 'This example creates a Delta table partitioned by category, which optimizes queries filtering by category.', pt: 'Este exemplo cria uma tabela Delta particionada por categoria, otimizando queries que filtram por categoria.' }
      },
      practicalTips: [
        { es: '‚ö° En Databricks, no necesit√°s escribir .format("delta") - es el default.', en: '‚ö° In Databricks, you don\'t need to write .format("delta") - it\'s the default.', pt: '‚ö° No Databricks, n√£o precisa escrever .format("delta") - √© o padr√£o.' },
        { es: 'üìÅ La carpeta _delta_log es sagrada. NUNCA la borres manualmente.', en: 'üìÅ The _delta_log folder is sacred. NEVER delete it manually.', pt: 'üìÅ A pasta _delta_log √© sagrada. NUNCA a delete manualmente.' },
        { es: 'üí° Us√° partitionBy() solo si ten√©s muchos datos y queries frecuentes por esa columna.', en: 'üí° Use partitionBy() only if you have lots of data and frequent queries by that column.', pt: 'üí° Use partitionBy() apenas se tiver muitos dados e queries frequentes por essa coluna.' }
      ],
      externalLinks: [
        { title: 'Create Delta Tables', url: 'https://docs.databricks.com/delta/delta-batch.html#create-a-table', type: 'docs' },
        { title: 'Delta Table Properties', url: 'https://docs.databricks.com/delta/table-properties.html', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øCreaste una tabla Delta y verificaste que existe la carpeta _delta_log?', en: '‚úÖ Did you create a Delta table and verify the _delta_log folder exists?', pt: '‚úÖ Voc√™ criou uma tabela Delta e verificou que existe a pasta _delta_log?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-5-2b',
      title: { es: 'COPY INTO: Carga Idempotente de Datos', en: 'COPY INTO: Idempotent Data Loading', pt: 'COPY INTO: Carga Idempotente de Dados' },
      description: { es: 'COPY INTO es el comando preferido para cargar datos de forma segura y sin duplicados.', en: 'COPY INTO is the preferred command for loading data safely and without duplicates.', pt: 'COPY INTO √© o comando preferido para carregar dados de forma segura e sem duplicados.' },
      theory: {
        es: `## COPY INTO: Carga de Datos Sin Duplicados

COPY INTO es un comando SQL de Databricks que carga datos de forma **idempotente** - si lo ejecutas 2 veces con los mismos archivos, no hay duplicados.

### ¬øPor qu√© COPY INTO?

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         COMPARACI√ìN DE M√âTODOS DE CARGA                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  INSERT INTO / spark.write.mode("append"):                  ‚îÇ
‚îÇ  ‚ùå Si ejecutas 2 veces = DUPLICADOS                        ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  COPY INTO:                                                  ‚îÇ
‚îÇ  ‚úÖ Si ejecutas 2 veces = MISMO RESULTADO (idempotente)     ‚îÇ
‚îÇ  ‚úÖ Trackea qu√© archivos ya se cargaron                     ‚îÇ
‚îÇ  ‚úÖ M√°s eficiente que Auto Loader para cargas puntuales     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Sintaxis B√°sica

\`\`\`sql
-- Cargar CSVs a tabla Delta
COPY INTO my_catalog.my_schema.my_table
FROM 's3://my-bucket/data/incoming/'
FILEFORMAT = CSV
FORMAT_OPTIONS (
    'header' = 'true',
    'delimiter' = ','
)
COPY_OPTIONS (
    'mergeSchema' = 'true'
);
\`\`\`

### Formatos Soportados

\`\`\`sql
-- JSON
COPY INTO target_table
FROM '/data/json/'
FILEFORMAT = JSON
FORMAT_OPTIONS ('multiLine' = 'true');

-- PARQUET
COPY INTO target_table
FROM '/data/parquet/'
FILEFORMAT = PARQUET;

-- AVRO
COPY INTO target_table
FROM '/data/avro/'
FILEFORMAT = AVRO;

-- CSV con opciones
COPY INTO target_table
FROM '/data/csv/'
FILEFORMAT = CSV
FORMAT_OPTIONS (
    'header' = 'true',
    'delimiter' = '|',
    'quote' = '"',
    'escape' = '\\\\',
    'nullValue' = 'NA',
    'dateFormat' = 'yyyy-MM-dd'
);
\`\`\`

### COPY INTO con Transformaciones

\`\`\`sql
-- Seleccionar y transformar columnas
COPY INTO bronze.sales
FROM (
    SELECT 
        _c0::INT as sale_id,
        _c1::STRING as product,
        _c2::DOUBLE as amount,
        _c3::DATE as sale_date,
        current_timestamp() as ingestion_time
    FROM 's3://bucket/raw/sales/'
)
FILEFORMAT = CSV
FORMAT_OPTIONS ('header' = 'false');

-- Filtrar archivos por patr√≥n
COPY INTO bronze.logs
FROM 's3://bucket/logs/'
FILEFORMAT = JSON
PATTERN = '*.json'  -- Solo archivos .json
COPY_OPTIONS ('force' = 'false');
\`\`\`

### Opciones Importantes

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| \`mergeSchema\` | Evolucionar schema autom√°ticamente |
| \`force\` | true = re-cargar archivos ya cargados |
| \`PATTERN\` | Filtrar archivos por regex |

### COPY INTO vs Auto Loader

| Feature | COPY INTO | Auto Loader |
|---------|-----------|-------------|
| Tipo | SQL command | Streaming |
| Uso | Cargas puntuales/scheduled | Continuo |
| Schema inference | Manual o inferSchema | Autom√°tico |
| Tracking | Por comando | Checkpoint |
| Mejor para | Batch ETL, migraciones | Streaming, pipelines |

### Ejemplo Completo: Pipeline de Carga

\`\`\`sql
-- 1. Crear tabla destino
CREATE TABLE IF NOT EXISTS bronze.transactions (
    transaction_id BIGINT,
    customer_id BIGINT,
    amount DOUBLE,
    currency STRING,
    transaction_date DATE,
    _ingestion_time TIMESTAMP
);

-- 2. Cargar datos (idempotente)
COPY INTO bronze.transactions
FROM (
    SELECT 
        transaction_id::BIGINT,
        customer_id::BIGINT,
        amount::DOUBLE,
        currency::STRING,
        transaction_date::DATE,
        current_timestamp() as _ingestion_time
    FROM 's3://raw-data/transactions/2024/01/'
)
FILEFORMAT = CSV
FORMAT_OPTIONS ('header' = 'true')
COPY_OPTIONS ('mergeSchema' = 'true');

-- 3. Verificar carga
SELECT COUNT(*) as rows_loaded FROM bronze.transactions;
\`\`\`

### Python API

\`\`\`python
# COPY INTO desde Python
spark.sql("""
    COPY INTO bronze.events
    FROM 's3://bucket/events/'
    FILEFORMAT = JSON
""")

# Verificar
df = spark.table("bronze.events")
print(f"Total rows: {df.count()}")
\`\`\``,
        en: `## COPY INTO: Idempotent Data Loading

COPY INTO is a Databricks SQL command that loads data **idempotently** - if you run it twice with the same files, there are no duplicates.

\`\`\`sql
COPY INTO my_table
FROM 's3://bucket/data/'
FILEFORMAT = CSV
FORMAT_OPTIONS ('header' = 'true')
COPY_OPTIONS ('mergeSchema' = 'true');
\`\`\`

### Key Benefits
- Idempotent (no duplicates)
- Tracks loaded files
- Schema evolution support`,
        pt: `## COPY INTO: Carga Idempotente de Dados

COPY INTO √© um comando SQL do Databricks que carrega dados de forma **idempotente** - se voc√™ executar 2 vezes com os mesmos arquivos, n√£o h√° duplicados.

\`\`\`sql
COPY INTO minha_tabela
FROM 's3://bucket/dados/'
FILEFORMAT = CSV
FORMAT_OPTIONS ('header' = 'true')
COPY_OPTIONS ('mergeSchema' = 'true');
\`\`\``
      },
      practicalTips: [
        { es: '‚≠ê COPY INTO es pregunta SEGURA en el examen de certificaci√≥n. Sab√© la sintaxis de memoria.', en: '‚≠ê COPY INTO is a SURE question on the certification exam. Know the syntax by heart.', pt: '‚≠ê COPY INTO √© pergunta CERTA no exame de certifica√ß√£o. Saiba a sintaxe de cor.' },
        { es: 'üîÑ force=true re-carga archivos ya procesados. √ösalo solo si necesitas reprocesar.', en: 'üîÑ force=true reloads already processed files. Use only if you need to reprocess.', pt: 'üîÑ force=true re-carrega arquivos j√° processados. Use s√≥ se precisar reprocessar.' },
        { es: 'üí° Para streaming continuo, usa Auto Loader. Para cargas batch/scheduled, usa COPY INTO.', en: 'üí° For continuous streaming, use Auto Loader. For batch/scheduled loads, use COPY INTO.', pt: 'üí° Para streaming cont√≠nuo, use Auto Loader. Para cargas batch/scheduled, use COPY INTO.' }
      ],
      externalLinks: [
        { title: 'COPY INTO', url: 'https://docs.databricks.com/sql/language-manual/delta-copy-into.html', type: 'docs' },
        { title: 'COPY INTO vs Auto Loader', url: 'https://docs.databricks.com/ingestion/copy-into/index.html', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øCargaste datos con COPY INTO y verificaste que no hay duplicados al re-ejecutar?', en: '‚úÖ Did you load data with COPY INTO and verify there are no duplicates when re-running?', pt: '‚úÖ Voc√™ carregou dados com COPY INTO e verificou que n√£o h√° duplicados ao re-executar?' },
      xpReward: 35,
      estimatedMinutes: 30
    },
    {
      id: 'db-5-3',
      title: { es: 'Time Travel: Viajar en el Tiempo', en: 'Time Travel: Travel Through Time', pt: 'Time Travel: Viagem no Tempo' },
      description: { es: 'Accede a cualquier versi√≥n anterior de tus datos. Ideal para auditor√≠as, debugging y recuperaci√≥n.', en: 'Access any previous version of your data. Ideal for audits, debugging and recovery.', pt: 'Acesse qualquer vers√£o anterior dos seus dados. Ideal para auditorias, debugging e recupera√ß√£o.' },
      theory: {
        es: `## Time Travel en Delta Lake

Time Travel te permite acceder a **cualquier versi√≥n anterior** de una tabla. Es como Git para tus datos.

### ¬øC√≥mo funciona?

Cada operaci√≥n en Delta Lake crea una nueva versi√≥n:

\`\`\`
Versi√≥n 0: Tabla creada con 100 filas
Versi√≥n 1: INSERT de 50 filas nuevas
Versi√≥n 2: UPDATE de 10 filas
Versi√≥n 3: DELETE de 5 filas
Versi√≥n 4: MERGE con datos nuevos
\`\`\`

### Ver el historial de versiones

\`\`\`sql
-- Ver todas las versiones
DESCRIBE HISTORY mi_tabla;

-- Resultado:
-- version | timestamp           | operation | operationParameters
-- 4       | 2024-01-18 15:30:00| MERGE     | ...
-- 3       | 2024-01-18 14:00:00| DELETE    | ...
-- 2       | 2024-01-17 10:00:00| UPDATE    | ...
-- 1       | 2024-01-16 09:00:00| WRITE     | ...
-- 0       | 2024-01-15 08:00:00| CREATE    | ...
\`\`\`

### Acceder a versiones anteriores

**Por n√∫mero de versi√≥n:**
\`\`\`python
# Leer versi√≥n espec√≠fica
df_v2 = spark.read.format("delta") \\
    .option("versionAsOf", 2) \\
    .load("/path/to/table")

# Con SQL
spark.sql("SELECT * FROM mi_tabla VERSION AS OF 2")

# Sintaxis alternativa con @
spark.sql("SELECT * FROM mi_tabla@v2")
\`\`\`

**Por timestamp:**
\`\`\`python
# Leer por fecha/hora
df_ayer = spark.read.format("delta") \\
    .option("timestampAsOf", "2024-01-17 10:00:00") \\
    .load("/path/to/table")

# Con SQL
spark.sql("SELECT * FROM mi_tabla TIMESTAMP AS OF '2024-01-17 10:00:00'")
\`\`\`

### Casos de uso de Time Travel

**1. Debugging - ¬øQu√© cambi√≥?**
\`\`\`python
# Comparar versi√≥n actual vs anterior
df_actual = spark.table("ventas")
df_ayer = spark.read.option("versionAsOf", 5).table("ventas")

# Encontrar diferencias
nuevas_filas = df_actual.exceptAll(df_ayer)
filas_eliminadas = df_ayer.exceptAll(df_actual)
\`\`\`

**2. Recuperaci√≥n - Rollback**
\`\`\`python
# Restaurar versi√≥n anterior
spark.sql("RESTORE TABLE ventas TO VERSION AS OF 3")
\`\`\`

**3. Auditor√≠a - ¬øQu√© hab√≠a el d√≠a X?**
\`\`\`python
# Ver datos del primer d√≠a del mes
df_inicio_mes = spark.read \\
    .option("timestampAsOf", "2024-01-01 00:00:00") \\
    .table("ventas")
\`\`\`

**4. Reproducibilidad - ML Training**
\`\`\`python
# Entrenar modelo con datos de versi√≥n espec√≠fica
# (para poder reproducir resultados exactos)
training_data = spark.read \\
    .option("versionAsOf", 42) \\
    .table("features")
\`\`\`

### Retenci√≥n de historial

Por default, Delta Lake mantiene historial por **30 d√≠as**. Despu√©s de VACUUM, las versiones antiguas se eliminan.

\`\`\`python
# Ver configuraci√≥n de retenci√≥n
spark.sql("SHOW TBLPROPERTIES mi_tabla")

# Cambiar retenci√≥n (ejemplo: 90 d√≠as)
spark.sql("""
    ALTER TABLE mi_tabla 
    SET TBLPROPERTIES (delta.logRetentionDuration = '90 days')
""")
\`\`\`

### ‚ö†Ô∏è Importante sobre Time Travel

\`\`\`
Time Travel solo funciona si:
1. No ejecutaste VACUUM con retenci√≥n menor al tiempo que quer√©s viajar
2. Los archivos de datos todav√≠a existen

Si ejecut√°s VACUUM RETAIN 0 HOURS, perd√©s todo el historial!
\`\`\``,
        en: `## Time Travel in Delta Lake

Time Travel lets you access **any previous version** of a table. It's like Git for your data.

### How it works?

Each Delta Lake operation creates a new version. You can access any version by number or timestamp.

### View version history

\`\`\`sql
DESCRIBE HISTORY my_table;
\`\`\`

### Access previous versions

**By version number:**
\`\`\`python
df_v2 = spark.read.option("versionAsOf", 2).table("my_table")
\`\`\`

**By timestamp:**
\`\`\`python
df_yesterday = spark.read.option("timestampAsOf", "2024-01-17").table("my_table")
\`\`\`

### Use cases

1. **Debugging**: Compare current vs previous version
2. **Recovery**: Rollback with RESTORE
3. **Audit**: What data existed on date X?
4. **ML Reproducibility**: Train with specific data version`,
        pt: `## Time Travel no Delta Lake

Time Travel permite acessar **qualquer vers√£o anterior** de uma tabela.

### Ver hist√≥rico

\`\`\`sql
DESCRIBE HISTORY minha_tabela;
\`\`\`

### Acessar vers√µes anteriores

**Por n√∫mero de vers√£o:**
\`\`\`python
df_v2 = spark.read.option("versionAsOf", 2).table("minha_tabela")
\`\`\`

**Por timestamp:**
\`\`\`python
df_ontem = spark.read.option("timestampAsOf", "2024-01-17").table("minha_tabela")
\`\`\``
      },
      codeExample: {
        language: 'python',
        code: `# Ejemplo pr√°ctico de Time Travel

# 1. Crear tabla inicial
spark.sql("""
    CREATE OR REPLACE TABLE demo_time_travel (
        id INT, nombre STRING, valor INT
    ) USING DELTA
""")
spark.sql("INSERT INTO demo_time_travel VALUES (1, 'A', 100), (2, 'B', 200)")

# 2. Hacer algunas modificaciones
spark.sql("UPDATE demo_time_travel SET valor = 150 WHERE id = 1")
spark.sql("INSERT INTO demo_time_travel VALUES (3, 'C', 300)")
spark.sql("DELETE FROM demo_time_travel WHERE id = 2")

# 3. Ver historial
display(spark.sql("DESCRIBE HISTORY demo_time_travel"))

# 4. Viajar en el tiempo
print("=== Versi√≥n 0 (original) ===")
display(spark.sql("SELECT * FROM demo_time_travel VERSION AS OF 0"))

print("=== Versi√≥n actual ===")
display(spark.sql("SELECT * FROM demo_time_travel"))

# 5. Restaurar versi√≥n anterior si fue un error
# spark.sql("RESTORE TABLE demo_time_travel TO VERSION AS OF 1")`,
        explanation: { es: 'Este ejemplo muestra c√≥mo cada operaci√≥n crea una nueva versi√≥n y c√≥mo viajar entre ellas.', en: 'This example shows how each operation creates a new version and how to travel between them.', pt: 'Este exemplo mostra como cada opera√ß√£o cria uma nova vers√£o e como viajar entre elas.' }
      },
      practicalTips: [
        { es: 'üîç Time Travel es PERFECTO para debugging: "¬øPor qu√© este reporte cambi√≥?"', en: 'üîç Time Travel is PERFECT for debugging: "Why did this report change?"', pt: 'üîç Time Travel √© PERFEITO para debugging: "Por que esse relat√≥rio mudou?"' },
        { es: '‚ö†Ô∏è RESTORE crea una nueva versi√≥n, no borra el historial. Siempre pod√©s volver atr√°s.', en: '‚ö†Ô∏è RESTORE creates a new version, doesn\'t delete history. You can always go back.', pt: '‚ö†Ô∏è RESTORE cria uma nova vers√£o, n√£o apaga o hist√≥rico. Sempre pode voltar atr√°s.' },
        { es: 'üí° Guard√° el n√∫mero de versi√≥n cuando entren√°s modelos ML para reproducibilidad.', en: 'üí° Save the version number when training ML models for reproducibility.', pt: 'üí° Salve o n√∫mero da vers√£o ao treinar modelos ML para reprodutibilidade.' }
      ],
      externalLinks: [
        { title: 'Delta Lake Time Travel', url: 'https://docs.databricks.com/delta/history.html', type: 'docs' },
        { title: 'RESTORE Command', url: 'https://docs.databricks.com/sql/language-manual/delta-restore.html', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øViajaste a una versi√≥n anterior y comparaste los datos con la versi√≥n actual?', en: '‚úÖ Did you travel to a previous version and compare the data with the current version?', pt: '‚úÖ Voc√™ viajou para uma vers√£o anterior e comparou os dados com a vers√£o atual?' },
      xpReward: 35,
      estimatedMinutes: 30
    },
    {
      id: 'db-5-3b',
      title: { es: 'CLONE: Copiar Tablas Eficientemente', en: 'CLONE: Copy Tables Efficiently', pt: 'CLONE: Copiar Tabelas Eficientemente' },
      description: { es: 'CLONE crea copias de tablas Delta de forma eficiente, ideal para testing, desarrollo y backups.', en: 'CLONE creates copies of Delta tables efficiently, ideal for testing, development and backups.', pt: 'CLONE cria c√≥pias de tabelas Delta de forma eficiente, ideal para testing, desenvolvimento e backups.' },
      theory: {
        es: `## CLONE: Copias de Tablas Sin Duplicar Datos

CLONE permite crear copias de tablas Delta de dos formas diferentes, cada una con sus casos de uso.

### Tipos de Clone

\`\`\`
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SHALLOW CLONE                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Solo copia METADATA (transaction log)                     ‚îÇ
‚îÇ ‚Ä¢ Los datos apuntan a los archivos originales              ‚îÇ
‚îÇ ‚Ä¢ MUY R√ÅPIDO (segundos)                                    ‚îÇ
‚îÇ ‚Ä¢ Sin costo de storage adicional                           ‚îÇ
‚îÇ ‚Ä¢ ‚ö†Ô∏è Si se elimina la tabla original, el clone se rompe   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ Ideal para:                                                  ‚îÇ
‚îÇ ‚Ä¢ Testing r√°pido                                            ‚îÇ
‚îÇ ‚Ä¢ Experimentos temporales                                   ‚îÇ
‚îÇ ‚Ä¢ Desarrollo local                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     DEEP CLONE                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Copia METADATA + TODOS LOS DATOS                          ‚îÇ
‚îÇ ‚Ä¢ Tabla completamente independiente                         ‚îÇ
‚îÇ ‚Ä¢ Tarda m√°s (copia archivos f√≠sicamente)                   ‚îÇ
‚îÇ ‚Ä¢ Duplica uso de storage                                    ‚îÇ
‚îÇ ‚Ä¢ ‚úÖ Tabla original y clone son independientes             ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ Ideal para:                                                  ‚îÇ
‚îÇ ‚Ä¢ Backups reales                                            ‚îÇ
‚îÇ ‚Ä¢ Migraciones a otro bucket/regi√≥n                         ‚îÇ
‚îÇ ‚Ä¢ Ambientes de staging permanentes                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Sintaxis SQL

\`\`\`sql
-- SHALLOW CLONE (r√°pido, comparte datos)
CREATE TABLE dev.mi_tabla_test
SHALLOW CLONE prod.mi_tabla;

-- DEEP CLONE (completo, independiente)
CREATE TABLE backup.mi_tabla_backup
DEEP CLONE prod.mi_tabla;

-- Clone de versi√≥n espec√≠fica (Time Travel)
CREATE TABLE dev.mi_tabla_v5
SHALLOW CLONE prod.mi_tabla VERSION AS OF 5;

-- Clone a timestamp espec√≠fico
CREATE TABLE dev.mi_tabla_ayer
DEEP CLONE prod.mi_tabla TIMESTAMP AS OF '2024-01-15';

-- Clone a ubicaci√≥n externa
CREATE TABLE backup.mi_tabla_s3
DEEP CLONE prod.mi_tabla
LOCATION 's3://backup-bucket/tables/mi_tabla/';
\`\`\`

### Python API

\`\`\`python
# Shallow clone
spark.sql("""
    CREATE TABLE dev.customers_test
    SHALLOW CLONE prod.customers
""")

# Deep clone con versi√≥n
spark.sql("""
    CREATE TABLE backup.customers_20240115
    DEEP CLONE prod.customers VERSION AS OF 100
""")

# Verificar que son independientes
original_count = spark.table("prod.customers").count()
clone_count = spark.table("backup.customers_20240115").count()
print(f"Original: {original_count}, Clone: {clone_count}")
\`\`\`

### Caso de Uso 1: Testing en Desarrollo

\`\`\`sql
-- Desarrollador necesita probar con datos de prod
-- Sin copiar terabytes de datos

-- 1. Crear shallow clone (segundos)
CREATE TABLE dev.orders_test
SHALLOW CLONE prod.orders;

-- 2. El desarrollador puede:
--    - Hacer queries
--    - Modificar datos (no afecta prod)
--    - Probar transformaciones

-- 3. Cuando termina, eliminar
DROP TABLE dev.orders_test;
-- (Los datos de prod siguen intactos)
\`\`\`

### Caso de Uso 2: Backup Antes de Migraci√≥n

\`\`\`sql
-- Antes de hacer cambios grandes, crear backup
CREATE TABLE backup.customers_pre_migration
DEEP CLONE prod.customers;

-- Ejecutar migraci√≥n riesgosa
ALTER TABLE prod.customers ADD COLUMN new_field STRING;
UPDATE prod.customers SET new_field = 'default';

-- Si algo sale mal:
-- DROP TABLE prod.customers;
-- ALTER TABLE backup.customers_pre_migration RENAME TO prod.customers;
\`\`\`

### Caso de Uso 3: Ambiente de Staging

\`\`\`sql
-- Crear staging como clone de prod
CREATE OR REPLACE TABLE staging.orders
DEEP CLONE prod.orders;

CREATE OR REPLACE TABLE staging.customers
DEEP CLONE prod.customers;

-- Staging es ahora una copia exacta de prod
-- para testing de nuevas features
\`\`\`

### Clone Incremental (Solo Deep Clone)

\`\`\`sql
-- Primera vez: clone completo
CREATE TABLE backup.orders_backup
DEEP CLONE prod.orders;

-- Actualizaciones incrementales (solo cambios)
CREATE OR REPLACE TABLE backup.orders_backup
DEEP CLONE prod.orders;
-- Delta Lake detecta cambios y solo copia lo nuevo
\`\`\`

### Comparaci√≥n R√°pida

| Aspecto | Shallow Clone | Deep Clone |
|---------|---------------|------------|
| Velocidad | Segundos | Minutos/Horas |
| Storage | 0 adicional | 100% duplicado |
| Independencia | Dependiente | Independiente |
| Time Travel | Hereda del original | Propio |
| Uso t√≠pico | Dev/Test temporal | Backup/Staging |`,
        en: `## CLONE: Copy Tables Without Duplicating Data

CLONE creates Delta table copies in two different ways.

\`\`\`sql
-- SHALLOW CLONE (fast, shares data)
CREATE TABLE dev.test_table
SHALLOW CLONE prod.source_table;

-- DEEP CLONE (complete, independent)
CREATE TABLE backup.backup_table
DEEP CLONE prod.source_table;

-- Clone specific version
CREATE TABLE dev.table_v5
SHALLOW CLONE prod.source VERSION AS OF 5;
\`\`\`

### When to Use
- Shallow: Dev/testing (fast, no storage cost)
- Deep: Backups, staging (independent copy)`,
        pt: `## CLONE: Copiar Tabelas Sem Duplicar Dados

CLONE cria c√≥pias de tabelas Delta de duas formas diferentes.

\`\`\`sql
-- SHALLOW CLONE (r√°pido, compartilha dados)
CREATE TABLE dev.tabela_teste
SHALLOW CLONE prod.tabela_fonte;

-- DEEP CLONE (completo, independente)
CREATE TABLE backup.tabela_backup
DEEP CLONE prod.tabela_fonte;
\`\`\``
      },
      practicalTips: [
        { es: '‚ö° Shallow clone es perfecto para desarrollo - crea un "ambiente de prod" en segundos.', en: '‚ö° Shallow clone is perfect for development - creates a "prod environment" in seconds.', pt: '‚ö° Shallow clone √© perfeito para desenvolvimento - cria um "ambiente de prod" em segundos.' },
        { es: 'üéØ CLONE es pregunta frecuente en el examen: "¬øDiferencia entre Shallow y Deep clone?"', en: 'üéØ CLONE is a frequent exam question: "Difference between Shallow and Deep clone?"', pt: 'üéØ CLONE √© pergunta frequente no exame: "Diferen√ßa entre Shallow e Deep clone?"' },
        { es: '‚ö†Ô∏è No uses shallow clone para backups reales - si se borra el original, el clone se rompe.', en: '‚ö†Ô∏è Don\'t use shallow clone for real backups - if original is deleted, clone breaks.', pt: '‚ö†Ô∏è N√£o use shallow clone para backups reais - se o original for deletado, o clone quebra.' }
      ],
      externalLinks: [
        { title: 'Delta Lake Clone', url: 'https://docs.databricks.com/delta/clone.html', type: 'docs' },
        { title: 'Clone a Delta Table', url: 'https://docs.databricks.com/sql/language-manual/delta-clone.html', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øCreaste un shallow clone y verificaste que los cambios no afectan la tabla original?', en: '‚úÖ Did you create a shallow clone and verify changes don\'t affect the original table?', pt: '‚úÖ Voc√™ criou um shallow clone e verificou que as mudan√ßas n√£o afetam a tabela original?' },
      xpReward: 30,
      estimatedMinutes: 25
    },
    {
      id: 'db-5-4',
      title: { es: 'MERGE: Upserts Eficientes', en: 'MERGE: Efficient Upserts', pt: 'MERGE: Upserts Eficientes' },
      description: { es: 'MERGE es LA operaci√≥n m√°s importante de Delta Lake. Permite INSERT, UPDATE y DELETE en una sola operaci√≥n at√≥mica.', en: 'MERGE is THE most important Delta Lake operation. Allows INSERT, UPDATE and DELETE in a single atomic operation.', pt: 'MERGE √© A opera√ß√£o mais importante do Delta Lake. Permite INSERT, UPDATE e DELETE em uma √∫nica opera√ß√£o at√¥mica.' },
      theory: {
        es: `## MERGE: La Operaci√≥n M√°s Poderosa de Delta

MERGE permite sincronizar una tabla destino con datos fuente en **una sola operaci√≥n at√≥mica**. Es esencial para:
- CDC (Change Data Capture)
- SCD Type 2 (Slowly Changing Dimensions)
- Deduplicaci√≥n
- Sincronizaci√≥n de datos

### Sintaxis B√°sica

\`\`\`sql
MERGE INTO tabla_destino AS target
USING datos_nuevos AS source
ON target.id = source.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
\`\`\`

### Casos de Uso Detallados

**1. Upsert Simple (Update + Insert)**
\`\`\`sql
MERGE INTO clientes AS target
USING nuevos_clientes AS source
ON target.cliente_id = source.cliente_id
WHEN MATCHED THEN 
    UPDATE SET 
        nombre = source.nombre,
        email = source.email,
        updated_at = current_timestamp()
WHEN NOT MATCHED THEN 
    INSERT (cliente_id, nombre, email, created_at)
    VALUES (source.cliente_id, source.nombre, source.email, current_timestamp());
\`\`\`

**2. Upsert con DELETE (CDC completo)**
\`\`\`sql
MERGE INTO productos AS target
USING cambios_productos AS source
ON target.producto_id = source.producto_id
WHEN MATCHED AND source.operacion = 'DELETE' THEN DELETE
WHEN MATCHED AND source.operacion = 'UPDATE' THEN UPDATE SET *
WHEN NOT MATCHED AND source.operacion = 'INSERT' THEN INSERT *;
\`\`\`

**3. SCD Type 1 (Sobrescribir hist√≥rico)**
\`\`\`sql
-- Simplemente actualizar el valor actual
MERGE INTO dim_cliente AS target
USING staging_cliente AS source
ON target.cliente_id = source.cliente_id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
\`\`\`

**4. SCD Type 2 (Mantener hist√≥rico)**
\`\`\`sql
-- Paso 1: Cerrar registros actuales que cambiaron
MERGE INTO dim_cliente AS target
USING (
    SELECT s.*, current_timestamp() as fecha_cierre
    FROM staging_cliente s
    JOIN dim_cliente d ON s.cliente_id = d.cliente_id
    WHERE d.es_actual = true AND (s.nombre != d.nombre OR s.email != d.email)
) AS source
ON target.cliente_id = source.cliente_id AND target.es_actual = true
WHEN MATCHED THEN UPDATE SET 
    es_actual = false,
    fecha_fin = source.fecha_cierre;

-- Paso 2: Insertar nuevos registros
MERGE INTO dim_cliente AS target
USING staging_cliente AS source
ON target.cliente_id = source.cliente_id AND target.es_actual = true
WHEN NOT MATCHED THEN INSERT (
    cliente_id, nombre, email, fecha_inicio, fecha_fin, es_actual
) VALUES (
    source.cliente_id, source.nombre, source.email, 
    current_timestamp(), null, true
);
\`\`\`

**5. Deduplicaci√≥n**
\`\`\`sql
-- Insertar solo si no existe
MERGE INTO eventos AS target
USING nuevos_eventos AS source
ON target.event_id = source.event_id
WHEN NOT MATCHED THEN INSERT *;
\`\`\`

### MERGE con Python (DeltaTable API)

\`\`\`python
from delta.tables import DeltaTable

# Cargar tabla destino
dt = DeltaTable.forPath(spark, "/path/to/tabla")
# o: dt = DeltaTable.forName(spark, "mi_tabla")

# Ejecutar MERGE
dt.alias("target").merge(
    df_nuevos.alias("source"),
    "target.id = source.id"
).whenMatchedUpdateAll() \\
 .whenNotMatchedInsertAll() \\
 .execute()

# Con condiciones espec√≠ficas
dt.alias("target").merge(
    df_cambios.alias("source"),
    "target.id = source.id"
).whenMatchedUpdate(
    condition="source.timestamp > target.timestamp",
    set={"valor": "source.valor", "updated_at": "current_timestamp()"}
).whenNotMatchedInsert(
    values={"id": "source.id", "valor": "source.valor", "created_at": "current_timestamp()"}
).execute()
\`\`\`

### Performance Tips para MERGE

1. **Usa partition pruning**: Si la tabla est√° particionada, inclu√≠ la columna de partici√≥n en la condici√≥n ON
2. **Orden√° los datos source**: Si es posible, orden√° por la columna de join
3. **Limita las columnas**: Usa columnas espec√≠ficas en vez de \`*\`
4. **Considera Z-ORDER**: En la columna de join para acelerar el matching`,
        en: `## MERGE: The Most Powerful Delta Operation

MERGE allows synchronizing a target table with source data in **a single atomic operation**.

### Basic Syntax

\`\`\`sql
MERGE INTO target_table AS target
USING new_data AS source
ON target.id = source.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
\`\`\`

### Use Cases

1. **Simple Upsert**: Update existing, insert new
2. **CDC with DELETE**: Handle inserts, updates, and deletes
3. **SCD Type 1**: Overwrite history
4. **SCD Type 2**: Maintain history
5. **Deduplication**: Insert only if not exists`,
        pt: `## MERGE: A Opera√ß√£o Mais Poderosa do Delta

MERGE permite sincronizar uma tabela destino com dados fonte em **uma √∫nica opera√ß√£o at√¥mica**.

### Sintaxe B√°sica

\`\`\`sql
MERGE INTO tabela_destino AS target
USING dados_novos AS source
ON target.id = source.id
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *;
\`\`\``
      },
      codeExample: {
        language: 'python',
        code: `# Ejemplo completo de MERGE en Python

from delta.tables import DeltaTable

# 1. Crear tabla destino
spark.sql("""
    CREATE OR REPLACE TABLE clientes_delta (
        id INT, nombre STRING, email STRING, saldo DOUBLE, updated_at TIMESTAMP
    ) USING DELTA
""")
spark.sql("""
    INSERT INTO clientes_delta VALUES 
    (1, 'Ana', 'ana@mail.com', 100.0, current_timestamp()),
    (2, 'Bob', 'bob@mail.com', 200.0, current_timestamp())
""")

# 2. Crear datos nuevos (algunos updates, algunos inserts)
nuevos = spark.createDataFrame([
    (1, "Ana Garc√≠a", "ana.garcia@mail.com", 150.0),  # UPDATE
    (2, "Bob", "bob@mail.com", 250.0),                 # UPDATE (solo saldo)
    (3, "Carlos", "carlos@mail.com", 300.0),          # INSERT
], ["id", "nombre", "email", "saldo"])

# 3. Ejecutar MERGE
dt = DeltaTable.forName(spark, "clientes_delta")

dt.alias("target").merge(
    nuevos.alias("source"),
    "target.id = source.id"
).whenMatchedUpdate(
    set={
        "nombre": "source.nombre",
        "email": "source.email",
        "saldo": "source.saldo",
        "updated_at": "current_timestamp()"
    }
).whenNotMatchedInsert(
    values={
        "id": "source.id",
        "nombre": "source.nombre",
        "email": "source.email",
        "saldo": "source.saldo",
        "updated_at": "current_timestamp()"
    }
).execute()

# 4. Ver resultado
display(spark.table("clientes_delta"))
display(spark.sql("DESCRIBE HISTORY clientes_delta"))`,
        explanation: { es: 'Este ejemplo muestra un MERGE completo: actualiza clientes existentes e inserta nuevos, todo en una operaci√≥n at√≥mica.', en: 'This example shows a complete MERGE: updates existing customers and inserts new ones, all in one atomic operation.', pt: 'Este exemplo mostra um MERGE completo: atualiza clientes existentes e insere novos, tudo em uma opera√ß√£o at√¥mica.' }
      },
      practicalTips: [
        { es: 'üéØ MERGE es la operaci√≥n #1 en entrevistas de Data Engineering. Practicala mucho.', en: 'üéØ MERGE is the #1 operation in Data Engineering interviews. Practice it a lot.', pt: 'üéØ MERGE √© a opera√ß√£o #1 em entrevistas de Data Engineering. Pratique muito.' },
        { es: '‚ö° whenMatchedUpdateAll() y whenNotMatchedInsertAll() son atajos para SET/VALUES *.', en: '‚ö° whenMatchedUpdateAll() and whenNotMatchedInsertAll() are shortcuts for SET/VALUES *.', pt: '‚ö° whenMatchedUpdateAll() e whenNotMatchedInsertAll() s√£o atalhos para SET/VALUES *.' },
        { es: 'üîç Us√° DESCRIBE HISTORY despu√©s de MERGE para ver cu√°ntas filas se afectaron.', en: 'üîç Use DESCRIBE HISTORY after MERGE to see how many rows were affected.', pt: 'üîç Use DESCRIBE HISTORY depois do MERGE para ver quantas filas foram afetadas.' }
      ],
      externalLinks: [
        { title: 'MERGE INTO', url: 'https://docs.databricks.com/delta/merge.html', type: 'docs' },
        { title: 'Delta MERGE Performance', url: 'https://docs.databricks.com/delta/merge.html#performance-tuning', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øEjecutaste un MERGE que hace UPDATE y INSERT en la misma operaci√≥n?', en: '‚úÖ Did you run a MERGE that does UPDATE and INSERT in the same operation?', pt: '‚úÖ Voc√™ executou um MERGE que faz UPDATE e INSERT na mesma opera√ß√£o?' },
      xpReward: 40,
      estimatedMinutes: 40
    },
    {
      id: 'db-5-5',
      title: { es: 'Schema Enforcement y Evolution', en: 'Schema Enforcement and Evolution', pt: 'Schema Enforcement e Evolution' },
      description: { es: 'Delta Lake protege la calidad de tus datos rechazando schemas incompatibles, pero permite evoluci√≥n controlada.', en: 'Delta Lake protects your data quality by rejecting incompatible schemas, but allows controlled evolution.', pt: 'Delta Lake protege a qualidade dos seus dados rejeitando schemas incompat√≠veis, mas permite evolu√ß√£o controlada.' },
      theory: {
        es: `## Schema Enforcement: El Guardi√°n de tus Datos

Delta Lake **rechaza autom√°ticamente** datos que no coinciden con el schema de la tabla. Esto evita la corrupci√≥n de datos.

### ¬øQu√© verifica Schema Enforcement?

\`\`\`
‚úÖ Nombres de columnas deben coincidir
‚úÖ Tipos de datos deben ser compatibles
‚úÖ Nullability debe ser respetada
‚ùå Columnas extra en source ‚Üí ERROR (por default)
‚ùå Columnas faltantes ‚Üí ERROR
‚ùå Tipos incompatibles ‚Üí ERROR
\`\`\`

### Ejemplo de Schema Enforcement

\`\`\`python
# Tabla existente
# Schema: id INT, nombre STRING, precio DOUBLE

# Esto funciona ‚úÖ
df_ok = spark.createDataFrame([(1, "Laptop", 999.99)], ["id", "nombre", "precio"])
df_ok.write.mode("append").saveAsTable("productos")

# Esto FALLA ‚ùå (columna extra)
df_extra = spark.createDataFrame([(2, "Mouse", 29.99, "Electronics")], 
                                  ["id", "nombre", "precio", "categoria"])
df_extra.write.mode("append").saveAsTable("productos")
# Error: A]cannot be merged to a schema

# Esto FALLA ‚ùå (tipo incorrecto)
df_tipo = spark.createDataFrame([(3, "Keyboard", "cien")],  # precio es STRING
                                 ["id", "nombre", "precio"])
df_tipo.write.mode("append").saveAsTable("productos")
# Error: Failed to merge fields
\`\`\`

## Schema Evolution: Evoluci√≥n Controlada

Cuando QUER√âS agregar columnas nuevas, us√°s Schema Evolution:

### Agregar columnas nuevas (mergeSchema)

\`\`\`python
# Agregar columna "categoria" a tabla existente
df_nuevo = spark.createDataFrame([
    (4, "Monitor", 299.99, "Electronics")
], ["id", "nombre", "precio", "categoria"])

df_nuevo.write \\
    .mode("append") \\
    .option("mergeSchema", "true") \\  # Permite agregar columnas
    .saveAsTable("productos")

# Ahora la tabla tiene: id, nombre, precio, categoria
# Las filas anteriores tendr√°n NULL en "categoria"
\`\`\`

### Con SQL

\`\`\`sql
-- Agregar columna manualmente
ALTER TABLE productos ADD COLUMN categoria STRING;

-- O permitir merge autom√°tico
SET spark.databricks.delta.schema.autoMerge.enabled = true;
INSERT INTO productos VALUES (5, 'Webcam', 79.99, 'Electronics');
\`\`\`

### Cambios de schema permitidos

| Cambio | mergeSchema | overwriteSchema |
|--------|-------------|-----------------|
| Agregar columna | ‚úÖ | ‚úÖ |
| Eliminar columna | ‚ùå | ‚úÖ |
| Cambiar tipo de dato | ‚ùå | ‚úÖ |
| Renombrar columna | ‚ùå | ‚úÖ |
| Reordenar columnas | ‚úÖ | ‚úÖ |

### overwriteSchema: El Martillo Grande

\`\`\`python
# CUIDADO: Esto reemplaza el schema completamente
df_nuevo_schema.write \\
    .mode("overwrite") \\
    .option("overwriteSchema", "true") \\
    .saveAsTable("productos")
\`\`\`

### Configuraci√≥n Global

\`\`\`python
# Habilitar auto-merge para toda la sesi√≥n
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")

# Habilitar para una tabla espec√≠fica
spark.sql("""
    ALTER TABLE mi_tabla 
    SET TBLPROPERTIES ('delta.autoMerge.enabled' = 'true')
""")
\`\`\`

### Best Practices de Schema Management

1. **Development**: Usa mergeSchema para iterar r√°pido
2. **Production**: Schema enforcement estricto
3. **Migraciones**: Usa ALTER TABLE para cambios controlados
4. **Breaking changes**: Crea tabla nueva + migraci√≥n`,
        en: `## Schema Enforcement: The Guardian of Your Data

Delta Lake **automatically rejects** data that doesn't match the table schema.

### Schema Evolution

When you WANT to add new columns:

\`\`\`python
df_new.write \\
    .mode("append") \\
    .option("mergeSchema", "true") \\
    .saveAsTable("products")
\`\`\`

### Allowed schema changes

| Change | mergeSchema | overwriteSchema |
|--------|-------------|-----------------|
| Add column | ‚úÖ | ‚úÖ |
| Remove column | ‚ùå | ‚úÖ |
| Change data type | ‚ùå | ‚úÖ |`,
        pt: `## Schema Enforcement: O Guardi√£o dos Seus Dados

Delta Lake **rejeita automaticamente** dados que n√£o coincidem com o schema da tabela.

### Schema Evolution

Quando voc√™ QUER adicionar colunas novas:

\`\`\`python
df_novo.write \\
    .mode("append") \\
    .option("mergeSchema", "true") \\
    .saveAsTable("produtos")
\`\`\``
      },
      practicalTips: [
        { es: 'üõ°Ô∏è Schema Enforcement es tu amigo. No lo desactives en producci√≥n.', en: 'üõ°Ô∏è Schema Enforcement is your friend. Don\'t disable it in production.', pt: 'üõ°Ô∏è Schema Enforcement √© seu amigo. N√£o o desabilite em produ√ß√£o.' },
        { es: 'üí° Us√° ALTER TABLE para cambios de schema en producci√≥n - es m√°s controlado.', en: 'üí° Use ALTER TABLE for schema changes in production - it\'s more controlled.', pt: 'üí° Use ALTER TABLE para mudan√ßas de schema em produ√ß√£o - √© mais controlado.' },
        { es: '‚ö†Ô∏è overwriteSchema puede perder datos si no ten√©s cuidado. Siempre hac√© backup.', en: '‚ö†Ô∏è overwriteSchema can lose data if you\'re not careful. Always backup.', pt: '‚ö†Ô∏è overwriteSchema pode perder dados se n√£o tiver cuidado. Sempre fa√ßa backup.' }
      ],
      externalLinks: [
        { title: 'Schema Enforcement', url: 'https://docs.databricks.com/delta/delta-batch.html#schema-enforcement', type: 'docs' },
        { title: 'Schema Evolution', url: 'https://docs.databricks.com/delta/delta-batch.html#schema-evolution', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øProbaste qu√© pasa cuando escrib√≠s datos con schema diferente? ¬øY con mergeSchema?', en: '‚úÖ Did you test what happens when writing data with different schema? And with mergeSchema?', pt: '‚úÖ Voc√™ testou o que acontece ao escrever dados com schema diferente? E com mergeSchema?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-5-6',
      title: { es: 'OPTIMIZE y Compactaci√≥n', en: 'OPTIMIZE and Compaction', pt: 'OPTIMIZE e Compacta√ß√£o' },
      description: { es: 'OPTIMIZE combina archivos peque√±os en archivos grandes para mejorar dram√°ticamente la performance de lectura.', en: 'OPTIMIZE combines small files into large files to dramatically improve read performance.', pt: 'OPTIMIZE combina arquivos pequenos em arquivos grandes para melhorar dramaticamente a performance de leitura.' },
      theory: {
        es: `## El Problema de los Archivos Peque√±os

Cuando hac√©s muchos appends peque√±os, termin√°s con miles de archivos diminutos:

\`\`\`
mi_tabla/
‚îú‚îÄ‚îÄ part-00001.parquet  (10 KB)
‚îú‚îÄ‚îÄ part-00002.parquet  (15 KB)
‚îú‚îÄ‚îÄ part-00003.parquet  (8 KB)
‚îú‚îÄ‚îÄ ... (10,000 archivos m√°s)
‚îî‚îÄ‚îÄ part-10000.parquet  (12 KB)

Problema: Leer 10,000 archivos peque√±os es MUY lento
- Cada archivo = 1 request al storage
- Overhead de metadata por archivo
- No aprovecha el I/O paralelo eficientemente
\`\`\`

## OPTIMIZE: La Soluci√≥n

OPTIMIZE compacta archivos peque√±os en archivos de ~1GB:

\`\`\`sql
-- Optimizar toda la tabla
OPTIMIZE mi_tabla;

-- Optimizar particiones espec√≠ficas
OPTIMIZE mi_tabla WHERE fecha >= '2024-01-01';

-- Ver resultado
DESCRIBE DETAIL mi_tabla;
-- Antes: numFiles = 10,000
-- Despu√©s: numFiles = 50
\`\`\`

### ¬øCu√°ndo ejecutar OPTIMIZE?

\`\`\`
‚úÖ Despu√©s de muchos appends peque√±os
‚úÖ Cuando las queries est√°n lentas
‚úÖ Antes de ejecutar queries anal√≠ticas grandes
‚úÖ En schedule (cada hora/d√≠a seg√∫n carga)

‚ùå Despu√©s de cada write (overhead innecesario)
‚ùå En tablas con pocos datos
\`\`\`

### Configuraci√≥n de OPTIMIZE

\`\`\`python
# Tama√±o objetivo de archivo (default: 1GB)
spark.sql("SET spark.databricks.delta.optimize.maxFileSize = 134217728")  # 128MB

# M√≠nimo de archivos para triggear optimize
spark.sql("SET spark.databricks.delta.optimize.minFileSize = 1048576")  # 1MB
\`\`\`

## OPTIMIZE ZORDER: Ordenamiento Multidimensional

Z-ORDER organiza los datos para que las queries sean m√°s r√°pidas:

\`\`\`sql
-- Optimizar Y ordenar por columnas frecuentes en WHERE
OPTIMIZE ventas ZORDER BY (region, fecha);
\`\`\`

### ¬øC√≥mo funciona Z-ORDER?

\`\`\`
Sin Z-ORDER:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Archivo 1: region=AR,MX,BR,CO           ‚îÇ
‚îÇ Archivo 2: region=AR,MX,BR,CO           ‚îÇ
‚îÇ Archivo 3: region=AR,MX,BR,CO           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Query: WHERE region = 'AR' ‚Üí Lee TODOS los archivos

Con ZORDER BY (region):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Archivo 1: region=AR (min=AR, max=AR)   ‚îÇ
‚îÇ Archivo 2: region=BR (min=BR, max=BR)   ‚îÇ
‚îÇ Archivo 3: region=MX,CO                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Query: WHERE region = 'AR' ‚Üí Lee SOLO archivo 1 (Data Skipping!)
\`\`\`

### Best Practices para Z-ORDER

\`\`\`
‚úÖ Usar en columnas de filtro frecuente (WHERE)
‚úÖ M√°ximo 3-4 columnas (m√°s no mejora mucho)
‚úÖ Columnas de alta cardinalidad funcionan mejor
‚úÖ La primera columna es la m√°s importante

‚ùå No usar en columnas que nunca filtr√°s
‚ùå No usar en columnas con muy baja cardinalidad (ej: boolean)
\`\`\`

### Auto Optimize (Databricks)

\`\`\`sql
-- Habilitar optimizaci√≥n autom√°tica en escritura
ALTER TABLE mi_tabla SET TBLPROPERTIES (
    'delta.autoOptimize.optimizeWrite' = 'true',
    'delta.autoOptimize.autoCompact' = 'true'
);
\`\`\`

**optimizeWrite**: Optimiza el tama√±o de archivo en cada escritura
**autoCompact**: Compacta archivos peque√±os autom√°ticamente`,
        en: `## The Small Files Problem

When you do many small appends, you end up with thousands of tiny files. OPTIMIZE compacts them into ~1GB files.

\`\`\`sql
OPTIMIZE my_table;
OPTIMIZE my_table ZORDER BY (region, date);
\`\`\`

### When to run OPTIMIZE?

- After many small appends
- When queries are slow
- On schedule (hourly/daily)

### Z-ORDER

Organizes data so queries filtering by those columns read fewer files (data skipping).`,
        pt: `## O Problema dos Arquivos Pequenos

OPTIMIZE compacta arquivos pequenos em arquivos de ~1GB.

\`\`\`sql
OPTIMIZE minha_tabela;
OPTIMIZE minha_tabela ZORDER BY (regiao, data);
\`\`\``
      },
      practicalTips: [
        { es: '‚è∞ Program√° OPTIMIZE para que corra en horarios de baja carga (ej: 3am).', en: '‚è∞ Schedule OPTIMIZE to run during low-load hours (e.g., 3am).', pt: '‚è∞ Programe OPTIMIZE para rodar em hor√°rios de baixa carga (ex: 3h).' },
        { es: 'üìä Us√° DESCRIBE DETAIL antes y despu√©s de OPTIMIZE para ver la mejora.', en: 'üìä Use DESCRIBE DETAIL before and after OPTIMIZE to see the improvement.', pt: 'üìä Use DESCRIBE DETAIL antes e depois do OPTIMIZE para ver a melhora.' },
        { es: 'üí° Auto Optimize es genial para tablas con streaming - evita archivos peque√±os.', en: 'üí° Auto Optimize is great for streaming tables - avoids small files.', pt: 'üí° Auto Optimize √© √≥timo para tabelas com streaming - evita arquivos pequenos.' }
      ],
      externalLinks: [
        { title: 'OPTIMIZE Command', url: 'https://docs.databricks.com/delta/optimize.html', type: 'docs' },
        { title: 'Z-Ordering', url: 'https://docs.databricks.com/delta/optimizations/file-mgmt.html#z-ordering-multi-dimensional-clustering', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øEjecutaste OPTIMIZE en una tabla y verificaste la reducci√≥n de archivos con DESCRIBE DETAIL?', en: '‚úÖ Did you run OPTIMIZE on a table and verify the file reduction with DESCRIBE DETAIL?', pt: '‚úÖ Voc√™ executou OPTIMIZE em uma tabela e verificou a redu√ß√£o de arquivos com DESCRIBE DETAIL?' },
      xpReward: 35,
      estimatedMinutes: 30
    },
    {
      id: 'db-5-7',
      title: { es: 'VACUUM: Limpieza y Espacio', en: 'VACUUM: Cleanup and Space', pt: 'VACUUM: Limpeza e Espa√ßo' },
      description: { es: 'VACUUM elimina archivos antiguos que ya no son necesarios, liberando espacio de almacenamiento.', en: 'VACUUM removes old files that are no longer needed, freeing storage space.', pt: 'VACUUM remove arquivos antigos que n√£o s√£o mais necess√°rios, liberando espa√ßo de armazenamento.' },
      theory: {
        es: `## VACUUM: Limpieza de Archivos Obsoletos

Cuando Delta Lake hace UPDATE, DELETE o OPTIMIZE, los archivos viejos no se borran inmediatamente (para permitir Time Travel). VACUUM los elimina.

### ¬øQu√© archivos elimina VACUUM?

\`\`\`
mi_tabla/
‚îú‚îÄ‚îÄ _delta_log/            # NUNCA se toca
‚îú‚îÄ‚îÄ part-00001.parquet     # Versi√≥n actual - NO eliminar
‚îú‚îÄ‚îÄ part-00002.parquet     # Versi√≥n actual - NO eliminar
‚îú‚îÄ‚îÄ part-00003-OLD.parquet # Ya no referenciado - ELIMINAR ‚úì
‚îú‚îÄ‚îÄ part-00004-OLD.parquet # Ya no referenciado - ELIMINAR ‚úì
‚îî‚îÄ‚îÄ part-00005-OLD.parquet # Ya no referenciado - ELIMINAR ‚úì
\`\`\`

### Ejecutar VACUUM

\`\`\`sql
-- Ver qu√© se eliminar√≠a (dry run)
VACUUM mi_tabla RETAIN 168 HOURS DRY RUN;

-- Eliminar archivos >7 d√≠as (default)
VACUUM mi_tabla;

-- Eliminar archivos >24 horas
VACUUM mi_tabla RETAIN 24 HOURS;

-- ‚ö†Ô∏è PELIGRO: Eliminar todo inmediatamente
-- spark.sql("SET spark.databricks.delta.retentionDurationCheck.enabled = false")
-- VACUUM mi_tabla RETAIN 0 HOURS;
\`\`\`

### Retenci√≥n Default

- **Default**: 168 horas (7 d√≠as)
- **M√≠nimo permitido**: 168 horas (sin desactivar check)
- **Recomendado**: 7-30 d√≠as seg√∫n necesidades de auditor√≠a

### ‚ö†Ô∏è ADVERTENCIA CR√çTICA

\`\`\`
DESPU√âS DE VACUUM, PIERDES TIME TRAVEL A VERSIONES ANTERIORES
AL PER√çODO DE RETENCI√ìN.

Ejemplo:
- VACUUM RETAIN 24 HOURS
- Ya NO puedes hacer: SELECT * FROM tabla VERSION AS OF 2
  (si versi√≥n 2 tiene m√°s de 24 horas)
\`\`\`

### VACUUM y Time Travel: El Trade-off

\`\`\`
Mayor retenci√≥n = M√°s espacio + M√°s Time Travel
Menor retenci√≥n = Menos espacio + Menos Time Travel

Recomendaciones:
- Tablas de auditor√≠a: 30-90 d√≠as
- Tablas operacionales: 7 d√≠as
- Tablas de desarrollo: 1-3 d√≠as
\`\`\`

### Configurar Retenci√≥n por Tabla

\`\`\`sql
-- Configurar retenci√≥n a 30 d√≠as
ALTER TABLE mi_tabla SET TBLPROPERTIES (
    'delta.deletedFileRetentionDuration' = '30 days',
    'delta.logRetentionDuration' = '30 days'
);

-- Ver configuraci√≥n actual
SHOW TBLPROPERTIES mi_tabla;
\`\`\`

### Automatizar VACUUM

\`\`\`python
# En un Job de Databricks (daily)
from delta.tables import DeltaTable

tablas = ["ventas", "clientes", "productos"]

for tabla in tablas:
    dt = DeltaTable.forName(spark, tabla)
    dt.vacuum(168)  # 7 d√≠as
    print(f"VACUUM completado para {tabla}")
\`\`\``,
        en: `## VACUUM: Cleanup of Obsolete Files

VACUUM removes old files that are no longer referenced by any version within the retention period.

\`\`\`sql
-- Dry run
VACUUM my_table RETAIN 168 HOURS DRY RUN;

-- Execute
VACUUM my_table RETAIN 24 HOURS;
\`\`\`

### ‚ö†Ô∏è CRITICAL WARNING

After VACUUM, you lose Time Travel to versions older than the retention period!`,
        pt: `## VACUUM: Limpeza de Arquivos Obsoletos

VACUUM remove arquivos antigos que n√£o s√£o mais referenciados.

\`\`\`sql
VACUUM minha_tabela RETAIN 168 HOURS;
\`\`\`

‚ö†Ô∏è Depois do VACUUM, voc√™ perde Time Travel para vers√µes anteriores ao per√≠odo de reten√ß√£o!`
      },
      practicalTips: [
        { es: 'üîí NUNCA uses RETAIN 0 HOURS en producci√≥n sin entender las consecuencias.', en: 'üîí NEVER use RETAIN 0 HOURS in production without understanding the consequences.', pt: 'üîí NUNCA use RETAIN 0 HOURS em produ√ß√£o sem entender as consequ√™ncias.' },
        { es: 'üìÖ Program√° VACUUM semanal despu√©s de OPTIMIZE para m√°xima limpieza.', en: 'üìÖ Schedule weekly VACUUM after OPTIMIZE for maximum cleanup.', pt: 'üìÖ Programe VACUUM semanal depois do OPTIMIZE para m√°xima limpeza.' },
        { es: 'üí° Us√° DRY RUN primero para ver cu√°nto espacio vas a recuperar.', en: 'üí° Use DRY RUN first to see how much space you\'ll recover.', pt: 'üí° Use DRY RUN primeiro para ver quanto espa√ßo vai recuperar.' }
      ],
      externalLinks: [
        { title: 'VACUUM Command', url: 'https://docs.databricks.com/delta/vacuum.html', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øEjecutaste VACUUM DRY RUN y viste cu√°ntos archivos se eliminar√≠an?', en: '‚úÖ Did you run VACUUM DRY RUN and see how many files would be deleted?', pt: '‚úÖ Voc√™ executou VACUUM DRY RUN e viu quantos arquivos seriam deletados?' },
      xpReward: 25,
      estimatedMinutes: 25
    },
    {
      id: 'db-5-8',
      title: { es: 'Change Data Feed (CDF)', en: 'Change Data Feed (CDF)', pt: 'Change Data Feed (CDF)' },
      description: { es: 'CDF permite capturar solo los cambios incrementales (inserts, updates, deletes) para pipelines eficientes.', en: 'CDF allows capturing only incremental changes (inserts, updates, deletes) for efficient pipelines.', pt: 'CDF permite capturar apenas as mudan√ßas incrementais (inserts, updates, deletes) para pipelines eficientes.' },
      theory: {
        es: `## Change Data Feed: CDC Nativo de Delta Lake

Change Data Feed (CDF) te permite leer solo los cambios desde la √∫ltima lectura, en vez de re-leer toda la tabla.

### ¬øPor qu√© CDF?

\`\`\`
SIN CDF (full table scan cada vez):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tabla: 1 bill√≥n de filas               ‚îÇ
‚îÇ Cambios diarios: 10,000 filas          ‚îÇ
‚îÇ Pipeline lee: 1 bill√≥n de filas üò∞     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

CON CDF (solo cambios):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tabla: 1 bill√≥n de filas               ‚îÇ
‚îÇ Cambios diarios: 10,000 filas          ‚îÇ
‚îÇ Pipeline lee: 10,000 filas üöÄ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Habilitar CDF

\`\`\`sql
-- En tabla nueva
CREATE TABLE eventos (
    id INT, tipo STRING, timestamp TIMESTAMP
) USING DELTA
TBLPROPERTIES (delta.enableChangeDataFeed = true);

-- En tabla existente
ALTER TABLE eventos SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
\`\`\`

### Leer cambios

\`\`\`python
# Leer cambios desde versi√≥n espec√≠fica
cambios = spark.read.format("delta") \\
    .option("readChangeFeed", "true") \\
    .option("startingVersion", 5) \\
    .table("eventos")

# Leer cambios desde timestamp
cambios = spark.read.format("delta") \\
    .option("readChangeFeed", "true") \\
    .option("startingTimestamp", "2024-01-15 00:00:00") \\
    .table("eventos")

# Leer rango de versiones
cambios = spark.read.format("delta") \\
    .option("readChangeFeed", "true") \\
    .option("startingVersion", 5) \\
    .option("endingVersion", 10) \\
    .table("eventos")
\`\`\`

### Columnas especiales de CDF

CDF agrega columnas autom√°ticamente:

| Columna | Descripci√≥n |
|---------|-------------|
| \`_change_type\` | 'insert', 'update_preimage', 'update_postimage', 'delete' |
| \`_commit_version\` | Versi√≥n de Delta donde ocurri√≥ el cambio |
| \`_commit_timestamp\` | Timestamp del cambio |

\`\`\`python
# Ejemplo de output
# +----+------+-------------+---------------+-------------------+
# | id | tipo |_change_type |_commit_version|_commit_timestamp  |
# +----+------+-------------+---------------+-------------------+
# |  1 |  A   | insert      |             5 | 2024-01-15 10:00  |
# |  2 |  B   | delete      |             6 | 2024-01-15 11:00  |
# |  3 |  C   | update_pre  |             7 | 2024-01-15 12:00  |
# |  3 |  D   | update_post |             7 | 2024-01-15 12:00  |
# +----+------+-------------+---------------+-------------------+
\`\`\`

### Streaming con CDF

\`\`\`python
# Stream incremental de cambios
stream = spark.readStream.format("delta") \\
    .option("readChangeFeed", "true") \\
    .option("startingVersion", 0) \\
    .table("eventos")

# Procesar solo inserts y updates
stream_filtrado = stream.filter(
    "_change_type IN ('insert', 'update_postimage')"
)
\`\`\`

### Casos de uso de CDF

1. **ETL incremental**: Solo procesar cambios nuevos
2. **Sincronizaci√≥n**: Replicar cambios a otro sistema
3. **Auditor√≠a**: Log de todos los cambios
4. **ML Feature Store**: Actualizar features incrementalmente`,
        en: `## Change Data Feed: Delta Lake's Native CDC

CDF lets you read only changes since the last read, instead of re-reading the entire table.

### Enable CDF

\`\`\`sql
ALTER TABLE events SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
\`\`\`

### Read changes

\`\`\`python
changes = spark.read.format("delta") \\
    .option("readChangeFeed", "true") \\
    .option("startingVersion", 5) \\
    .table("events")
\`\`\`

CDF adds columns: _change_type, _commit_version, _commit_timestamp`,
        pt: `## Change Data Feed: CDC Nativo do Delta Lake

CDF permite ler apenas mudan√ßas desde a √∫ltima leitura.

\`\`\`sql
ALTER TABLE eventos SET TBLPROPERTIES (delta.enableChangeDataFeed = true);
\`\`\`

\`\`\`python
mudancas = spark.read.format("delta") \\
    .option("readChangeFeed", "true") \\
    .option("startingVersion", 5) \\
    .table("eventos")
\`\`\``
      },
      practicalTips: [
        { es: 'üöÄ CDF puede reducir el tiempo de ETL de horas a minutos.', en: 'üöÄ CDF can reduce ETL time from hours to minutes.', pt: 'üöÄ CDF pode reduzir o tempo de ETL de horas para minutos.' },
        { es: 'üíæ CDF tiene un peque√±o overhead de storage (~1%). Vale la pena.', en: 'üíæ CDF has a small storage overhead (~1%). Worth it.', pt: 'üíæ CDF tem um pequeno overhead de storage (~1%). Vale a pena.' },
        { es: 'üîç Guard√° el √∫ltimo _commit_version procesado para saber d√≥nde continuar.', en: 'üîç Save the last processed _commit_version to know where to continue.', pt: 'üîç Salve o √∫ltimo _commit_version processado para saber onde continuar.' }
      ],
      externalLinks: [
        { title: 'Change Data Feed', url: 'https://docs.databricks.com/delta/delta-change-data-feed.html', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øHabilitaste CDF, hiciste cambios en la tabla, y le√≠ste solo los cambios?', en: '‚úÖ Did you enable CDF, make changes to the table, and read only the changes?', pt: '‚úÖ Voc√™ habilitou CDF, fez mudan√ßas na tabela e leu apenas as mudan√ßas?' },
      xpReward: 35,
      estimatedMinutes: 35
    },
    {
      id: 'db-5-9',
      title: { es: 'Liquid Clustering (Nuevo)', en: 'Liquid Clustering (New)', pt: 'Liquid Clustering (Novo)' },
      description: { es: 'Liquid Clustering es la evoluci√≥n de Z-ORDER: clustering autom√°tico e incremental sin particiones.', en: 'Liquid Clustering is the evolution of Z-ORDER: automatic and incremental clustering without partitions.', pt: 'Liquid Clustering √© a evolu√ß√£o do Z-ORDER: clustering autom√°tico e incremental sem parti√ß√µes.' },
      theory: {
        es: `## Liquid Clustering: El Futuro del Ordenamiento

Liquid Clustering es una feature **exclusiva de Databricks** que reemplaza y mejora:
- Particionamiento tradicional
- Z-ORDER manual

### Problemas con el approach tradicional

\`\`\`
Particionamiento:
‚ùå Hay que elegir columnas de partici√≥n upfront
‚ùå Cambiar particiones requiere reescribir toda la tabla
‚ùå Over-partitioning = small files
‚ùå Under-partitioning = queries lentas

Z-ORDER:
‚ùå Hay que ejecutarlo manualmente
‚ùå No es incremental (reescribe toda la tabla)
‚ùå Costoso en tablas grandes
\`\`\`

### Liquid Clustering: La Soluci√≥n

\`\`\`
‚úÖ Clustering autom√°tico en escritura
‚úÖ Incremental (solo archivos nuevos)
‚úÖ Puedes cambiar columnas de clustering sin reescribir
‚úÖ Sin problema de small files
‚úÖ Data skipping optimizado
\`\`\`

### Crear tabla con Liquid Clustering

\`\`\`sql
-- Nueva tabla
CREATE TABLE ventas_liquid (
    id BIGINT,
    producto STRING,
    region STRING,
    fecha DATE,
    monto DOUBLE
) 
USING DELTA
CLUSTER BY (region, fecha);  -- ‚Üê Liquid Clustering

-- Convertir tabla existente
ALTER TABLE ventas_existente
CLUSTER BY (region, fecha);
\`\`\`

### Clustering autom√°tico

\`\`\`sql
-- Habilitar clustering autom√°tico
ALTER TABLE ventas_liquid 
SET TBLPROPERTIES ('delta.enableOptimizeWrite' = 'true');

-- Los datos se clusterean autom√°ticamente en cada escritura!
INSERT INTO ventas_liquid VALUES (1, 'Laptop', 'AR', '2024-01-15', 999.99);
\`\`\`

### Cambiar columnas de clustering

\`\`\`sql
-- Cambiar de (region, fecha) a (producto, region)
ALTER TABLE ventas_liquid
CLUSTER BY (producto, region);

-- No reescribe datos existentes
-- Nuevos datos usar√°n el nuevo clustering
-- OPTIMIZE aplicar√° el nuevo clustering a datos existentes
\`\`\`

### OPTIMIZE con Liquid Clustering

\`\`\`sql
-- Aplica clustering a datos no clusterados
OPTIMIZE ventas_liquid;
-- No necesita ZORDER BY - usa las columnas de CLUSTER BY autom√°ticamente
\`\`\`

### Verificar clustering

\`\`\`sql
DESCRIBE DETAIL ventas_liquid;
-- Ver√°s: clusteringColumns = ["region", "fecha"]
\`\`\`

### Liquid Clustering vs Partitioning vs Z-ORDER

| Feature | Partitioning | Z-ORDER | Liquid Clustering |
|---------|--------------|---------|-------------------|
| Setup | Upfront | Manual | Flexible |
| Cambiar columnas | Reescribir todo | N/A | F√°cil |
| Incremental | N/A | No | S√≠ |
| Small files | Problem√°tico | N/A | Resuelto |
| Costo de OPTIMIZE | N/A | Alto | Bajo |

### Cu√°ndo usar Liquid Clustering

\`\`\`
‚úÖ Tablas nuevas (siempre preferir LC)
‚úÖ Tablas que necesitan filtrar por m√∫ltiples columnas
‚úÖ Tablas con patrones de query cambiantes
‚úÖ Tablas con streaming (evita small files)

‚ùå Tablas muy peque√±as (<1GB)
‚ùå Si necesitas compatibilidad con Delta OSS
\`\`\``,
        en: `## Liquid Clustering: The Future of Data Layout

Liquid Clustering is a **Databricks exclusive** feature that replaces partitioning and Z-ORDER.

\`\`\`sql
CREATE TABLE sales_liquid (id BIGINT, region STRING, date DATE)
USING DELTA CLUSTER BY (region, date);
\`\`\`

Benefits:
- Automatic clustering on write
- Incremental (only new files)
- Can change clustering columns without rewriting
- No small files problem`,
        pt: `## Liquid Clustering: O Futuro do Layout de Dados

Liquid Clustering √© uma feature **exclusiva do Databricks** que substitui particionamento e Z-ORDER.

\`\`\`sql
CREATE TABLE vendas_liquid (id BIGINT, regiao STRING, data DATE)
USING DELTA CLUSTER BY (regiao, data);
\`\`\``
      },
      practicalTips: [
        { es: 'üÜï Liquid Clustering es relativamente nuevo. Para tablas nuevas, √∫salo siempre.', en: 'üÜï Liquid Clustering is relatively new. For new tables, always use it.', pt: 'üÜï Liquid Clustering √© relativamente novo. Para tabelas novas, sempre use.' },
        { es: 'üí° Liquid Clustering funciona mejor con Photon habilitado.', en: 'üí° Liquid Clustering works best with Photon enabled.', pt: 'üí° Liquid Clustering funciona melhor com Photon habilitado.' },
        { es: '‚ö†Ô∏è Es exclusivo de Databricks, no funciona en Delta Lake OSS.', en: '‚ö†Ô∏è It\'s Databricks exclusive, doesn\'t work in Delta Lake OSS.', pt: '‚ö†Ô∏è √â exclusivo do Databricks, n√£o funciona no Delta Lake OSS.' }
      ],
      externalLinks: [
        { title: 'Liquid Clustering', url: 'https://docs.databricks.com/delta/clustering.html', type: 'docs' }
      ],
      checkpoint: { es: '‚úÖ ¬øCreaste una tabla con CLUSTER BY y verificaste con DESCRIBE DETAIL?', en: '‚úÖ Did you create a table with CLUSTER BY and verify with DESCRIBE DETAIL?', pt: '‚úÖ Voc√™ criou uma tabela com CLUSTER BY e verificou com DESCRIBE DETAIL?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-5-10',
      title: { es: 'Proyecto: Pipeline Delta Lake Completo', en: 'Project: Complete Delta Lake Pipeline', pt: 'Projeto: Pipeline Delta Lake Completo' },
      description: { es: 'Constru√≠ un pipeline de producci√≥n usando todas las features de Delta Lake que aprendiste.', en: 'Build a production pipeline using all the Delta Lake features you learned.', pt: 'Construa um pipeline de produ√ß√£o usando todas as features do Delta Lake que aprendeu.' },
      theory: {
        es: `## Proyecto Final: E-commerce Data Pipeline

Vas a construir un pipeline completo para un e-commerce ficticio usando todas las t√©cnicas de Delta Lake.

### Arquitectura del Pipeline

\`\`\`
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ   RAW DATA      ‚îÇ
                  ‚îÇ  (JSON files)   ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BRONZE LAYER                               ‚îÇ
‚îÇ  ‚Ä¢ Ingesta raw con schema evolution                          ‚îÇ
‚îÇ  ‚Ä¢ Append-only (historial completo)                          ‚îÇ
‚îÇ  ‚Ä¢ CDF habilitado para downstream                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SILVER LAYER                               ‚îÇ
‚îÇ  ‚Ä¢ Limpieza y deduplicaci√≥n                                  ‚îÇ
‚îÇ  ‚Ä¢ MERGE para upserts                                        ‚îÇ
‚îÇ  ‚Ä¢ Schema enforcement estricto                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     GOLD LAYER                                ‚îÇ
‚îÇ  ‚Ä¢ Agregaciones de negocio                                   ‚îÇ
‚îÇ  ‚Ä¢ OPTIMIZE + Z-ORDER para queries                           ‚îÇ
‚îÇ  ‚Ä¢ M√©tricas y KPIs                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
\`\`\`

### Paso 1: Crear tablas Bronze

\`\`\`python
# Bronze: Pedidos raw
spark.sql("""
CREATE TABLE IF NOT EXISTS bronze_pedidos (
    order_id STRING,
    customer_id STRING,
    product_id STRING,
    quantity INT,
    price DOUBLE,
    order_date TIMESTAMP,
    raw_data STRING,
    ingestion_time TIMESTAMP
) USING DELTA
TBLPROPERTIES (
    delta.enableChangeDataFeed = true,
    delta.autoOptimize.optimizeWrite = true
)
""")
\`\`\`

### Paso 2: Silver con MERGE

\`\`\`python
from delta.tables import DeltaTable

# Leer cambios de bronze usando CDF
cambios = spark.read.format("delta") \\
    .option("readChangeFeed", "true") \\
    .option("startingVersion", last_processed_version) \\
    .table("bronze_pedidos") \\
    .filter("_change_type IN ('insert', 'update_postimage')")

# MERGE a silver (deduplicado)
silver = DeltaTable.forName(spark, "silver_pedidos")
silver.alias("target").merge(
    cambios.alias("source"),
    "target.order_id = source.order_id"
).whenMatchedUpdateAll() \\
 .whenNotMatchedInsertAll() \\
 .execute()
\`\`\`

### Paso 3: Gold con agregaciones

\`\`\`python
# Crear m√©tricas de negocio
spark.sql("""
CREATE OR REPLACE TABLE gold_ventas_diarias AS
SELECT 
    DATE(order_date) as fecha,
    COUNT(DISTINCT order_id) as total_pedidos,
    COUNT(DISTINCT customer_id) as clientes_unicos,
    SUM(quantity * price) as revenue,
    AVG(quantity * price) as ticket_promedio
FROM silver_pedidos
GROUP BY DATE(order_date)
""")

# Optimizar para queries
spark.sql("OPTIMIZE gold_ventas_diarias ZORDER BY (fecha)")
\`\`\`

### Paso 4: Mantenimiento

\`\`\`python
# Script de mantenimiento diario
tablas = ["bronze_pedidos", "silver_pedidos", "gold_ventas_diarias"]

for tabla in tablas:
    print(f"Mantenimiento de {tabla}...")
    spark.sql(f"OPTIMIZE {tabla}")
    spark.sql(f"VACUUM {tabla} RETAIN 168 HOURS")
    print(f"‚úÖ {tabla} completado")
\`\`\`

### Checklist del Proyecto

- [ ] Crear 3 tablas Delta (bronze, silver, gold)
- [ ] Implementar MERGE para deduplicaci√≥n
- [ ] Habilitar CDF en bronze
- [ ] Usar Time Travel para debugging
- [ ] Ejecutar OPTIMIZE + Z-ORDER en gold
- [ ] Configurar VACUUM automatizado
- [ ] Documentar el pipeline`,
        en: `## Final Project: E-commerce Data Pipeline

Build a complete pipeline using all Delta Lake techniques.

### Checklist

- [ ] Create 3 Delta tables (bronze, silver, gold)
- [ ] Implement MERGE for deduplication
- [ ] Enable CDF on bronze
- [ ] Use Time Travel for debugging
- [ ] Run OPTIMIZE + Z-ORDER on gold
- [ ] Configure automated VACUUM
- [ ] Document the pipeline`,
        pt: `## Projeto Final: Pipeline de E-commerce

Construa um pipeline completo usando todas as t√©cnicas de Delta Lake.

### Checklist

- [ ] Criar 3 tabelas Delta (bronze, silver, gold)
- [ ] Implementar MERGE para deduplica√ß√£o
- [ ] Habilitar CDF na bronze
- [ ] Usar Time Travel para debugging
- [ ] Executar OPTIMIZE + Z-ORDER na gold
- [ ] Configurar VACUUM automatizado
- [ ] Documentar o pipeline`
      },
      practicalTips: [
        { es: 'üìù Este proyecto puede ir directo a tu portfolio de GitHub.', en: 'üìù This project can go directly to your GitHub portfolio.', pt: 'üìù Este projeto pode ir direto para seu portf√≥lio no GitHub.' },
        { es: 'üéØ Practic√° explicar cada decisi√≥n de dise√±o - es pregunta de entrevista.', en: 'üéØ Practice explaining each design decision - it\'s an interview question.', pt: 'üéØ Pratique explicar cada decis√£o de design - √© pergunta de entrevista.' },
        { es: 'üí° Agreg√° monitoreo: cuenta de filas, tiempos de ejecuci√≥n, errores.', en: 'üí° Add monitoring: row counts, execution times, errors.', pt: 'üí° Adicione monitoramento: contagem de linhas, tempos de execu√ß√£o, erros.' }
      ],
      externalLinks: [
        { title: 'Medallion Architecture', url: 'https://docs.databricks.com/lakehouse/medallion.html', type: 'docs' },
        { title: 'Delta Lake Best Practices', url: 'https://docs.databricks.com/delta/best-practices.html', type: 'docs' }
      ],
      checkpoint: { es: 'üèÜ ¬øCompletaste el pipeline con las 3 capas y todas las operaciones de Delta Lake?', en: 'üèÜ Did you complete the pipeline with all 3 layers and all Delta Lake operations?', pt: 'üèÜ Voc√™ completou o pipeline com as 3 camadas e todas as opera√ß√µes do Delta Lake?' },
      xpReward: 75,
      estimatedMinutes: 90
    }
  ]
};
