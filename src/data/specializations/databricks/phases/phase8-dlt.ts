/**
 * FASE 8: Delta Live Tables (DLT)
 * 8 pasos para pipelines declarativos
 */

import { DatabricksPhase } from '../types';

export const PHASE_8_DLT: DatabricksPhase = {
  id: 'db-phase-8',
  number: 8,
  title: { es: 'Delta Live Tables', en: 'Delta Live Tables', pt: 'Delta Live Tables' },
  subtitle: { es: 'Pipelines declarativos', en: 'Declarative pipelines', pt: 'Pipelines declarativos' },
  description: { es: 'DLT simplifica la creaciÃ³n de pipelines de datos confiables con una sintaxis declarativa y manejo automÃ¡tico de calidad. âš ï¸ Nota: DLT no estÃ¡ disponible en Community Edition. Practica con los labs gratuitos de Databricks Academy o el trial de 14 dÃ­as.', en: 'DLT simplifies creating reliable data pipelines with declarative syntax and automatic quality handling. âš ï¸ Note: DLT is not available in Community Edition. Practice with free Databricks Academy labs or 14-day trial.', pt: 'DLT simplifica a criaÃ§Ã£o de pipelines de dados confiÃ¡veis com sintaxe declarativa e tratamento automÃ¡tico de qualidade. âš ï¸ Nota: DLT nÃ£o estÃ¡ disponÃ­vel no Community Edition. Pratique com os labs gratuitos do Databricks Academy ou trial de 14 dias.' },
  icon: 'ğŸ“Š',
  color: 'teal',
  estimatedDays: '4-5 dÃ­as',
  steps: [
    { id: 'db-8-1', title: { es: 'Â¿QuÃ© es Delta Live Tables?', en: 'What is Delta Live Tables?', pt: 'O que Ã© Delta Live Tables?' }, description: { es: 'DLT: la forma mÃ¡s fÃ¡cil de crear pipelines en Databricks.', en: 'DLT: the easiest way to create pipelines in Databricks.', pt: 'DLT: a forma mais fÃ¡cil de criar pipelines no Databricks.' },
      theory: { es: `## Delta Live Tables\n\nDLT es un framework declarativo para definir pipelines:\n\n### Ventajas:\n- **Declarativo**: DefinÃ­s QUÃ‰ querÃ©s, no CÃ“MO\n- **Calidad automÃ¡tica**: Expectations integradas\n- **Linaje automÃ¡tico**: Se genera solo\n- **Manejo de errores**: Quarantine automÃ¡tico\n- **OptimizaciÃ³n**: Auto-OPTIMIZE, Auto-VACUUM\n\n### Sintaxis bÃ¡sica:\n\`\`\`python\nimport dlt\n\n@dlt.table\ndef mi_tabla_bronze():\n    return spark.read.csv("/data/raw")\n\n@dlt.table\ndef mi_tabla_silver():\n    return dlt.read("mi_tabla_bronze").filter("valid = true")\n\`\`\``, en: `## Delta Live Tables\n\nDLT is a declarative framework for defining pipelines:\n\n### Advantages:\n- **Declarative**: Define WHAT you want, not HOW\n- **Automatic quality**: Built-in expectations\n- **Automatic lineage**: Self-generated\n- **Error handling**: Automatic quarantine\n- **Optimization**: Auto-OPTIMIZE, Auto-VACUUM\n\n### Basic syntax:\n\`\`\`python\nimport dlt\n\n@dlt.table\ndef my_bronze_table():\n    return spark.read.csv("/data/raw")\n\n@dlt.table\ndef my_silver_table():\n    return dlt.read("my_bronze_table").filter("valid = true")\n\`\`\``, pt: `## Delta Live Tables\n\nDLT Ã© um framework declarativo para definir pipelines:\n\n### Vantagens:\n- **Declarativo**: Define O QUÃŠ vocÃª quer, nÃ£o COMO\n- **Qualidade automÃ¡tica**: Expectations integradas\n- **Linhagem automÃ¡tica**: Se gera sozinha\n- **Tratamento de erros**: Quarantine automÃ¡tico\n- **OtimizaÃ§Ã£o**: Auto-OPTIMIZE, Auto-VACUUM\n\n### Sintaxe bÃ¡sica:\n\`\`\`python\nimport dlt\n\n@dlt.table\ndef minha_tabela_bronze():\n    return spark.read.csv("/data/raw")\n\n@dlt.table\ndef minha_tabela_silver():\n    return dlt.read("minha_tabela_bronze").filter("valid = true")\n\`\`\`` },
      practicalTips: [{ es: 'ğŸ’¡ DLT es ideal para pipelines Medallion (Bronze/Silver/Gold).', en: 'ğŸ’¡ DLT is ideal for Medallion pipelines (Bronze/Silver/Gold).', pt: 'ğŸ’¡ DLT Ã© ideal para pipelines Medallion (Bronze/Silver/Gold).' }],
      externalLinks: [{ title: 'DLT Overview', url: 'https://docs.databricks.com/delta-live-tables/index.html', type: 'docs' }],
      checkpoint: { es: 'ğŸ¤” Â¿CuÃ¡les son las ventajas de DLT sobre notebooks tradicionales?', en: 'ğŸ¤” What are the advantages of DLT over traditional notebooks?', pt: 'ğŸ¤” Quais sÃ£o as vantagens do DLT sobre notebooks tradicionais?' },
      xpReward: 20, estimatedMinutes: 15 },
    { id: 'db-8-2', title: { es: 'Tables vs Views vs Streaming', en: 'Tables vs Views vs Streaming', pt: 'Tables vs Views vs Streaming' }, description: { es: 'EntendÃ© los 3 tipos de objetos en DLT.', en: 'Understand the 3 types of objects in DLT.', pt: 'Entenda os 3 tipos de objetos no DLT.' },
      theory: { es: `## Tipos de Objetos en DLT\n\n### @dlt.table (Materialized)\n\`\`\`python\n@dlt.table\ndef bronze_ventas():\n    return spark.read.csv("/data/ventas")\n\`\`\`\n- Datos persistidos en Delta\n- Consume storage\n- Ideal para datos que se consultan frecuentemente\n\n### @dlt.view (Virtual)\n\`\`\`python\n@dlt.view\ndef ventas_filtradas():\n    return dlt.read("bronze_ventas").filter("monto > 0")\n\`\`\`\n- No persiste datos\n- Se computa on-demand\n- Ideal para transformaciones intermedias\n\n### @dlt.table (Streaming)\n\`\`\`python\n@dlt.table\ndef bronze_streaming():\n    return spark.readStream.format("cloudFiles") \\\n        .load("/data/incoming/")\n\`\`\`\n- Procesa datos incrementalmente\n- Usa Structured Streaming\n- Ideal para datos en tiempo real`, en: `## Object Types in DLT\n\n### @dlt.table (Materialized)\n\`\`\`python\n@dlt.table\ndef bronze_sales():\n    return spark.read.csv("/data/sales")\n\`\`\`\n- Data persisted in Delta\n- Consumes storage\n- Ideal for frequently queried data\n\n### @dlt.view (Virtual)\n\`\`\`python\n@dlt.view\ndef filtered_sales():\n    return dlt.read("bronze_sales").filter("amount > 0")\n\`\`\`\n- No data persisted\n- Computed on-demand\n- Ideal for intermediate transformations\n\n### @dlt.table (Streaming)\n\`\`\`python\n@dlt.table\ndef bronze_streaming():\n    return spark.readStream.format("cloudFiles") \\\n        .load("/data/incoming/")\n\`\`\`\n- Processes data incrementally\n- Uses Structured Streaming\n- Ideal for real-time data`, pt: `## Tipos de Objetos no DLT\n\n### @dlt.table (Materializado)\n\`\`\`python\n@dlt.table\ndef bronze_vendas():\n    return spark.read.csv("/data/vendas")\n\`\`\`\n- Dados persistidos em Delta\n- Consome storage\n- Ideal para dados consultados frequentemente\n\n### @dlt.view (Virtual)\n\`\`\`python\n@dlt.view\ndef vendas_filtradas():\n    return dlt.read("bronze_vendas").filter("valor > 0")\n\`\`\`\n- NÃ£o persiste dados\n- Computado on-demand\n- Ideal para transformaÃ§Ãµes intermediÃ¡rias\n\n### @dlt.table (Streaming)\n\`\`\`python\n@dlt.table\ndef bronze_streaming():\n    return spark.readStream.format("cloudFiles") \\\n        .load("/data/incoming/")\n\`\`\`\n- Processa dados incrementalmente\n- Usa Structured Streaming\n- Ideal para dados em tempo real` },
      practicalTips: [{ es: 'âš¡ UsÃ¡ views para transformaciones intermedias que no necesitÃ¡s persistir.', en: 'âš¡ Use views for intermediate transformations you don\'t need to persist.', pt: 'âš¡ Use views para transformaÃ§Ãµes intermediÃ¡rias que vocÃª nÃ£o precisa persistir.' }],
      externalLinks: [{ title: 'DLT Datasets', url: 'https://docs.databricks.com/delta-live-tables/python-ref.html', type: 'docs' }],
      checkpoint: { es: 'ğŸ¤” Â¿CuÃ¡ndo usarÃ­as @dlt.view vs @dlt.table?', en: 'ğŸ¤” When would you use @dlt.view vs @dlt.table?', pt: 'ğŸ¤” Quando vocÃª usaria @dlt.view vs @dlt.table?' },
      xpReward: 25, estimatedMinutes: 20 },
    { id: 'db-8-3', title: { es: 'Expectations: Data Quality', en: 'Expectations: Data Quality', pt: 'Expectations: Data Quality' }, description: { es: 'DefinÃ­ reglas de calidad que se verifican automÃ¡ticamente.', en: 'Define quality rules that are automatically verified.', pt: 'Defina regras de qualidade que sÃ£o verificadas automaticamente.' },
      theory: { es: `## Expectations\n\nExpectations validan datos automÃ¡ticamente.\n\n### Tipos de acciones:\n\n**expect (warn)**:\n\`\`\`python\n@dlt.table\n@dlt.expect("valid_amount", "amount > 0")\ndef silver_ventas():\n    return dlt.read("bronze_ventas")\n\`\`\`\n- Registra violaciÃ³n, NO falla\n- Datos pasan igual\n\n**expect_or_drop**:\n\`\`\`python\n@dlt.expect_or_drop("valid_email", "email IS NOT NULL")\n\`\`\`\n- Elimina filas invÃ¡lidas\n- Datos limpios pasan\n\n**expect_or_fail**:\n\`\`\`python\n@dlt.expect_or_fail("critical_field", "id IS NOT NULL")\n\`\`\`\n- Pipeline falla si hay violaciÃ³n\n- Para reglas crÃ­ticas\n\n### MÃºltiples expectations:\n\`\`\`python\n@dlt.expect_all({\n    "valid_id": "id IS NOT NULL",\n    "valid_amount": "amount > 0",\n    "valid_date": "date <= current_date()"\n})\n\`\`\``, en: `## Expectations\n\nExpectations validate data automatically.\n\n### Action types:\n\n**expect (warn)**:\n\`\`\`python\n@dlt.table\n@dlt.expect("valid_amount", "amount > 0")\ndef silver_sales():\n    return dlt.read("bronze_sales")\n\`\`\`\n- Records violation, does NOT fail\n- Data passes through\n\n**expect_or_drop**:\n\`\`\`python\n@dlt.expect_or_drop("valid_email", "email IS NOT NULL")\n\`\`\`\n- Removes invalid rows\n- Clean data passes\n\n**expect_or_fail**:\n\`\`\`python\n@dlt.expect_or_fail("critical_field", "id IS NOT NULL")\n\`\`\`\n- Pipeline fails if violation\n- For critical rules\n\n### Multiple expectations:\n\`\`\`python\n@dlt.expect_all({\n    "valid_id": "id IS NOT NULL",\n    "valid_amount": "amount > 0",\n    "valid_date": "date <= current_date()"\n})\n\`\`\``, pt: `## Expectations\n\nExpectations validam dados automaticamente.\n\n### Tipos de aÃ§Ãµes:\n\n**expect (warn)**:\n\`\`\`python\n@dlt.table\n@dlt.expect("valid_amount", "amount > 0")\ndef silver_vendas():\n    return dlt.read("bronze_vendas")\n\`\`\`\n- Registra violaÃ§Ã£o, NÃƒO falha\n- Dados passam igual\n\n**expect_or_drop**:\n\`\`\`python\n@dlt.expect_or_drop("valid_email", "email IS NOT NULL")\n\`\`\`\n- Remove linhas invÃ¡lidas\n- Dados limpos passam\n\n**expect_or_fail**:\n\`\`\`python\n@dlt.expect_or_fail("critical_field", "id IS NOT NULL")\n\`\`\`\n- Pipeline falha se hÃ¡ violaÃ§Ã£o\n- Para regras crÃ­ticas\n\n### MÃºltiplas expectations:\n\`\`\`python\n@dlt.expect_all({\n    "valid_id": "id IS NOT NULL",\n    "valid_amount": "amount > 0",\n    "valid_date": "date <= current_date()"\n})\n\`\`\`` },
      practicalTips: [{ es: 'ğŸ’¡ UsÃ¡ expect_or_drop para limpieza automÃ¡tica sin fallar el pipeline.', en: 'ğŸ’¡ Use expect_or_drop for automatic cleanup without failing the pipeline.', pt: 'ğŸ’¡ Use expect_or_drop para limpeza automÃ¡tica sem falhar o pipeline.' }],
      externalLinks: [{ title: 'Expectations', url: 'https://docs.databricks.com/delta-live-tables/expectations.html', type: 'docs' }],
      checkpoint: { es: 'âœ… Â¿Agregaste expectations a tus tablas DLT?', en: 'âœ… Did you add expectations to your DLT tables?', pt: 'âœ… VocÃª adicionou expectations Ã s suas tabelas DLT?' },
      xpReward: 35, estimatedMinutes: 30 },
    { id: 'db-8-4', title: { es: 'Auto Loader: Ingesta Incremental', en: 'Auto Loader: Incremental Ingestion', pt: 'Auto Loader: IngestÃ£o Incremental' }, description: { es: 'Auto Loader procesa archivos nuevos automÃ¡ticamente.', en: 'Auto Loader processes new files automatically.', pt: 'Auto Loader processa arquivos novos automaticamente.' },
      theory: { es: `## Auto Loader\n\nProcesa solo archivos NUEVOS automÃ¡ticamente.\n\n\`\`\`python\n@dlt.table\ndef bronze_raw():\n    return (\n        spark.readStream\n        .format("cloudFiles")\n        .option("cloudFiles.format", "json")\n        .option("cloudFiles.schemaLocation", "/schema")\n        .option("cloudFiles.schemaEvolutionMode", "rescue")\n        .load("/data/incoming/")\n    )\n\`\`\`\n\n### Opciones importantes:\n- \`cloudFiles.format\`: csv, json, parquet, etc.\n- \`cloudFiles.schemaLocation\`: Donde guardar schema inferido\n- \`cloudFiles.schemaEvolutionMode\`: addNewColumns, rescue, failOnNewColumns\n\n### Rescue column:\nSi hay columnas nuevas o datos malformados:\n\`\`\`python\n.option("cloudFiles.schemaEvolutionMode", "rescue")\n# Crea columna _rescued_data con datos problemÃ¡ticos\n\`\`\``, en: `## Auto Loader\n\nProcesses only NEW files automatically.\n\n\`\`\`python\n@dlt.table\ndef bronze_raw():\n    return (\n        spark.readStream\n        .format("cloudFiles")\n        .option("cloudFiles.format", "json")\n        .option("cloudFiles.schemaLocation", "/schema")\n        .option("cloudFiles.schemaEvolutionMode", "rescue")\n        .load("/data/incoming/")\n    )\n\`\`\`\n\n### Important options:\n- \`cloudFiles.format\`: csv, json, parquet, etc.\n- \`cloudFiles.schemaLocation\`: Where to save inferred schema\n- \`cloudFiles.schemaEvolutionMode\`: addNewColumns, rescue, failOnNewColumns\n\n### Rescue column:\nIf there are new columns or malformed data:\n\`\`\`python\n.option("cloudFiles.schemaEvolutionMode", "rescue")\n# Creates _rescued_data column with problematic data\n\`\`\``, pt: `## Auto Loader\n\nProcessa apenas arquivos NOVOS automaticamente.\n\n\`\`\`python\n@dlt.table\ndef bronze_raw():\n    return (\n        spark.readStream\n        .format("cloudFiles")\n        .option("cloudFiles.format", "json")\n        .option("cloudFiles.schemaLocation", "/schema")\n        .option("cloudFiles.schemaEvolutionMode", "rescue")\n        .load("/data/incoming/")\n    )\n\`\`\`\n\n### OpÃ§Ãµes importantes:\n- \`cloudFiles.format\`: csv, json, parquet, etc.\n- \`cloudFiles.schemaLocation\`: Onde salvar schema inferido\n- \`cloudFiles.schemaEvolutionMode\`: addNewColumns, rescue, failOnNewColumns\n\n### Rescue column:\nSe hÃ¡ colunas novas ou dados malformados:\n\`\`\`python\n.option("cloudFiles.schemaEvolutionMode", "rescue")\n# Cria coluna _rescued_data com dados problemÃ¡ticos\n\`\`\`` },
      practicalTips: [{ es: 'ğŸš€ Auto Loader es mucho mÃ¡s eficiente que listar y filtrar archivos manualmente.', en: 'ğŸš€ Auto Loader is much more efficient than manually listing and filtering files.', pt: 'ğŸš€ Auto Loader Ã© muito mais eficiente que listar e filtrar arquivos manualmente.' }],
      externalLinks: [{ title: 'Auto Loader', url: 'https://docs.databricks.com/ingestion/auto-loader/index.html', type: 'docs' }],
      checkpoint: { es: 'âœ… Â¿Configuraste Auto Loader para ingesta incremental?', en: 'âœ… Did you configure Auto Loader for incremental ingestion?', pt: 'âœ… VocÃª configurou Auto Loader para ingestÃ£o incremental?' },
      xpReward: 30, estimatedMinutes: 25 },
    { id: 'db-8-5', title: { es: 'Crear y Configurar Pipeline', en: 'Create and Configure Pipeline', pt: 'Criar e Configurar Pipeline' }, description: { es: 'CÃ³mo desplegar tu pipeline DLT en Databricks.', en: 'How to deploy your DLT pipeline in Databricks.', pt: 'Como fazer deploy do seu pipeline DLT no Databricks.' },
      theory: { es: `## Crear Pipeline DLT\n\n### En UI:\n1. Workflows > Delta Live Tables > Create Pipeline\n2. Configurar:\n   - **Name**: "mi-pipeline-dlt"\n   - **Source**: Notebook con cÃ³digo DLT\n   - **Target**: Schema donde crear tablas\n   - **Storage**: UbicaciÃ³n de datos\n\n### Modos de pipeline:\n- **Triggered**: Ejecuta una vez\n- **Continuous**: Siempre corriendo (streaming)\n\n### Development vs Production:\n\`\`\`json\n{\n  "development": true,  // Modo dev: no auto-retry\n  "configuration": {\n    "pipelines.enableTrackHistory": "true"\n  }\n}\n\`\`\`\n\n### Clusters:\n- Default: Clusters automÃ¡ticos\n- Custom: ConfiguraciÃ³n manual`, en: `## Create DLT Pipeline\n\n### In UI:\n1. Workflows > Delta Live Tables > Create Pipeline\n2. Configure:\n   - **Name**: "my-dlt-pipeline"\n   - **Source**: Notebook with DLT code\n   - **Target**: Schema where to create tables\n   - **Storage**: Data location\n\n### Pipeline modes:\n- **Triggered**: Runs once\n- **Continuous**: Always running (streaming)\n\n### Development vs Production:\n\`\`\`json\n{\n  "development": true,  // Dev mode: no auto-retry\n  "configuration": {\n    "pipelines.enableTrackHistory": "true"\n  }\n}\n\`\`\`\n\n### Clusters:\n- Default: Automatic clusters\n- Custom: Manual configuration`, pt: `## Criar Pipeline DLT\n\n### Na UI:\n1. Workflows > Delta Live Tables > Create Pipeline\n2. Configurar:\n   - **Name**: "meu-pipeline-dlt"\n   - **Source**: Notebook com cÃ³digo DLT\n   - **Target**: Schema onde criar tabelas\n   - **Storage**: LocalizaÃ§Ã£o de dados\n\n### Modos de pipeline:\n- **Triggered**: Executa uma vez\n- **Continuous**: Sempre rodando (streaming)\n\n### Development vs Production:\n\`\`\`json\n{\n  "development": true,  // Modo dev: sem auto-retry\n  "configuration": {\n    "pipelines.enableTrackHistory": "true"\n  }\n}\n\`\`\`\n\n### Clusters:\n- PadrÃ£o: Clusters automÃ¡ticos\n- Custom: ConfiguraÃ§Ã£o manual` },
      practicalTips: [{ es: 'ğŸ’¡ Siempre probÃ¡ en modo Development antes de pasarlo a Production.', en: 'ğŸ’¡ Always test in Development mode before moving to Production.', pt: 'ğŸ’¡ Sempre teste em modo Development antes de passar para Production.' }],
      externalLinks: [{ title: 'Create Pipeline', url: 'https://docs.databricks.com/delta-live-tables/configure-pipeline.html', type: 'docs' }],
      checkpoint: { es: 'âœ… Â¿Creaste un pipeline DLT y lo ejecutaste?', en: 'âœ… Did you create a DLT pipeline and run it?', pt: 'âœ… VocÃª criou um pipeline DLT e executou?' },
      xpReward: 30, estimatedMinutes: 25 },
    { id: 'db-8-6', title: { es: 'SCD Type 2 con DLT', en: 'SCD Type 2 with DLT', pt: 'SCD Type 2 com DLT' }, description: { es: 'ImplementÃ¡ Slowly Changing Dimensions con apply_changes.', en: 'Implement Slowly Changing Dimensions with apply_changes.', pt: 'Implemente Slowly Changing Dimensions com apply_changes.' },
      theory: { es: `## SCD Type 2 en DLT\n\n\`\`\`python\nimport dlt\nfrom dlt import read_stream\n\ndlt.create_streaming_table("clientes_scd")\n\ndlt.apply_changes(\n    target="clientes_scd",\n    source="bronze_clientes",\n    keys=["cliente_id"],\n    sequence_by="updated_at",\n    stored_as_scd_type=2\n)\n\`\`\`\n\n### Columnas generadas:\n- \`__START_AT\`: Fecha inicio de validez\n- \`__END_AT\`: Fecha fin de validez\n- \`__CHANGE_TYPE\`: insert, update_preimage, update_postimage\n\n### SCD Type 1 (sobrescribir):\n\`\`\`python\ndlt.apply_changes(\n    ...,\n    stored_as_scd_type=1  # Sobrescribe, no guarda historia\n)\n\`\`\``, en: `## SCD Type 2 in DLT\n\n\`\`\`python\nimport dlt\nfrom dlt import read_stream\n\ndlt.create_streaming_table("customers_scd")\n\ndlt.apply_changes(\n    target="customers_scd",\n    source="bronze_customers",\n    keys=["customer_id"],\n    sequence_by="updated_at",\n    stored_as_scd_type=2\n)\n\`\`\`\n\n### Generated columns:\n- \`__START_AT\`: Validity start date\n- \`__END_AT\`: Validity end date\n- \`__CHANGE_TYPE\`: insert, update_preimage, update_postimage\n\n### SCD Type 1 (overwrite):\n\`\`\`python\ndlt.apply_changes(\n    ...,\n    stored_as_scd_type=1  # Overwrites, doesn't keep history\n)\n\`\`\``, pt: `## SCD Type 2 no DLT\n\n\`\`\`python\nimport dlt\nfrom dlt import read_stream\n\ndlt.create_streaming_table("clientes_scd")\n\ndlt.apply_changes(\n    target="clientes_scd",\n    source="bronze_clientes",\n    keys=["cliente_id"],\n    sequence_by="updated_at",\n    stored_as_scd_type=2\n)\n\`\`\`\n\n### Colunas geradas:\n- \`__START_AT\`: Data inÃ­cio de validade\n- \`__END_AT\`: Data fim de validade\n- \`__CHANGE_TYPE\`: insert, update_preimage, update_postimage\n\n### SCD Type 1 (sobrescrever):\n\`\`\`python\ndlt.apply_changes(\n    ...,\n    stored_as_scd_type=1  # Sobrescreve, nÃ£o guarda histÃ³rico\n)\n\`\`\`` },
      practicalTips: [{ es: 'â­ apply_changes simplifica MUCHO la lÃ³gica de SCD que antes era muy compleja.', en: 'â­ apply_changes greatly simplifies SCD logic that was previously very complex.', pt: 'â­ apply_changes simplifica MUITO a lÃ³gica de SCD que antes era muito complexa.' }],
      externalLinks: [{ title: 'apply_changes', url: 'https://docs.databricks.com/delta-live-tables/cdc.html', type: 'docs' }],
      checkpoint: { es: 'âœ… Â¿Implementaste SCD Type 2 con apply_changes?', en: 'âœ… Did you implement SCD Type 2 with apply_changes?', pt: 'âœ… VocÃª implementou SCD Type 2 com apply_changes?' },
      xpReward: 40, estimatedMinutes: 35 },
    { id: 'db-8-7', title: { es: 'Monitoring y Event Logs', en: 'Monitoring and Event Logs', pt: 'Monitoramento e Event Logs' }, description: { es: 'MonitoreÃ¡ la salud y mÃ©tricas de tu pipeline.', en: 'Monitor the health and metrics of your pipeline.', pt: 'Monitore a saÃºde e mÃ©tricas do seu pipeline.' },
      theory: { es: `## Monitoreo de DLT\n\n### Pipeline Event Logs:\n\`\`\`sql\nSELECT * FROM event_log(TABLE(mi_pipeline))\nWHERE event_type = 'flow_progress'\nORDER BY timestamp DESC;\n\`\`\`\n\n### MÃ©tricas disponibles:\n- Filas procesadas\n- Expectations violadas\n- Latencia\n- Backlog\n\n### Query mÃ©tricas de calidad:\n\`\`\`sql\nSELECT \n  expectation,\n  passed_records,\n  failed_records,\n  failed_records * 100.0 / (passed_records + failed_records) as failure_rate\nFROM event_log(...)\nWHERE event_type = 'flow_progress'\n\`\`\`\n\n### Alertas:\nIntegrar con Workflows para notificaciones.`, en: `## DLT Monitoring\n\n### Pipeline Event Logs:\n\`\`\`sql\nSELECT * FROM event_log(TABLE(my_pipeline))\nWHERE event_type = 'flow_progress'\nORDER BY timestamp DESC;\n\`\`\`\n\n### Available metrics:\n- Rows processed\n- Violated expectations\n- Latency\n- Backlog\n\n### Query quality metrics:\n\`\`\`sql\nSELECT \n  expectation,\n  passed_records,\n  failed_records,\n  failed_records * 100.0 / (passed_records + failed_records) as failure_rate\nFROM event_log(...)\nWHERE event_type = 'flow_progress'\n\`\`\`\n\n### Alerts:\nIntegrate with Workflows for notifications.`, pt: `## Monitoramento de DLT\n\n### Pipeline Event Logs:\n\`\`\`sql\nSELECT * FROM event_log(TABLE(meu_pipeline))\nWHERE event_type = 'flow_progress'\nORDER BY timestamp DESC;\n\`\`\`\n\n### MÃ©tricas disponÃ­veis:\n- Linhas processadas\n- Expectations violadas\n- LatÃªncia\n- Backlog\n\n### Consultar mÃ©tricas de qualidade:\n\`\`\`sql\nSELECT \n  expectation,\n  passed_records,\n  failed_records,\n  failed_records * 100.0 / (passed_records + failed_records) as failure_rate\nFROM event_log(...)\nWHERE event_type = 'flow_progress'\n\`\`\`\n\n### Alertas:\nIntegrar com Workflows para notificaÃ§Ãµes.` },
      practicalTips: [{ es: 'ğŸ“Š CreÃ¡ un dashboard con las mÃ©tricas de expectations para monitoreo continuo.', en: 'ğŸ“Š Create a dashboard with expectations metrics for continuous monitoring.', pt: 'ğŸ“Š Crie um dashboard com mÃ©tricas de expectations para monitoramento contÃ­nuo.' }],
      externalLinks: [{ title: 'Monitoring', url: 'https://docs.databricks.com/delta-live-tables/observability.html', type: 'docs' }],
      checkpoint: { es: 'âœ… Â¿Consultaste los event logs de tu pipeline?', en: 'âœ… Did you query your pipeline\'s event logs?', pt: 'âœ… VocÃª consultou os event logs do seu pipeline?' },
      xpReward: 25, estimatedMinutes: 25 },
    { id: 'db-8-8', title: { es: 'Proyecto: Pipeline Medallion con DLT', en: 'Project: Medallion Pipeline with DLT', pt: 'Projeto: Pipeline Medallion com DLT' }, description: { es: 'ConstruÃ­ un pipeline completo Bronze/Silver/Gold.', en: 'Build a complete Bronze/Silver/Gold pipeline.', pt: 'Construa um pipeline completo Bronze/Silver/Gold.' },
      theory: { es: `## Proyecto: Medallion con DLT\n\n\`\`\`python\nimport dlt\n\n# BRONZE\n@dlt.table\ndef bronze_raw():\n    return spark.readStream.format("cloudFiles")...\n\n# SILVER\n@dlt.table\n@dlt.expect_or_drop("valid_id", "id IS NOT NULL")\n@dlt.expect("valid_amount", "amount > 0")\ndef silver_cleaned():\n    return dlt.read("bronze_raw").dropDuplicates()\n\n# GOLD\n@dlt.table\ndef gold_daily_metrics():\n    return (\n        dlt.read("silver_cleaned")\n        .groupBy("date")\n        .agg(sum("amount").alias("total"))\n    )\n\`\`\`\n\n### Checklist:\n- [ ] Auto Loader para ingesta\n- [ ] 3 capas: Bronze, Silver, Gold\n- [ ] Expectations en Silver\n- [ ] Agregaciones en Gold\n- [ ] Pipeline ejecutÃ¡ndose exitosamente\n- [ ] Event logs consultados`, en: `## Project: Medallion with DLT\n\n\`\`\`python\nimport dlt\n\n# BRONZE\n@dlt.table\ndef bronze_raw():\n    return spark.readStream.format("cloudFiles")...\n\n# SILVER\n@dlt.table\n@dlt.expect_or_drop("valid_id", "id IS NOT NULL")\n@dlt.expect("valid_amount", "amount > 0")\ndef silver_cleaned():\n    return dlt.read("bronze_raw").dropDuplicates()\n\n# GOLD\n@dlt.table\ndef gold_daily_metrics():\n    return (\n        dlt.read("silver_cleaned")\n        .groupBy("date")\n        .agg(sum("amount").alias("total"))\n    )\n\`\`\`\n\n### Checklist:\n- [ ] Auto Loader for ingestion\n- [ ] 3 layers: Bronze, Silver, Gold\n- [ ] Expectations in Silver\n- [ ] Aggregations in Gold\n- [ ] Pipeline running successfully\n- [ ] Event logs queried`, pt: `## Projeto: Medallion com DLT\n\n\`\`\`python\nimport dlt\n\n# BRONZE\n@dlt.table\ndef bronze_raw():\n    return spark.readStream.format("cloudFiles")...\n\n# SILVER\n@dlt.table\n@dlt.expect_or_drop("valid_id", "id IS NOT NULL")\n@dlt.expect("valid_amount", "amount > 0")\ndef silver_cleaned():\n    return dlt.read("bronze_raw").dropDuplicates()\n\n# GOLD\n@dlt.table\ndef gold_daily_metrics():\n    return (\n        dlt.read("silver_cleaned")\n        .groupBy("date")\n        .agg(sum("amount").alias("total"))\n    )\n\`\`\`\n\n### Checklist:\n- [ ] Auto Loader para ingestÃ£o\n- [ ] 3 camadas: Bronze, Silver, Gold\n- [ ] Expectations no Silver\n- [ ] AgregaÃ§Ãµes no Gold\n- [ ] Pipeline executando com sucesso\n- [ ] Event logs consultados` },
      practicalTips: [{ es: 'ğŸ† DLT es el estÃ¡ndar actual para pipelines en Databricks. Dominalo!', en: 'ğŸ† DLT is the current standard for pipelines in Databricks. Master it!', pt: 'ğŸ† DLT Ã© o padrÃ£o atual para pipelines no Databricks. Domine!' }],
      externalLinks: [{ title: 'DLT Best Practices', url: 'https://docs.databricks.com/delta-live-tables/best-practices.html', type: 'docs' }],
      checkpoint: { es: 'ğŸ† Â¿Tu pipeline Medallion estÃ¡ funcionando con expectations?', en: 'ğŸ† Is your Medallion pipeline working with expectations?', pt: 'ğŸ† Seu pipeline Medallion estÃ¡ funcionando com expectations?' },
      xpReward: 100, estimatedMinutes: 90 }
  ]
};


