/**
 * FASES 9-12: SQL Warehouse, MLflow, Best Practices, CertificaciÃ³n
 * 33 pasos finales para completar la especializaciÃ³n
 * 
 * ACTUALIZADO: Enero 2026 - Notas sobre Free Edition:
 * - SQL Warehouse usa serverless automÃ¡ticamente en Free Edition
 * - MLflow estÃ¡ disponible en Free Edition
 * - AI/BI Dashboards (nuevos) reemplazan los legacy dashboards
 * - Databricks Academy tiene cursos gratuitos para certificaciÃ³n
 */

import { DatabricksPhase, DatabricksStep } from '../types';

// Helper para crear steps compactos
const createStep = (id: string, titleEs: string, titleEn: string, titlePt: string, descEs: string, descEn: string, descPt: string, theoryEs: string, theoryEn: string, theoryPt: string, tip: {es: string, en: string, pt: string}, checkEs: string, checkEn: string, checkPt: string, xp: number, mins: number): DatabricksStep => ({
  id, title: {es: titleEs, en: titleEn, pt: titlePt}, description: {es: descEs, en: descEn, pt: descPt}, theory: {es: theoryEs, en: theoryEn, pt: theoryPt},
  practicalTips: [tip], externalLinks: [], checkpoint: {es: checkEs, en: checkEn, pt: checkPt}, xpReward: xp, estimatedMinutes: mins
});

export const PHASE_9_SQL_WAREHOUSE: DatabricksPhase = {
  id: 'db-phase-9', number: 9,
  title: { es: 'SQL Warehouse & BI', en: 'SQL Warehouse & BI', pt: 'SQL Warehouse & BI' },
  subtitle: { es: 'Analytics y visualizaciÃ³n', en: 'Analytics and visualization', pt: 'Analytics e visualizaÃ§Ã£o' },
  description: { es: 'SQL Warehouses permiten ejecutar SQL a escala y conectar herramientas de BI. En Free Edition, usas serverless compute automÃ¡ticamente para tus queries SQL. Los nuevos AI/BI Dashboards (2026) reemplazan los dashboards legacy.', en: 'SQL Warehouses allow running SQL at scale and connecting BI tools. In Free Edition, you use serverless compute automatically for your SQL queries. New AI/BI Dashboards (2026) replace legacy dashboards.', pt: 'SQL Warehouses permitem executar SQL em escala e conectar ferramentas de BI. No Free Edition, vocÃª usa serverless compute automaticamente para suas queries SQL. Os novos AI/BI Dashboards (2026) substituem os dashboards legacy.' },
  icon: 'ğŸ“ˆ', color: 'indigo', estimatedDays: '3-4 dÃ­as',
  steps: [
    createStep('db-9-1', 'Â¿QuÃ© es SQL Warehouse?', 'What is SQL Warehouse?', 'O que Ã© SQL Warehouse?',
      'Compute optimizado para SQL analytics.', 'Compute optimized for SQL analytics.', 'Compute otimizado para SQL analytics.',
      `## SQL Warehouse\n\nSQL Warehouses son clusters optimizados para:\n- Queries SQL ad-hoc\n- Dashboards y reportes\n- ConexiÃ³n con BI tools (Tableau, Power BI)\n\n### Tipos:\n- **Serverless**: Auto-escala, pago por uso\n- **Pro**: Balance costo/performance\n- **Classic**: MÃ¡s control, menos costo\n\n### Crear:\nSQL > SQL Warehouses > Create`,
      `## SQL Warehouse\n\nSQL Warehouses are clusters optimized for:\n- Ad-hoc SQL queries\n- Dashboards and reports\n- BI tool connections (Tableau, Power BI)\n\n### Types:\n- **Serverless**: Auto-scales, pay per use\n- **Pro**: Cost/performance balance\n- **Classic**: More control, less cost\n\n### Create:\nSQL > SQL Warehouses > Create`,
      `## SQL Warehouse\n\nSQL Warehouses sÃ£o clusters otimizados para:\n- Queries SQL ad-hoc\n- Dashboards e relatÃ³rios\n- ConexÃ£o com BI tools (Tableau, Power BI)\n\n### Tipos:\n- **Serverless**: Auto-escala, pague pelo uso\n- **Pro**: BalanÃ§o custo/performance\n- **Classic**: Mais controle, menos custo\n\n### Criar:\nSQL > SQL Warehouses > Create`,
      {es: 'ğŸ’¡ Serverless es ideal para cargas variables.', en: 'ğŸ’¡ Serverless is ideal for variable workloads.', pt: 'ğŸ’¡ Serverless Ã© ideal para cargas variÃ¡veis.'},
      'ğŸ¤” Â¿CuÃ¡ndo usar Serverless vs Pro?', 'ğŸ¤” When to use Serverless vs Pro?', 'ğŸ¤” Quando usar Serverless vs Pro?', 20, 15),
    createStep('db-9-2', 'Databricks SQL Editor', 'Databricks SQL Editor', 'Databricks SQL Editor',
      'EscribÃ­ y ejecutÃ¡ queries directamente en el browser.', 'Write and run queries directly in the browser.', 'Escreva e execute queries diretamente no browser.',
      `## SQL Editor\n\n\`\`\`sql\n-- Query bÃ¡sica\nSELECT * FROM catalog.schema.tabla LIMIT 100;\n\n-- Con parÃ¡metros\nSELECT * FROM ventas WHERE fecha = :fecha_param;\n\n-- CTE\nWITH ventas_mes AS (\n  SELECT * FROM ventas WHERE month(fecha) = 1\n)\nSELECT categoria, SUM(monto) FROM ventas_mes GROUP BY 1;\n\`\`\`\n\n### Features:\n- Autocompletado inteligente\n- Historial de queries\n- Guardar queries como snippets`,
      `## SQL Editor\n\n\`\`\`sql\n-- Basic query\nSELECT * FROM catalog.schema.table LIMIT 100;\n\n-- With parameters\nSELECT * FROM sales WHERE date = :date_param;\n\n-- CTE\nWITH month_sales AS (\n  SELECT * FROM sales WHERE month(date) = 1\n)\nSELECT category, SUM(amount) FROM month_sales GROUP BY 1;\n\`\`\`\n\n### Features:\n- Smart autocomplete\n- Query history\n- Save queries as snippets`,
      `## SQL Editor\n\n\`\`\`sql\n-- Query bÃ¡sica\nSELECT * FROM catalog.schema.tabela LIMIT 100;\n\n-- Com parÃ¢metros\nSELECT * FROM vendas WHERE data = :data_param;\n\n-- CTE\nWITH vendas_mes AS (\n  SELECT * FROM vendas WHERE month(data) = 1\n)\nSELECT categoria, SUM(valor) FROM vendas_mes GROUP BY 1;\n\`\`\`\n\n### Features:\n- Autocomplete inteligente\n- HistÃ³rico de queries\n- Salvar queries como snippets`,
      {es: 'âŒ¨ï¸ Ctrl+Enter ejecuta la query seleccionada.', en: 'âŒ¨ï¸ Ctrl+Enter runs the selected query.', pt: 'âŒ¨ï¸ Ctrl+Enter executa a query selecionada.'},
      'âœ… Â¿Ejecutaste una query con parÃ¡metros?', 'âœ… Did you run a query with parameters?', 'âœ… VocÃª executou uma query com parÃ¢metros?', 20, 15),
    createStep('db-9-3', 'Dashboards Nativos', 'Native Dashboards', 'Dashboards Nativos',
      'CreÃ¡ dashboards interactivos sin cÃ³digo.', 'Create interactive dashboards without code.', 'Crie dashboards interativos sem cÃ³digo.',
      `## Dashboards\n\n### Crear Dashboard:\n1. SQL > Dashboards > Create\n2. Add visualization\n3. Conectar query guardada\n4. Configurar refresh\n\n### Tipos de visualizaciÃ³n:\n- Line/Bar/Pie charts\n- Tables\n- Counters\n- Maps\n- Pivot tables\n\n### Refresh automÃ¡tico:\n- Schedule: cada X minutos\n- Trigger: cuando cambian datos`,
      `## Dashboards\n\n### Create Dashboard:\n1. SQL > Dashboards > Create\n2. Add visualization\n3. Connect saved query\n4. Configure refresh\n\n### Visualization types:\n- Line/Bar/Pie charts\n- Tables\n- Counters\n- Maps\n- Pivot tables\n\n### Auto refresh:\n- Schedule: every X minutes\n- Trigger: when data changes`,
      `## Dashboards\n\n### Criar Dashboard:\n1. SQL > Dashboards > Create\n2. Add visualization\n3. Conectar query salva\n4. Configurar refresh\n\n### Tipos de visualizaÃ§Ã£o:\n- Line/Bar/Pie charts\n- Tables\n- Counters\n- Maps\n- Pivot tables\n\n### Refresh automÃ¡tico:\n- Schedule: cada X minutos\n- Trigger: quando dados mudam`,
      {es: 'ğŸ“Š Los dashboards son perfectos para stakeholders no tÃ©cnicos.', en: 'ğŸ“Š Dashboards are perfect for non-technical stakeholders.', pt: 'ğŸ“Š Dashboards sÃ£o perfeitos para stakeholders nÃ£o tÃ©cnicos.'},
      'âœ… Â¿Creaste un dashboard con 3+ visualizaciones?', 'âœ… Did you create a dashboard with 3+ visualizations?', 'âœ… VocÃª criou um dashboard com 3+ visualizaÃ§Ãµes?', 30, 25),
    createStep('db-9-4', 'Alerts SQL', 'SQL Alerts', 'Alertas SQL',
      'RecibÃ­ notificaciones cuando tus mÃ©tricas cambian.', 'Get notifications when your metrics change.', 'Receba notificaÃ§Ãµes quando suas mÃ©tricas mudam.',
      `## SQL Alerts\n\n### Crear Alert:\n1. Guardar query que retorna valor numÃ©rico\n2. SQL > Alerts > Create\n3. Configurar condiciÃ³n: > < = !=\n4. Configurar destino: email, Slack\n5. Schedule de evaluaciÃ³n\n\n### Ejemplo query para alert:\n\`\`\`sql\nSELECT COUNT(*) as errores\nFROM logs\nWHERE level = 'ERROR'\n  AND timestamp > current_timestamp() - INTERVAL 1 HOUR\n\`\`\`\n\n### CondiciÃ³n: errores > 100`,
      `## SQL Alerts\n\n### Create Alert:\n1. Save query that returns numeric value\n2. SQL > Alerts > Create\n3. Configure condition: > < = !=\n4. Configure destination: email, Slack\n5. Evaluation schedule\n\n### Example alert query:\n\`\`\`sql\nSELECT COUNT(*) as errors\nFROM logs\nWHERE level = 'ERROR'\n  AND timestamp > current_timestamp() - INTERVAL 1 HOUR\n\`\`\`\n\n### Condition: errors > 100`,
      `## SQL Alerts\n\n### Criar Alert:\n1. Salvar query que retorna valor numÃ©rico\n2. SQL > Alerts > Create\n3. Configurar condiÃ§Ã£o: > < = !=\n4. Configurar destino: email, Slack\n5. Schedule de avaliaÃ§Ã£o\n\n### Exemplo query para alert:\n\`\`\`sql\nSELECT COUNT(*) as erros\nFROM logs\nWHERE level = 'ERROR'\n  AND timestamp > current_timestamp() - INTERVAL 1 HOUR\n\`\`\`\n\n### CondiÃ§Ã£o: erros > 100`,
      {es: 'ğŸš¨ Alerts son clave para monitoreo proactivo.', en: 'ğŸš¨ Alerts are key for proactive monitoring.', pt: 'ğŸš¨ Alerts sÃ£o chave para monitoramento proativo.'},
      'âœ… Â¿Configuraste un alert?', 'âœ… Did you configure an alert?', 'âœ… VocÃª configurou um alert?', 25, 20),
    createStep('db-9-5', 'Conectar BI Tools', 'Connect BI Tools', 'Conectar BI Tools',
      'Tableau, Power BI, Looker se conectan fÃ¡cilmente.', 'Tableau, Power BI, Looker connect easily.', 'Tableau, Power BI, Looker se conectam facilmente.',
      `## ConexiÃ³n BI\n\n### Drivers:\n- ODBC/JDBC disponibles\n- Partner Connect para setup rÃ¡pido\n\n### Tableau:\n1. Server: tu-workspace.cloud.databricks.com\n2. HTTP Path: del SQL Warehouse\n3. Auth: Token personal\n\n### Power BI:\n1. Get Data > Azure Databricks\n2. Ingresar Server/Path\n3. DirectQuery o Import\n\n### Connection string:\n\`\`\`\njdbc:databricks://HOST:443/default;transportMode=http;ssl=1;httpPath=PATH;AuthMech=3;UID=token;PWD=TOKEN\n\`\`\``,
      `## BI Connection\n\n### Drivers:\n- ODBC/JDBC available\n- Partner Connect for quick setup\n\n### Tableau:\n1. Server: your-workspace.cloud.databricks.com\n2. HTTP Path: from SQL Warehouse\n3. Auth: Personal token\n\n### Power BI:\n1. Get Data > Azure Databricks\n2. Enter Server/Path\n3. DirectQuery or Import\n\n### Connection string:\n\`\`\`\njdbc:databricks://HOST:443/default;transportMode=http;ssl=1;httpPath=PATH;AuthMech=3;UID=token;PWD=TOKEN\n\`\`\``,
      `## ConexÃ£o BI\n\n### Drivers:\n- ODBC/JDBC disponÃ­veis\n- Partner Connect para setup rÃ¡pido\n\n### Tableau:\n1. Server: seu-workspace.cloud.databricks.com\n2. HTTP Path: do SQL Warehouse\n3. Auth: Token pessoal\n\n### Power BI:\n1. Get Data > Azure Databricks\n2. Inserir Server/Path\n3. DirectQuery ou Import\n\n### Connection string:\n\`\`\`\njdbc:databricks://HOST:443/default;transportMode=http;ssl=1;httpPath=PATH;AuthMech=3;UID=token;PWD=TOKEN\n\`\`\``,
      {es: 'ğŸ’¡ Partner Connect automatiza la configuraciÃ³n de muchas herramientas.', en: 'ğŸ’¡ Partner Connect automates configuration for many tools.', pt: 'ğŸ’¡ Partner Connect automatiza a configuraÃ§Ã£o de muitas ferramentas.'},
      'âœ… Â¿Conectaste alguna herramienta BI?', 'âœ… Did you connect any BI tool?', 'âœ… VocÃª conectou alguma ferramenta BI?', 30, 30),
    createStep('db-9-6', 'Query Federation', 'Query Federation', 'Query Federation',
      'ConsultÃ¡ datos en sistemas externos desde Databricks.', 'Query data in external systems from Databricks.', 'Consulte dados em sistemas externos do Databricks.',
      `## Lakehouse Federation\n\n### Crear External Catalog:\n\`\`\`sql\nCREATE FOREIGN CATALOG mysql_catalog\nUSING CONNECTION mysql_conn;\n\`\`\`\n\n### Query federada:\n\`\`\`sql\nSELECT * FROM mysql_catalog.db.tabla\nJOIN delta_catalog.schema.tabla ON ...\n\`\`\`\n\n### Sistemas soportados:\n- MySQL, PostgreSQL\n- SQL Server\n- Snowflake\n- BigQuery`,
      `## Lakehouse Federation\n\n### Create External Catalog:\n\`\`\`sql\nCREATE FOREIGN CATALOG mysql_catalog\nUSING CONNECTION mysql_conn;\n\`\`\`\n\n### Federated query:\n\`\`\`sql\nSELECT * FROM mysql_catalog.db.table\nJOIN delta_catalog.schema.table ON ...\n\`\`\`\n\n### Supported systems:\n- MySQL, PostgreSQL\n- SQL Server\n- Snowflake\n- BigQuery`,
      `## Lakehouse Federation\n\n### Criar External Catalog:\n\`\`\`sql\nCREATE FOREIGN CATALOG mysql_catalog\nUSING CONNECTION mysql_conn;\n\`\`\`\n\n### Query federada:\n\`\`\`sql\nSELECT * FROM mysql_catalog.db.tabela\nJOIN delta_catalog.schema.tabela ON ...\n\`\`\`\n\n### Sistemas suportados:\n- MySQL, PostgreSQL\n- SQL Server\n- Snowflake\n- BigQuery`,
      {es: 'ğŸŒ Federation evita mover datos innecesariamente.', en: 'ğŸŒ Federation avoids moving data unnecessarily.', pt: 'ğŸŒ Federation evita mover dados desnecessariamente.'},
      'ğŸ¤” Â¿CuÃ¡ndo usarÃ­as Federation vs ETL?', 'ğŸ¤” When would you use Federation vs ETL?', 'ğŸ¤” Quando vocÃª usaria Federation vs ETL?', 25, 20),
    createStep('db-9-7', 'Query Optimization', 'Query Optimization', 'OtimizaÃ§Ã£o de Queries',
      'OptimizÃ¡ tus queries SQL para mÃ¡xima performance.', 'Optimize your SQL queries for maximum performance.', 'Otimize suas queries SQL para performance mÃ¡xima.',
      `## OptimizaciÃ³n SQL\n\n### Query Profile:\n- Click en query > Query Profile\n- Ver tiempo por operaciÃ³n\n- Identificar scans grandes\n\n### Tips:\n\`\`\`sql\n-- Usar partition pruning\nWHERE fecha = '2024-01-01'  -- Si particionado por fecha\n\n-- Evitar SELECT *\nSELECT col1, col2 FROM tabla\n\n-- Usar LIMIT en exploraciÃ³n\nSELECT * FROM tabla LIMIT 100\n\n-- Z-ORDER para filtros frecuentes\nOPTIMIZE tabla ZORDER BY (columna_filtro)\n\`\`\``,
      `## SQL Optimization\n\n### Query Profile:\n- Click query > Query Profile\n- View time per operation\n- Identify large scans\n\n### Tips:\n\`\`\`sql\n-- Use partition pruning\nWHERE date = '2024-01-01'  -- If partitioned by date\n\n-- Avoid SELECT *\nSELECT col1, col2 FROM table\n\n-- Use LIMIT in exploration\nSELECT * FROM table LIMIT 100\n\n-- Z-ORDER for frequent filters\nOPTIMIZE table ZORDER BY (filter_column)\n\`\`\``,
      `## OtimizaÃ§Ã£o SQL\n\n### Query Profile:\n- Click na query > Query Profile\n- Ver tempo por operaÃ§Ã£o\n- Identificar scans grandes\n\n### Dicas:\n\`\`\`sql\n-- Usar partition pruning\nWHERE data = '2024-01-01'  -- Se particionado por data\n\n-- Evitar SELECT *\nSELECT col1, col2 FROM tabela\n\n-- Usar LIMIT em exploraÃ§Ã£o\nSELECT * FROM tabela LIMIT 100\n\n-- Z-ORDER para filtros frequentes\nOPTIMIZE tabela ZORDER BY (coluna_filtro)\n\`\`\``,
      {es: 'âš¡ Query Profile es tu mejor amigo para optimizaciÃ³n.', en: 'âš¡ Query Profile is your best friend for optimization.', pt: 'âš¡ Query Profile Ã© seu melhor amigo para otimizaÃ§Ã£o.'},
      'âœ… Â¿Usaste Query Profile para optimizar una query?', 'âœ… Did you use Query Profile to optimize a query?', 'âœ… VocÃª usou Query Profile para otimizar uma query?', 25, 25),
    createStep('db-9-8', 'Proyecto: Dashboard Analytics', 'Project: Analytics Dashboard', 'Projeto: Dashboard Analytics',
      'ConstruÃ­ un dashboard ejecutivo completo.', 'Build a complete executive dashboard.', 'Construa um dashboard executivo completo.',
      `## Proyecto: Dashboard Ejecutivo\n\n### Componentes:\n- [ ] 5+ queries guardadas\n- [ ] Dashboard con 6+ visualizaciones\n- [ ] KPIs principales (counters)\n- [ ] Trends (line charts)\n- [ ] Breakdown (bar/pie)\n- [ ] Filtros interactivos\n- [ ] Refresh automÃ¡tico\n- [ ] Alert configurado`,
      `## Project: Executive Dashboard\n\n### Components:\n- [ ] 5+ saved queries\n- [ ] Dashboard with 6+ visualizations\n- [ ] Main KPIs (counters)\n- [ ] Trends (line charts)\n- [ ] Breakdown (bar/pie)\n- [ ] Interactive filters\n- [ ] Auto refresh\n- [ ] Alert configured`,
      `## Projeto: Dashboard Executivo\n\n### Componentes:\n- [ ] 5+ queries salvas\n- [ ] Dashboard com 6+ visualizaÃ§Ãµes\n- [ ] KPIs principais (counters)\n- [ ] TendÃªncias (line charts)\n- [ ] Breakdown (bar/pie)\n- [ ] Filtros interativos\n- [ ] Refresh automÃ¡tico\n- [ ] Alert configurado`,
      {es: 'ğŸ† Un buen dashboard puede impresionar en entrevistas.', en: 'ğŸ† A good dashboard can impress in interviews.', pt: 'ğŸ† Um bom dashboard pode impressionar em entrevistas.'},
      'ğŸ† Â¿Tu dashboard tiene 6+ visualizaciones?', 'ğŸ† Does your dashboard have 6+ visualizations?', 'ğŸ† Seu dashboard tem 6+ visualizaÃ§Ãµes?', 75, 60)
  ]
};

export const PHASE_10_MLFLOW: DatabricksPhase = {
  id: 'db-phase-10', number: 10,
  title: { es: 'MLflow & ML Engineering', en: 'MLflow & ML Engineering', pt: 'MLflow & ML Engineering' },
  subtitle: { es: 'Machine Learning en producciÃ³n', en: 'Machine Learning in production', pt: 'Machine Learning em produÃ§Ã£o' },
  description: { es: 'MLflow es la plataforma open-source de Databricks para gestionar el ciclo de vida de ML.', en: 'MLflow is Databricks\' open-source platform for managing the ML lifecycle.', pt: 'MLflow Ã© a plataforma open-source do Databricks para gerenciar o ciclo de vida de ML.' },
  icon: 'ğŸ¤–', color: 'pink', estimatedDays: '4-5 dÃ­as',
  steps: [
    createStep('db-10-1', 'IntroducciÃ³n a MLflow', 'Introduction to MLflow', 'IntroduÃ§Ã£o ao MLflow',
      'MLflow: tracking, registry, deployment de modelos.', 'MLflow: model tracking, registry, deployment.', 'MLflow: tracking, registry, deployment de modelos.',
      `## MLflow\n\n### Componentes:\n- **Tracking**: Loguear experimentos, mÃ©tricas, parÃ¡metros\n- **Models**: Empaquetar modelos en formato estÃ¡ndar\n- **Registry**: Gestionar versiones y stages\n- **Projects**: Reproducibilidad\n\n### En Databricks:\nMLflow viene integrado y mejorado con:\n- UI nativa\n- Autologging\n- Model Serving\n- Feature Store integrado`,
      `## MLflow\n\n### Components:\n- **Tracking**: Log experiments, metrics, parameters\n- **Models**: Package models in standard format\n- **Registry**: Manage versions and stages\n- **Projects**: Reproducibility\n\n### In Databricks:\nMLflow comes integrated and enhanced with:\n- Native UI\n- Autologging\n- Model Serving\n- Integrated Feature Store`,
      `## MLflow\n\n### Componentes:\n- **Tracking**: Logar experimentos, mÃ©tricas, parÃ¢metros\n- **Models**: Empacotar modelos em formato padrÃ£o\n- **Registry**: Gerenciar versÃµes e stages\n- **Projects**: Reproducibilidade\n\n### No Databricks:\nMLflow vem integrado e aprimorado com:\n- UI nativa\n- Autologging\n- Model Serving\n- Feature Store integrado`,
      {es: 'ğŸ’¡ MLflow es el estÃ¡ndar de la industria para MLOps.', en: 'ğŸ’¡ MLflow is the industry standard for MLOps.', pt: 'ğŸ’¡ MLflow Ã© o padrÃ£o da indÃºstria para MLOps.'},
      'ğŸ¤” Â¿CuÃ¡les son los 4 componentes de MLflow?', 'ğŸ¤” What are the 4 components of MLflow?', 'ğŸ¤” Quais sÃ£o os 4 componentes do MLflow?', 20, 15),
    createStep('db-10-2', 'Experiment Tracking', 'Experiment Tracking', 'Experiment Tracking',
      'RegistrÃ¡ todos tus experimentos de ML.', 'Record all your ML experiments.', 'Registre todos seus experimentos de ML.',
      `## Tracking\n\n\`\`\`python\nimport mlflow\n\n# Crear/usar experimento\nmlflow.set_experiment("/Users/mi_user/mi_experimento")\n\n# Iniciar run\nwith mlflow.start_run():\n    # Loguear parÃ¡metros\n    mlflow.log_param("learning_rate", 0.01)\n    mlflow.log_param("epochs", 100)\n    \n    # Entrenar...\n    \n    # Loguear mÃ©tricas\n    mlflow.log_metric("accuracy", 0.95)\n    mlflow.log_metric("loss", 0.05)\n    \n    # Loguear modelo\n    mlflow.sklearn.log_model(model, "model")\n\`\`\``,
      `## Tracking\n\n\`\`\`python\nimport mlflow\n\n# Create/use experiment\nmlflow.set_experiment("/Users/my_user/my_experiment")\n\n# Start run\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_param("learning_rate", 0.01)\n    mlflow.log_param("epochs", 100)\n    \n    # Train...\n    \n    # Log metrics\n    mlflow.log_metric("accuracy", 0.95)\n    mlflow.log_metric("loss", 0.05)\n    \n    # Log model\n    mlflow.sklearn.log_model(model, "model")\n\`\`\``,
      `## Tracking\n\n\`\`\`python\nimport mlflow\n\n# Criar/usar experimento\nmlflow.set_experiment("/Users/meu_user/meu_experimento")\n\n# Iniciar run\nwith mlflow.start_run():\n    # Logar parÃ¢metros\n    mlflow.log_param("learning_rate", 0.01)\n    mlflow.log_param("epochs", 100)\n    \n    # Treinar...\n    \n    # Logar mÃ©tricas\n    mlflow.log_metric("accuracy", 0.95)\n    mlflow.log_metric("loss", 0.05)\n    \n    # Logar modelo\n    mlflow.sklearn.log_model(model, "model")\n\`\`\``,
      {es: 'ğŸ“Š Todo queda registrado en la UI de MLflow.', en: 'ğŸ“Š Everything is recorded in the MLflow UI.', pt: 'ğŸ“Š Tudo fica registrado na UI do MLflow.'},
      'âœ… Â¿Logueaste un experimento con mÃ©tricas?', 'âœ… Did you log an experiment with metrics?', 'âœ… VocÃª logou um experimento com mÃ©tricas?', 30, 25),
    createStep('db-10-3', 'Autologging', 'Autologging', 'Autologging',
      'Logueo automÃ¡tico para frameworks populares.', 'Automatic logging for popular frameworks.', 'Log automÃ¡tico para frameworks populares.',
      `## Autologging\n\n\`\`\`python\nimport mlflow\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Habilitar autologging\nmlflow.sklearn.autolog()\n\n# Entrenar (se loguea TODO automÃ¡ticamente)\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\n# AutomÃ¡ticamente loguea:\n# - Todos los parÃ¡metros\n# - MÃ©tricas (accuracy, f1, etc.)\n# - Modelo serializado\n# - Feature importance\n\`\`\`\n\n### Frameworks soportados:\n- scikit-learn\n- TensorFlow/Keras\n- PyTorch\n- XGBoost\n- LightGBM\n- Spark MLlib`,
      `## Autologging\n\n\`\`\`python\nimport mlflow\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Enable autologging\nmlflow.sklearn.autolog()\n\n# Train (EVERYTHING logged automatically)\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\n# Automatically logs:\n# - All parameters\n# - Metrics (accuracy, f1, etc.)\n# - Serialized model\n# - Feature importance\n\`\`\`\n\n### Supported frameworks:\n- scikit-learn\n- TensorFlow/Keras\n- PyTorch\n- XGBoost\n- LightGBM\n- Spark MLlib`,
      `## Autologging\n\n\`\`\`python\nimport mlflow\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Habilitar autologging\nmlflow.sklearn.autolog()\n\n# Treinar (TUDO Ã© logado automaticamente)\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\n# Automaticamente loga:\n# - Todos os parÃ¢metros\n# - MÃ©tricas (accuracy, f1, etc.)\n# - Modelo serializado\n# - Feature importance\n\`\`\`\n\n### Frameworks suportados:\n- scikit-learn\n- TensorFlow/Keras\n- PyTorch\n- XGBoost\n- LightGBM\n- Spark MLlib`,
      {es: 'âš¡ Autolog es magia. ActÃ­valo siempre.', en: 'âš¡ Autolog is magic. Always enable it.', pt: 'âš¡ Autolog Ã© mÃ¡gico. Sempre ative.'},
      'âœ… Â¿Usaste autolog para un modelo?', 'âœ… Did you use autolog for a model?', 'âœ… VocÃª usou autolog para um modelo?', 25, 20),
    createStep('db-10-4', 'Model Registry', 'Model Registry', 'Model Registry',
      'GestionÃ¡ versiones y stages de tus modelos.', 'Manage versions and stages of your models.', 'Gerencie versÃµes e stages dos seus modelos.',
      `## Model Registry\n\n### Registrar modelo:\n\`\`\`python\n# Desde un run existente\nmlflow.register_model(\n    "runs:/RUN_ID/model",\n    "mi_modelo_produccion"\n)\n\n# O al loguear\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(\n        model, "model",\n        registered_model_name="mi_modelo"\n    )\n\`\`\`\n\n### Stages:\n- **None**: ReciÃ©n registrado\n- **Staging**: En pruebas\n- **Production**: En producciÃ³n\n- **Archived**: Descartado\n\n### Transicionar:\n\`\`\`python\nfrom mlflow import MlflowClient\nclient = MlflowClient()\nclient.transition_model_version_stage(\n    name="mi_modelo",\n    version=1,\n    stage="Production"\n)\n\`\`\``,
      `## Model Registry\n\n### Register model:\n\`\`\`python\n# From existing run\nmlflow.register_model(\n    "runs:/RUN_ID/model",\n    "my_production_model"\n)\n\n# Or when logging\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(\n        model, "model",\n        registered_model_name="my_model"\n    )\n\`\`\`\n\n### Stages:\n- **None**: Newly registered\n- **Staging**: In testing\n- **Production**: In production\n- **Archived**: Discarded\n\n### Transition:\n\`\`\`python\nfrom mlflow import MlflowClient\nclient = MlflowClient()\nclient.transition_model_version_stage(\n    name="my_model",\n    version=1,\n    stage="Production"\n)\n\`\`\``,
      `## Model Registry\n\n### Registrar modelo:\n\`\`\`python\n# De um run existente\nmlflow.register_model(\n    "runs:/RUN_ID/model",\n    "meu_modelo_producao"\n)\n\n# Ou ao logar\nwith mlflow.start_run():\n    mlflow.sklearn.log_model(\n        model, "model",\n        registered_model_name="meu_modelo"\n    )\n\`\`\`\n\n### Stages:\n- **None**: RecÃ©m registrado\n- **Staging**: Em testes\n- **Production**: Em produÃ§Ã£o\n- **Archived**: Descartado\n\n### Transicionar:\n\`\`\`python\nfrom mlflow import MlflowClient\nclient = MlflowClient()\nclient.transition_model_version_stage(\n    name="meu_modelo",\n    version=1,\n    stage="Production"\n)\n\`\`\``,
      {es: 'ğŸ”„ Registry permite rollback fÃ¡cil si algo sale mal.', en: 'ğŸ”„ Registry allows easy rollback if something goes wrong.', pt: 'ğŸ”„ Registry permite rollback fÃ¡cil se algo der errado.'},
      'âœ… Â¿Registraste un modelo y lo moviste a Production?', 'âœ… Did you register a model and move it to Production?', 'âœ… VocÃª registrou um modelo e o moveu para Production?', 30, 25),
    createStep('db-10-5', 'Model Serving', 'Model Serving', 'Model Serving',
      'DesplegÃ¡ modelos como endpoints REST.', 'Deploy models as REST endpoints.', 'Implante modelos como endpoints REST.',
      `## Model Serving\n\n### Habilitar serving:\n1. Models > Tu modelo > Serving\n2. Enable serving\n3. Esperar que el endpoint estÃ© listo\n\n### Llamar endpoint:\n\`\`\`python\nimport requests\n\nurl = "https://workspace.cloud.databricks.com/serving-endpoints/mi_modelo/invocations"\nheaders = {"Authorization": f"Bearer {token}"}\ndata = {"dataframe_records": [{"feature1": 1, "feature2": 2}]}\n\nresponse = requests.post(url, json=data, headers=headers)\npredictions = response.json()\n\`\`\`\n\n### Opciones:\n- Serverless (recomendado)\n- GPU serving\n- A/B testing`,
      `## Model Serving\n\n### Enable serving:\n1. Models > Your model > Serving\n2. Enable serving\n3. Wait for endpoint to be ready\n\n### Call endpoint:\n\`\`\`python\nimport requests\n\nurl = "https://workspace.cloud.databricks.com/serving-endpoints/my_model/invocations"\nheaders = {"Authorization": f"Bearer {token}"}\ndata = {"dataframe_records": [{"feature1": 1, "feature2": 2}]}\n\nresponse = requests.post(url, json=data, headers=headers)\npredictions = response.json()\n\`\`\`\n\n### Options:\n- Serverless (recommended)\n- GPU serving\n- A/B testing`,
      `## Model Serving\n\n### Habilitar serving:\n1. Models > Seu modelo > Serving\n2. Enable serving\n3. Esperar que o endpoint esteja pronto\n\n### Chamar endpoint:\n\`\`\`python\nimport requests\n\nurl = "https://workspace.cloud.databricks.com/serving-endpoints/meu_modelo/invocations"\nheaders = {"Authorization": f"Bearer {token}"}\ndata = {"dataframe_records": [{"feature1": 1, "feature2": 2}]}\n\nresponse = requests.post(url, json=data, headers=headers)\npredictions = response.json()\n\`\`\`\n\n### OpÃ§Ãµes:\n- Serverless (recomendado)\n- GPU serving\n- A/B testing`,
      {es: 'ğŸš€ Model Serving es la forma mÃ¡s fÃ¡cil de poner ML en producciÃ³n.', en: 'ğŸš€ Model Serving is the easiest way to put ML in production.', pt: 'ğŸš€ Model Serving Ã© a forma mais fÃ¡cil de colocar ML em produÃ§Ã£o.'},
      'âœ… Â¿Desplegaste un modelo y lo llamaste via API?', 'âœ… Did you deploy a model and call it via API?', 'âœ… VocÃª implantou um modelo e o chamou via API?', 35, 30),
    createStep('db-10-6', 'Feature Store', 'Feature Store', 'Feature Store',
      'AlmacenÃ¡ y reutilizÃ¡ features de ML.', 'Store and reuse ML features.', 'Armazene e reutilize features de ML.',
      `## Feature Store\n\n### Crear Feature Table:\n\`\`\`python\nfrom databricks.feature_store import FeatureStoreClient\n\nfs = FeatureStoreClient()\n\n# Crear tabla de features\nfs.create_table(\n    name="catalog.schema.customer_features",\n    primary_keys=["customer_id"],\n    df=features_df,\n    description="Features de clientes"\n)\n\`\`\`\n\n### Entrenar con features:\n\`\`\`python\n# Crear training set\ntraining_set = fs.create_training_set(\n    df=labels_df,\n    feature_lookups=[\n        FeatureLookup(\n            table_name="catalog.schema.customer_features",\n            lookup_key="customer_id"\n        )\n    ],\n    label="target"\n)\n\ntraining_df = training_set.load_df()\n\`\`\``,
      `## Feature Store\n\n### Create Feature Table:\n\`\`\`python\nfrom databricks.feature_store import FeatureStoreClient\n\nfs = FeatureStoreClient()\n\n# Create feature table\nfs.create_table(\n    name="catalog.schema.customer_features",\n    primary_keys=["customer_id"],\n    df=features_df,\n    description="Customer features"\n)\n\`\`\`\n\n### Train with features:\n\`\`\`python\n# Create training set\ntraining_set = fs.create_training_set(\n    df=labels_df,\n    feature_lookups=[\n        FeatureLookup(\n            table_name="catalog.schema.customer_features",\n            lookup_key="customer_id"\n        )\n    ],\n    label="target"\n)\n\ntraining_df = training_set.load_df()\n\`\`\``,
      `## Feature Store\n\n### Criar Feature Table:\n\`\`\`python\nfrom databricks.feature_store import FeatureStoreClient\n\nfs = FeatureStoreClient()\n\n# Criar tabela de features\nfs.create_table(\n    name="catalog.schema.customer_features",\n    primary_keys=["customer_id"],\n    df=features_df,\n    description="Features de clientes"\n)\n\`\`\`\n\n### Treinar com features:\n\`\`\`python\n# Criar training set\ntraining_set = fs.create_training_set(\n    df=labels_df,\n    feature_lookups=[\n        FeatureLookup(\n            table_name="catalog.schema.customer_features",\n            lookup_key="customer_id"\n        )\n    ],\n    label="target"\n)\n\ntraining_df = training_set.load_df()\n\`\`\``,
      {es: 'â™»ï¸ Feature Store evita duplicar cÃ³digo de feature engineering.', en: 'â™»ï¸ Feature Store avoids duplicating feature engineering code.', pt: 'â™»ï¸ Feature Store evita duplicar cÃ³digo de feature engineering.'},
      'âœ… Â¿Creaste una feature table?', 'âœ… Did you create a feature table?', 'âœ… VocÃª criou uma feature table?', 35, 30),
    createStep('db-10-7', 'AutoML', 'AutoML', 'AutoML',
      'Databricks AutoML entrena modelos automÃ¡ticamente.', 'Databricks AutoML trains models automatically.', 'Databricks AutoML treina modelos automaticamente.',
      `## AutoML\n\n### Desde UI:\n1. Machine Learning > Experiments > Create AutoML\n2. Seleccionar tabla de datos\n3. Seleccionar target column\n4. Elegir tipo: Classification, Regression, Forecasting\n5. Iniciar\n\n### Desde cÃ³digo:\n\`\`\`python\nfrom databricks import automl\n\nsummary = automl.classify(\n    dataset=df,\n    target_col="label",\n    timeout_minutes=30\n)\n\n# Ver mejor modelo\nbest_run = summary.best_trial\nprint(best_run.metrics)\n\`\`\`\n\n### Genera automÃ¡ticamente:\n- Notebooks de feature engineering\n- CÃ³digo de entrenamiento\n- ComparaciÃ³n de modelos\n- Modelo registrado`,
      `## AutoML\n\n### From UI:\n1. Machine Learning > Experiments > Create AutoML\n2. Select data table\n3. Select target column\n4. Choose type: Classification, Regression, Forecasting\n5. Start\n\n### From code:\n\`\`\`python\nfrom databricks import automl\n\nsummary = automl.classify(\n    dataset=df,\n    target_col="label",\n    timeout_minutes=30\n)\n\n# View best model\nbest_run = summary.best_trial\nprint(best_run.metrics)\n\`\`\`\n\n### Automatically generates:\n- Feature engineering notebooks\n- Training code\n- Model comparison\n- Registered model`,
      `## AutoML\n\n### Pela UI:\n1. Machine Learning > Experiments > Create AutoML\n2. Selecionar tabela de dados\n3. Selecionar coluna target\n4. Escolher tipo: Classification, Regression, Forecasting\n5. Iniciar\n\n### Pelo cÃ³digo:\n\`\`\`python\nfrom databricks import automl\n\nsummary = automl.classify(\n    dataset=df,\n    target_col="label",\n    timeout_minutes=30\n)\n\n# Ver melhor modelo\nbest_run = summary.best_trial\nprint(best_run.metrics)\n\`\`\`\n\n### Gera automaticamente:\n- Notebooks de feature engineering\n- CÃ³digo de treinamento\n- ComparaÃ§Ã£o de modelos\n- Modelo registrado`,
      {es: 'ğŸ¤– AutoML es perfecto para baseline rÃ¡pido o usuarios no expertos en ML.', en: 'ğŸ¤– AutoML is perfect for quick baseline or non-ML expert users.', pt: 'ğŸ¤– AutoML Ã© perfeito para baseline rÃ¡pido ou usuÃ¡rios nÃ£o especialistas em ML.'},
      'âœ… Â¿Ejecutaste AutoML y revisaste el mejor modelo?', 'âœ… Did you run AutoML and review the best model?', 'âœ… VocÃª executou AutoML e revisou o melhor modelo?', 30, 30),
    createStep('db-10-8', 'Proyecto: Pipeline ML End-to-End', 'Project: End-to-End ML Pipeline', 'Projeto: Pipeline ML End-to-End',
      'ConstruÃ­ un pipeline ML completo: datos â†’ features â†’ modelo â†’ serving.', 'Build a complete ML pipeline: data â†’ features â†’ model â†’ serving.', 'Construa um pipeline ML completo: dados â†’ features â†’ modelo â†’ serving.',
      `## Proyecto: ML Pipeline\n\n### Checklist:\n- [ ] Datos en Delta Lake\n- [ ] Features en Feature Store\n- [ ] Experimento con tracking\n- [ ] Autologging habilitado\n- [ ] Mejor modelo en Registry\n- [ ] Modelo en stage Production\n- [ ] Endpoint serving activo\n- [ ] API llamable`,
      `## Project: ML Pipeline\n\n### Checklist:\n- [ ] Data in Delta Lake\n- [ ] Features in Feature Store\n- [ ] Experiment with tracking\n- [ ] Autologging enabled\n- [ ] Best model in Registry\n- [ ] Model in Production stage\n- [ ] Active serving endpoint\n- [ ] Callable API`,
      `## Projeto: Pipeline ML\n\n### Checklist:\n- [ ] Dados no Delta Lake\n- [ ] Features no Feature Store\n- [ ] Experimento com tracking\n- [ ] Autologging habilitado\n- [ ] Melhor modelo no Registry\n- [ ] Modelo em stage Production\n- [ ] Endpoint serving ativo\n- [ ] API chamÃ¡vel`,
      {es: 'ğŸ† Este pipeline es lo que hacen Data Scientists en empresas top.', en: 'ğŸ† This pipeline is what Data Scientists do at top companies.', pt: 'ğŸ† Este pipeline Ã© o que Data Scientists fazem em empresas top.'},
      'ğŸ† Â¿Tu modelo estÃ¡ sirviendo predicciones via API?', 'ğŸ† Is your model serving predictions via API?', 'ğŸ† Seu modelo estÃ¡ servindo prediÃ§Ãµes via API?', 100, 90)
  ]
};

export const PHASE_11_BEST_PRACTICES: DatabricksPhase = {
  id: 'db-phase-11', number: 11,
  title: { es: 'Best Practices & Performance', en: 'Best Practices & Performance', pt: 'Melhores PrÃ¡ticas & Performance' },
  subtitle: { es: 'OptimizaciÃ³n y producciÃ³n', en: 'Optimization and production', pt: 'OtimizaÃ§Ã£o e produÃ§Ã£o' },
  description: { es: 'Las mejores prÃ¡cticas para pipelines de producciÃ³n escalables y mantenibles.', en: 'Best practices for scalable and maintainable production pipelines.', pt: 'Melhores prÃ¡ticas para pipelines de produÃ§Ã£o escalÃ¡veis e manutenÃ­veis.' },
  icon: 'ğŸ¯', color: 'yellow', estimatedDays: '3-4 dÃ­as',
  steps: [
    createStep('db-11-1', 'Estructura de Proyecto', 'Project Structure', 'Estrutura de Projeto', 'OrganizÃ¡ tu cÃ³digo de forma mantenible.', 'Organize your code in a maintainable way.', 'Organize seu cÃ³digo de forma manutenÃ­vel.',
      `## Estructura Recomendada\n\n\`\`\`\nproyecto/\nâ”œâ”€â”€ notebooks/\nâ”‚   â”œâ”€â”€ 01_ingesta.py\nâ”‚   â”œâ”€â”€ 02_transformacion.py\nâ”‚   â””â”€â”€ 03_gold.py\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ utils.py\nâ”‚   â””â”€â”€ transformations.py\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_transformations.py\nâ”œâ”€â”€ config/\nâ”‚   â””â”€â”€ config.yaml\nâ””â”€â”€ requirements.txt\n\`\`\`\n\n### Tips:\n- CÃ³digo reutilizable en src/\n- ConfiguraciÃ³n externa\n- Tests para funciones crÃ­ticas`,
      `## Recommended Structure\n\n\`\`\`\nproject/\nâ”œâ”€â”€ notebooks/\nâ”‚   â”œâ”€â”€ 01_ingestion.py\nâ”‚   â”œâ”€â”€ 02_transformation.py\nâ”‚   â””â”€â”€ 03_gold.py\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ utils.py\nâ”‚   â””â”€â”€ transformations.py\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_transformations.py\nâ”œâ”€â”€ config/\nâ”‚   â””â”€â”€ config.yaml\nâ””â”€â”€ requirements.txt\n\`\`\`\n\n### Tips:\n- Reusable code in src/\n- External configuration\n- Tests for critical functions`,
      `## Estrutura Recomendada\n\n\`\`\`\nprojeto/\nâ”œâ”€â”€ notebooks/\nâ”‚   â”œâ”€â”€ 01_ingestao.py\nâ”‚   â”œâ”€â”€ 02_transformacao.py\nâ”‚   â””â”€â”€ 03_gold.py\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ utils.py\nâ”‚   â””â”€â”€ transformations.py\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_transformations.py\nâ”œâ”€â”€ config/\nâ”‚   â””â”€â”€ config.yaml\nâ””â”€â”€ requirements.txt\n\`\`\`\n\n### Dicas:\n- CÃ³digo reutilizÃ¡vel em src/\n- ConfiguraÃ§Ã£o externa\n- Testes para funÃ§Ãµes crÃ­ticas`,
      {es: 'ğŸ“ Una buena estructura escala con el equipo.', en: 'ğŸ“ Good structure scales with the team.', pt: 'ğŸ“ Uma boa estrutura escala com a equipe.'}, 'âœ… Â¿Organizaste tu proyecto con esta estructura?', 'âœ… Did you organize your project with this structure?', 'âœ… VocÃª organizou seu projeto com essa estrutura?', 20, 15),
    createStep('db-11-2', 'Git Integration', 'Git Integration', 'IntegraÃ§Ã£o Git', 'ConectÃ¡ repos Git para versionado de cÃ³digo.', 'Connect Git repos for code versioning.', 'Conecte repos Git para versionamento de cÃ³digo.',
      `## Repos en Databricks\n\n### Conectar repositorio:\n1. Workspace > Repos > Add Repo\n2. URL del repo Git\n3. Credenciales (token)\n\n### Operaciones:\n- Pull: traer cambios\n- Push: subir cambios\n- Checkout branch\n- Crear branch\n\n### CI/CD:\n- GitHub Actions\n- Azure DevOps\n- Databricks Asset Bundles`,
      `## Repos in Databricks\n\n### Connect repository:\n1. Workspace > Repos > Add Repo\n2. Git repo URL\n3. Credentials (token)\n\n### Operations:\n- Pull: get changes\n- Push: upload changes\n- Checkout branch\n- Create branch\n\n### CI/CD:\n- GitHub Actions\n- Azure DevOps\n- Databricks Asset Bundles`,
      `## Repos no Databricks\n\n### Conectar repositÃ³rio:\n1. Workspace > Repos > Add Repo\n2. URL do repo Git\n3. Credenciais (token)\n\n### OperaÃ§Ãµes:\n- Pull: trazer mudanÃ§as\n- Push: subir mudanÃ§as\n- Checkout branch\n- Criar branch\n\n### CI/CD:\n- GitHub Actions\n- Azure DevOps\n- Databricks Asset Bundles`,
      {es: 'ğŸ”„ Siempre usÃ¡ Git, incluso para proyectos pequeÃ±os.', en: 'ğŸ”„ Always use Git, even for small projects.', pt: 'ğŸ”„ Sempre use Git, mesmo para projetos pequenos.'}, 'âœ… Â¿Conectaste un repo Git?', 'âœ… Did you connect a Git repo?', 'âœ… VocÃª conectou um repo Git?', 25, 20),
    {
      id: 'db-11-2b',
      title: { es: 'Databricks Asset Bundles (DABs)', en: 'Databricks Asset Bundles (DABs)', pt: 'Databricks Asset Bundles (DABs)' },
      description: { es: 'El nuevo estÃ¡ndar (2024) para CI/CD en Databricks. Reemplaza a dbx.', en: 'The new standard (2024) for CI/CD in Databricks. Replaces dbx.', pt: 'O novo padrÃ£o (2024) para CI/CD no Databricks. Substitui dbx.' },
      theory: {
        es: `## Databricks Asset Bundles (DABs)

DABs es la forma **oficial y recomendada** de hacer CI/CD en Databricks (2024). Reemplaza a herramientas anteriores como dbx.

### Â¿QuÃ© es un Asset Bundle?

Es un **proyecto como cÃ³digo** que define:
- Jobs y pipelines
- Clusters
- Notebooks
- DLT pipelines
- Permisos
- Variables por ambiente

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DATABRICKS ASSET BUNDLE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  databricks.yml           # ConfiguraciÃ³n principal          â”‚
â”‚       â”‚                                                      â”‚
â”‚       â”œâ”€â”€ Define: Jobs, Pipelines, Clusters                 â”‚
â”‚       â”‚                                                      â”‚
â”‚       â””â”€â”€ Ambientes: dev â†’ staging â†’ prod                   â”‚
â”‚                                                              â”‚
â”‚  src/                    # Tu cÃ³digo                         â”‚
â”‚  resources/              # Configuraciones adicionales       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Estructura de un Bundle

\`\`\`
my-project/
â”œâ”€â”€ databricks.yml           # ConfiguraciÃ³n principal
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ jobs.yml            # DefiniciÃ³n de jobs
â”‚   â””â”€â”€ pipelines.yml       # DefiniciÃ³n de DLT
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ bronze.py
â”‚   â”‚   â”œâ”€â”€ silver.py
â”‚   â”‚   â””â”€â”€ gold.py
â”‚   â””â”€â”€ libs/
â”‚       â””â”€â”€ transforms.py
â””â”€â”€ tests/
    â””â”€â”€ test_transforms.py
\`\`\`

### databricks.yml BÃ¡sico

\`\`\`yaml
# databricks.yml
bundle:
  name: mi-pipeline-etl

# Variables que cambian por ambiente
variables:
  catalog:
    description: "Catalog de Unity"
    default: "dev"
  warehouse_id:
    description: "SQL Warehouse ID"

# Recursos (jobs, pipelines, etc)
include:
  - resources/*.yml

# Ambientes
targets:
  dev:
    workspace:
      host: https://dbc-xxxxx.cloud.databricks.com
    variables:
      catalog: dev
  
  staging:
    workspace:
      host: https://dbc-xxxxx.cloud.databricks.com
    variables:
      catalog: staging
  
  prod:
    workspace:
      host: https://dbc-xxxxx.cloud.databricks.com
    variables:
      catalog: prod
    mode: production  # Requiere permisos explÃ­citos
\`\`\`

### resources/jobs.yml

\`\`\`yaml
resources:
  jobs:
    daily_etl:
      name: "Daily ETL Pipeline"
      schedule:
        quartz_cron_expression: "0 0 6 * * ?"
        timezone_id: "America/Buenos_Aires"
      
      tasks:
        - task_key: bronze
          notebook_task:
            notebook_path: ./src/notebooks/bronze.py
          job_cluster_key: etl_cluster
        
        - task_key: silver
          depends_on:
            - task_key: bronze
          notebook_task:
            notebook_path: ./src/notebooks/silver.py
          job_cluster_key: etl_cluster
        
        - task_key: gold
          depends_on:
            - task_key: silver
          notebook_task:
            notebook_path: ./src/notebooks/gold.py
          job_cluster_key: etl_cluster
      
      job_clusters:
        - job_cluster_key: etl_cluster
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            num_workers: 2
            node_type_id: i3.xlarge
            aws_attributes:
              availability: SPOT_WITH_FALLBACK
      
      email_notifications:
        on_failure:
          - team@company.com
\`\`\`

### Comandos CLI

\`\`\`bash
# Instalar CLI
pip install databricks-cli

# Configurar autenticaciÃ³n
databricks configure --token

# Validar bundle
databricks bundle validate

# Desplegar a dev
databricks bundle deploy -t dev

# Ver cambios sin aplicar
databricks bundle deploy -t staging --dry-run

# Desplegar a producciÃ³n
databricks bundle deploy -t prod

# Ejecutar job manualmente
databricks bundle run daily_etl -t dev

# Destruir recursos (cuidado!)
databricks bundle destroy -t dev
\`\`\`

### CI/CD con GitHub Actions

\`\`\`yaml
# .github/workflows/deploy.yml
name: Deploy to Databricks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: databricks/setup-cli@main
      - run: databricks bundle validate
        env:
          DATABRICKS_TOKEN: \${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_HOST: \${{ secrets.DATABRICKS_HOST }}

  deploy-staging:
    needs: validate
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: databricks/setup-cli@main
      - run: databricks bundle deploy -t staging
        env:
          DATABRICKS_TOKEN: \${{ secrets.DATABRICKS_TOKEN }}
          DATABRICKS_HOST: \${{ secrets.DATABRICKS_HOST }}

  deploy-prod:
    needs: validate
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: production
    steps:
      - uses: actions/checkout@v3
      - uses: databricks/setup-cli@main
      - run: databricks bundle deploy -t prod
        env:
          DATABRICKS_TOKEN: \${{ secrets.DATABRICKS_TOKEN_PROD }}
          DATABRICKS_HOST: \${{ secrets.DATABRICKS_HOST_PROD }}
\`\`\`

### DABs vs Otras Herramientas

| Feature | DABs (2024) | dbx (legacy) | Terraform |
|---------|-------------|--------------|-----------|
| Oficial Databricks | âœ… SÃ­ | âŒ Deprecated | âŒ No |
| YAML config | âœ… | âœ… | HCL |
| Ambientes | âœ… Built-in | Requiere config | Requiere config |
| ValidaciÃ³n | âœ… CLI | Limited | Limited |
| Preview cambios | âœ… dry-run | âŒ | âœ… plan |
| DLT support | âœ… Nativo | Limited | Limited |`,
        en: `## Databricks Asset Bundles (DABs)

DABs is the **official and recommended** way to do CI/CD in Databricks (2024). Replaces previous tools like dbx.

### What is an Asset Bundle?

It's a **project as code** that defines jobs, pipelines, clusters, DLT pipelines, permissions, and environment variables.

\`\`\`yaml
# databricks.yml
bundle:
  name: my-etl-pipeline

targets:
  dev:
    workspace:
      host: https://dbc-xxxxx.cloud.databricks.com
  prod:
    workspace:
      host: https://dbc-xxxxx.cloud.databricks.com
    mode: production
\`\`\`

### CLI Commands
\`\`\`bash
databricks bundle validate     # Validate
databricks bundle deploy -t dev # Deploy to dev
databricks bundle run job -t dev # Run job
\`\`\``,
        pt: `## Databricks Asset Bundles (DABs)

DABs Ã© a forma **oficial e recomendada** de fazer CI/CD no Databricks (2024). Substitui ferramentas anteriores como dbx.

### O que Ã© um Asset Bundle?

Ã‰ um **projeto como cÃ³digo** que define jobs, pipelines, clusters, pipelines DLT, permissÃµes e variÃ¡veis de ambiente.

\`\`\`yaml
# databricks.yml
bundle:
  name: meu-pipeline-etl

targets:
  dev:
    workspace:
      host: https://dbc-xxxxx.cloud.databricks.com
  prod:
    workspace:
      host: https://dbc-xxxxx.cloud.databricks.com
\`\`\``
      },
      practicalTips: [
        { es: 'ğŸš€ DABs es el FUTURO de CI/CD en Databricks. Aprendelo ahora.', en: 'ğŸš€ DABs is the FUTURE of CI/CD in Databricks. Learn it now.', pt: 'ğŸš€ DABs Ã© o FUTURO de CI/CD no Databricks. Aprenda agora.' },
        { es: 'ğŸ”„ Si usÃ¡s dbx, migrÃ¡ a DABs. dbx estÃ¡ deprecated.', en: 'ğŸ”„ If you use dbx, migrate to DABs. dbx is deprecated.', pt: 'ğŸ”„ Se vocÃª usa dbx, migre para DABs. dbx estÃ¡ deprecated.' },
        { es: 'ğŸ’¡ UsÃ¡ targets (dev/staging/prod) para separar ambientes.', en: 'ğŸ’¡ Use targets (dev/staging/prod) to separate environments.', pt: 'ğŸ’¡ Use targets (dev/staging/prod) para separar ambientes.' }
      ],
      externalLinks: [
        { title: 'Asset Bundles Docs', url: 'https://docs.databricks.com/dev-tools/bundles/index.html', type: 'docs' },
        { title: 'Bundle Examples', url: 'https://github.com/databricks/bundle-examples', type: 'article' }
      ],
      checkpoint: { es: 'âœ… Â¿Creaste un bundle bÃ¡sico y lo desplegaste a dev?', en: 'âœ… Did you create a basic bundle and deploy it to dev?', pt: 'âœ… VocÃª criou um bundle bÃ¡sico e o fez deploy para dev?' },
      xpReward: 40,
      estimatedMinutes: 40
    },
    createStep('db-11-3', 'Secrets Management', 'Secrets Management', 'Gerenciamento de Secrets', 'Nunca hardcodees credenciales. UsÃ¡ secrets.', 'Never hardcode credentials. Use secrets.', 'Nunca coloque credenciais fixas. Use secrets.',
      `## Databricks Secrets\n\n### Crear scope y secret:\n\`\`\`bash\ndatabricks secrets create-scope --scope mi_scope\ndatabricks secrets put --scope mi_scope --key db_password\n\`\`\`\n\n### Usar en cÃ³digo:\n\`\`\`python\npassword = dbutils.secrets.get(\n    scope="mi_scope", \n    key="db_password"\n)\n\`\`\`\n\n### Best practices:\n- Un scope por proyecto/ambiente\n- Rotar secrets regularmente\n- Limitar acceso por grupo`,
      `## Databricks Secrets\n\n### Create scope and secret:\n\`\`\`bash\ndatabricks secrets create-scope --scope my_scope\ndatabricks secrets put --scope my_scope --key db_password\n\`\`\`\n\n### Use in code:\n\`\`\`python\npassword = dbutils.secrets.get(\n    scope="my_scope", \n    key="db_password"\n)\n\`\`\`\n\n### Best practices:\n- One scope per project/environment\n- Rotate secrets regularly\n- Limit access by group`,
      `## Databricks Secrets\n\n### Criar scope e secret:\n\`\`\`bash\ndatabricks secrets create-scope --scope meu_scope\ndatabricks secrets put --scope meu_scope --key db_password\n\`\`\`\n\n### Usar no cÃ³digo:\n\`\`\`python\npassword = dbutils.secrets.get(\n    scope="meu_scope", \n    key="db_password"\n)\n\`\`\`\n\n### Melhores prÃ¡ticas:\n- Um scope por projeto/ambiente\n- Rotacionar secrets regularmente\n- Limitar acesso por grupo`,
      {es: 'ğŸ” Si veo passwords en cÃ³digo, es red flag inmediata.', en: 'ğŸ” If I see passwords in code, it\'s an immediate red flag.', pt: 'ğŸ” Se eu vejo senhas no cÃ³digo, Ã© red flag imediata.'}, 'âœ… Â¿Creaste un secret y lo usaste en cÃ³digo?', 'âœ… Did you create a secret and use it in code?', 'âœ… VocÃª criou um secret e o usou no cÃ³digo?', 25, 20),
    createStep('db-11-4', 'Cost Management', 'Cost Management', 'GestÃ£o de Custos', 'ControlÃ¡ y optimizÃ¡ costos de Databricks.', 'Control and optimize Databricks costs.', 'Controle e otimize custos do Databricks.',
      `## OptimizaciÃ³n de Costos\n\n### Quick wins:\n- Auto-terminate clusters (30 min)\n- Job clusters vs All-Purpose\n- Spot instances (60-90% ahorro)\n- Right-size clusters\n\n### Monitoreo:\n\`\`\`sql\nSELECT \n  workspace_id,\n  sku_name,\n  SUM(usage_quantity) as dbus,\n  SUM(usage_quantity * list_price) as cost\nFROM system.billing.usage\nGROUP BY 1, 2\n\`\`\`\n\n### Tags para chargeback:\n- Por equipo\n- Por proyecto\n- Por ambiente`,
      `## Cost Optimization\n\n### Quick wins:\n- Auto-terminate clusters (30 min)\n- Job clusters vs All-Purpose\n- Spot instances (60-90% savings)\n- Right-size clusters\n\n### Monitoring:\n\`\`\`sql\nSELECT \n  workspace_id,\n  sku_name,\n  SUM(usage_quantity) as dbus,\n  SUM(usage_quantity * list_price) as cost\nFROM system.billing.usage\nGROUP BY 1, 2\n\`\`\`\n\n### Tags for chargeback:\n- By team\n- By project\n- By environment`,
      `## OtimizaÃ§Ã£o de Custos\n\n### Quick wins:\n- Auto-terminate clusters (30 min)\n- Job clusters vs All-Purpose\n- Spot instances (60-90% economia)\n- Right-size clusters\n\n### Monitoramento:\n\`\`\`sql\nSELECT \n  workspace_id,\n  sku_name,\n  SUM(usage_quantity) as dbus,\n  SUM(usage_quantity * list_price) as cost\nFROM system.billing.usage\nGROUP BY 1, 2\n\`\`\`\n\n### Tags para chargeback:\n- Por equipe\n- Por projeto\n- Por ambiente`,
      {es: 'ğŸ’° El costo es responsabilidad del DE. MonitorÃ©alo.', en: 'ğŸ’° Cost is the DE\'s responsibility. Monitor it.', pt: 'ğŸ’° Custo Ã© responsabilidade do DE. Monitore.'}, 'âœ… Â¿Configuraste tags de costo en tus clusters?', 'âœ… Did you configure cost tags on your clusters?', 'âœ… VocÃª configurou tags de custo nos seus clusters?', 25, 20),
    createStep('db-11-5', 'Testing Data Pipelines', 'Testing Data Pipelines', 'Testando Pipelines de Dados', 'Testear pipelines es esencial para producciÃ³n.', 'Testing pipelines is essential for production.', 'Testar pipelines Ã© essencial para produÃ§Ã£o.',
      `## Testing\n\n### Unit tests:\n\`\`\`python\nimport pytest\nfrom src.transformations import clean_data\n\ndef test_clean_data():\n    input_df = spark.createDataFrame([...])\n    result = clean_data(input_df)\n    assert result.count() == expected_count\n\`\`\`\n\n### Integration tests:\n- Test con datos de sample\n- Verificar schema output\n- Verificar constraints\n\n### Data quality tests:\n- Great Expectations\n- DLT Expectations\n- Custom assertions`,
      `## Testing\n\n### Unit tests:\n\`\`\`python\nimport pytest\nfrom src.transformations import clean_data\n\ndef test_clean_data():\n    input_df = spark.createDataFrame([...])\n    result = clean_data(input_df)\n    assert result.count() == expected_count\n\`\`\`\n\n### Integration tests:\n- Test with sample data\n- Verify output schema\n- Verify constraints\n\n### Data quality tests:\n- Great Expectations\n- DLT Expectations\n- Custom assertions`,
      `## Testing\n\n### Unit tests:\n\`\`\`python\nimport pytest\nfrom src.transformations import clean_data\n\ndef test_clean_data():\n    input_df = spark.createDataFrame([...])\n    result = clean_data(input_df)\n    assert result.count() == expected_count\n\`\`\`\n\n### Testes de integraÃ§Ã£o:\n- Teste com dados de amostra\n- Verificar schema output\n- Verificar constraints\n\n### Testes de qualidade de dados:\n- Great Expectations\n- DLT Expectations\n- Custom assertions`,
      {es: 'ğŸ§ª Un pipeline sin tests es una bomba de tiempo.', en: 'ğŸ§ª A pipeline without tests is a time bomb.', pt: 'ğŸ§ª Um pipeline sem testes Ã© uma bomba-relÃ³gio.'}, 'âœ… Â¿Escribiste tests para tu pipeline?', 'âœ… Did you write tests for your pipeline?', 'âœ… VocÃª escreveu testes para seu pipeline?', 30, 25),
    createStep('db-11-6', 'Monitoring & Observability', 'Monitoring & Observability', 'Monitoramento & Observabilidade', 'MonitoreÃ¡ salud y performance de tus pipelines.', 'Monitor health and performance of your pipelines.', 'Monitore saÃºde e performance dos seus pipelines.',
      `## Observabilidad\n\n### System Tables:\n\`\`\`sql\n-- Jobs\nSELECT * FROM system.workflow.jobs;\n\n-- Clusters\nSELECT * FROM system.compute.clusters;\n\n-- Queries SQL\nSELECT * FROM system.query.history;\n\`\`\`\n\n### MÃ©tricas clave:\n- Job success rate\n- DuraciÃ³n de runs\n- Data freshness\n- Error rate\n\n### IntegraciÃ³n:\n- Datadog\n- Grafana\n- Custom dashboards`,
      `## Observability\n\n### System Tables:\n\`\`\`sql\n-- Jobs\nSELECT * FROM system.workflow.jobs;\n\n-- Clusters\nSELECT * FROM system.compute.clusters;\n\n-- SQL Queries\nSELECT * FROM system.query.history;\n\`\`\`\n\n### Key metrics:\n- Job success rate\n- Run duration\n- Data freshness\n- Error rate\n\n### Integration:\n- Datadog\n- Grafana\n- Custom dashboards`,
      `## Observabilidade\n\n### System Tables:\n\`\`\`sql\n-- Jobs\nSELECT * FROM system.workflow.jobs;\n\n-- Clusters\nSELECT * FROM system.compute.clusters;\n\n-- Queries SQL\nSELECT * FROM system.query.history;\n\`\`\`\n\n### MÃ©tricas chave:\n- Job success rate\n- DuraÃ§Ã£o de runs\n- Data freshness\n- Error rate\n\n### IntegraÃ§Ã£o:\n- Datadog\n- Grafana\n- Custom dashboards`,
      {es: 'ğŸ“Š Si no lo medÃ­s, no lo podÃ©s mejorar.', en: 'ğŸ“Š If you don\'t measure it, you can\'t improve it.', pt: 'ğŸ“Š Se vocÃª nÃ£o mede, nÃ£o pode melhorar.'}, 'âœ… Â¿Creaste un dashboard de monitoreo?', 'âœ… Did you create a monitoring dashboard?', 'âœ… VocÃª criou um dashboard de monitoramento?', 30, 25),
    createStep('db-11-7', 'Performance Tuning Checklist', 'Performance Tuning Checklist', 'Checklist de Performance', 'Lista de verificaciÃ³n para optimizar performance.', 'Checklist for optimizing performance.', 'Lista de verificaÃ§Ã£o para otimizar performance.',
      `## Performance Checklist\n\n### Antes de ejecutar:\n- [ ] Usar Delta (no CSV/JSON)\n- [ ] Particionamiento correcto\n- [ ] Z-ORDER en columnas de filtro\n- [ ] AQE habilitado\n- [ ] Broadcast para tablas pequeÃ±as\n\n### Durante desarrollo:\n- [ ] Revisar Spark UI\n- [ ] Buscar skew\n- [ ] Verificar shuffle\n- [ ] Cache datos reutilizados\n\n### Post-deployment:\n- [ ] OPTIMIZE schedulado\n- [ ] VACUUM schedulado\n- [ ] Monitoreo de mÃ©tricas`,
      `## Performance Checklist\n\n### Before running:\n- [ ] Use Delta (not CSV/JSON)\n- [ ] Correct partitioning\n- [ ] Z-ORDER on filter columns\n- [ ] AQE enabled\n- [ ] Broadcast for small tables\n\n### During development:\n- [ ] Review Spark UI\n- [ ] Look for skew\n- [ ] Verify shuffle\n- [ ] Cache reused data\n\n### Post-deployment:\n- [ ] Scheduled OPTIMIZE\n- [ ] Scheduled VACUUM\n- [ ] Metrics monitoring`,
      `## Checklist de Performance\n\n### Antes de executar:\n- [ ] Usar Delta (nÃ£o CSV/JSON)\n- [ ] Particionamento correto\n- [ ] Z-ORDER em colunas de filtro\n- [ ] AQE habilitado\n- [ ] Broadcast para tabelas pequenas\n\n### Durante desenvolvimento:\n- [ ] Revisar Spark UI\n- [ ] Buscar skew\n- [ ] Verificar shuffle\n- [ ] Cache dados reutilizados\n\n### PÃ³s-deployment:\n- [ ] OPTIMIZE schedulado\n- [ ] VACUUM schedulado\n- [ ] Monitoramento de mÃ©tricas`,
      {es: 'âœ… Este checklist te va a ahorrar horas de debugging.', en: 'âœ… This checklist will save you hours of debugging.', pt: 'âœ… Este checklist vai te economizar horas de debugging.'}, 'âœ… Â¿Revisaste todos los items del checklist?', 'âœ… Did you review all checklist items?', 'âœ… VocÃª revisou todos os items do checklist?', 20, 15),
    createStep('db-11-8', 'Production Readiness', 'Production Readiness', 'ProntidÃ£o para ProduÃ§Ã£o', 'VerificÃ¡ que tu pipeline estÃ© listo para prod.', 'Verify your pipeline is ready for prod.', 'Verifique que seu pipeline estÃ¡ pronto para prod.',
      `## Production Checklist\n\n### CÃ³digo:\n- [ ] En Git\n- [ ] Tests pasando\n- [ ] Code review hecho\n- [ ] DocumentaciÃ³n\n\n### Pipeline:\n- [ ] Job clusters (no All-Purpose)\n- [ ] Retries configurados\n- [ ] Timeouts\n- [ ] Alertas de fallo\n\n### Datos:\n- [ ] Unity Catalog governance\n- [ ] Data quality checks\n- [ ] Particionamiento\n- [ ] Backup strategy\n\n### Operacional:\n- [ ] Monitoreo\n- [ ] Runbooks\n- [ ] On-call definido`,
      `## Production Checklist\n\n### Code:\n- [ ] In Git\n- [ ] Tests passing\n- [ ] Code review done\n- [ ] Documentation\n\n### Pipeline:\n- [ ] Job clusters (not All-Purpose)\n- [ ] Retries configured\n- [ ] Timeouts\n- [ ] Failure alerts\n\n### Data:\n- [ ] Unity Catalog governance\n- [ ] Data quality checks\n- [ ] Partitioning\n- [ ] Backup strategy\n\n### Operational:\n- [ ] Monitoring\n- [ ] Runbooks\n- [ ] On-call defined`,
      `## Checklist de ProduÃ§Ã£o\n\n### CÃ³digo:\n- [ ] No Git\n- [ ] Testes passando\n- [ ] Code review feito\n- [ ] DocumentaÃ§Ã£o\n\n### Pipeline:\n- [ ] Job clusters (nÃ£o All-Purpose)\n- [ ] Retries configurados\n- [ ] Timeouts\n- [ ] Alertas de falha\n\n### Dados:\n- [ ] Unity Catalog governance\n- [ ] Data quality checks\n- [ ] Particionamento\n- [ ] EstratÃ©gia de backup\n\n### Operacional:\n- [ ] Monitoramento\n- [ ] Runbooks\n- [ ] On-call definido`,
      {es: 'ğŸš€ Este checklist es lo que revisan en empresas top.', en: 'ğŸš€ This checklist is what top companies review.', pt: 'ğŸš€ Este checklist Ã© o que empresas top revisam.'}, 'âœ… Â¿Tu pipeline pasa todos los checks de producciÃ³n?', 'âœ… Does your pipeline pass all production checks?', 'âœ… Seu pipeline passa todos os checks de produÃ§Ã£o?', 25, 20),
    createStep('db-11-9', 'Proyecto: Pipeline Production-Ready', 'Project: Production-Ready Pipeline', 'Projeto: Pipeline Production-Ready', 'AplicÃ¡ todo lo aprendido en un pipeline completo.', 'Apply everything learned in a complete pipeline.', 'Aplique tudo o que aprendeu em um pipeline completo.',
      `## Proyecto Final: Best Practices\n\n### Requerimientos:\n- [ ] CÃ³digo en Git repo\n- [ ] Estructura de proyecto correcta\n- [ ] Secrets (no passwords en cÃ³digo)\n- [ ] Unit tests\n- [ ] DLT con expectations\n- [ ] Job con retries/alerts\n- [ ] Monitoreo dashboard\n- [ ] DocumentaciÃ³n completa\n\n### Bonus:\n- [ ] CI/CD con GitHub Actions\n- [ ] Feature Store\n- [ ] Data quality dashboard`,
      `## Final Project: Best Practices\n\n### Requirements:\n- [ ] Code in Git repo\n- [ ] Correct project structure\n- [ ] Secrets (no passwords in code)\n- [ ] Unit tests\n- [ ] DLT with expectations\n- [ ] Job with retries/alerts\n- [ ] Monitoring dashboard\n- [ ] Complete documentation\n\n### Bonus:\n- [ ] CI/CD with GitHub Actions\n- [ ] Feature Store\n- [ ] Data quality dashboard`,
      `## Projeto Final: Melhores PrÃ¡ticas\n\n### Requisitos:\n- [ ] CÃ³digo em Git repo\n- [ ] Estrutura de projeto correta\n- [ ] Secrets (sem senhas no cÃ³digo)\n- [ ] Unit tests\n- [ ] DLT com expectations\n- [ ] Job com retries/alerts\n- [ ] Dashboard de monitoramento\n- [ ] DocumentaÃ§Ã£o completa\n\n### BÃ´nus:\n- [ ] CI/CD com GitHub Actions\n- [ ] Feature Store\n- [ ] Dashboard de qualidade de dados`,
      {es: 'ğŸ† Este proyecto demuestra que sos production-ready.', en: 'ğŸ† This project demonstrates you\'re production-ready.', pt: 'ğŸ† Este projeto demonstra que vocÃª Ã© production-ready.'}, 'ğŸ† Â¿Completaste todos los requerimientos?', 'ğŸ† Did you complete all requirements?', 'ğŸ† VocÃª completou todos os requisitos?', 100, 90)
  ]
};

export const PHASE_12_CERTIFICATION: DatabricksPhase = {
  id: 'db-phase-12', number: 12,
  title: { es: 'CertificaciÃ³n Databricks', en: 'Databricks Certification', pt: 'CertificaÃ§Ã£o Databricks' },
  subtitle: { es: 'PreparaciÃ³n para el examen', en: 'Exam preparation', pt: 'PreparaÃ§Ã£o para o exame' },
  description: { es: 'Preparate para la certificaciÃ³n Databricks Data Engineer Associate, una de las mÃ¡s demandadas del mercado.', en: 'Prepare for the Databricks Data Engineer Associate certification, one of the most in-demand in the market.', pt: 'Prepare-se para a certificaÃ§Ã£o Databricks Data Engineer Associate, uma das mais demandadas do mercado.' },
  icon: 'ğŸ“', color: 'gold', estimatedDays: '5-7 dÃ­as',
  steps: [
    createStep('db-12-1', 'Overview del Examen', 'Exam Overview', 'VisÃ£o Geral do Exame', 'ConocÃ© la estructura y contenido del examen.', 'Know the structure and content of the exam.', 'ConheÃ§a a estrutura e conteÃºdo do exame.',
      `## Databricks DE Associate\n\n### Formato:\n- 45 preguntas\n- 90 minutos\n- 70% para aprobar\n- Proctored online\n\n### Temas:\n1. Databricks Lakehouse Platform (24%)\n2. ELT with Spark SQL & Python (29%)\n3. Incremental Processing (22%)\n4. Production Pipelines (16%)\n5. Data Governance (9%)\n\n### Costo: $200 USD\n### Validez: 2 aÃ±os`,
      `## Databricks DE Associate\n\n### Format:\n- 45 questions\n- 90 minutes\n- 70% to pass\n- Proctored online\n\n### Topics:\n1. Databricks Lakehouse Platform (24%)\n2. ELT with Spark SQL & Python (29%)\n3. Incremental Processing (22%)\n4. Production Pipelines (16%)\n5. Data Governance (9%)\n\n### Cost: $200 USD\n### Validity: 2 years`,
      `## Databricks DE Associate\n\n### Formato:\n- 45 questÃµes\n- 90 minutos\n- 70% para passar\n- Proctored online\n\n### TÃ³picos:\n1. Databricks Lakehouse Platform (24%)\n2. ELT com Spark SQL & Python (29%)\n3. Processamento Incremental (22%)\n4. Pipelines de ProduÃ§Ã£o (16%)\n5. GovernanÃ§a de Dados (9%)\n\n### Custo: $200 USD\n### Validade: 2 anos`,
      {es: 'ğŸ“š ELT con Spark es el tema mÃ¡s importante. Enfocate ahÃ­.', en: 'ğŸ“š ELT with Spark is the most important topic. Focus there.', pt: 'ğŸ“š ELT com Spark Ã© o tÃ³pico mais importante. Foque lÃ¡.'}, 'ğŸ¤” Â¿CuÃ¡nto tiempo tenÃ©s para el examen?', 'ğŸ¤” How much time do you have for the exam?', 'ğŸ¤” Quanto tempo vocÃª tem para o exame?', 15, 10),
    createStep('db-12-2', 'Temas Clave: Lakehouse Platform', 'Key Topics: Lakehouse Platform', 'TÃ³picos Chave: Lakehouse Platform', 'Lo que tenÃ©s que saber sobre la plataforma.', 'What you need to know about the platform.', 'O que vocÃª precisa saber sobre a plataforma.',
      `## Lakehouse Platform (24%)\n\n### Debes saber:\n- Control Plane vs Data Plane\n- Cluster types (All-Purpose vs Job)\n- Databricks Runtime versions\n- Repos y versionado\n- DBFS y almacenamiento\n\n### Preguntas tÃ­picas:\n- Â¿DÃ³nde se almacenan los datos?\n- Â¿CuÃ¡ndo usar Job vs All-Purpose cluster?\n- Â¿QuÃ© es un workspace?`,
      `## Lakehouse Platform (24%)\n\n### You must know:\n- Control Plane vs Data Plane\n- Cluster types (All-Purpose vs Job)\n- Databricks Runtime versions\n- Repos and versioning\n- DBFS and storage\n\n### Typical questions:\n- Where is data stored?\n- When to use Job vs All-Purpose cluster?\n- What is a workspace?`,
      `## Lakehouse Platform (24%)\n\n### VocÃª deve saber:\n- Control Plane vs Data Plane\n- Tipos de cluster (All-Purpose vs Job)\n- VersÃµes do Databricks Runtime\n- Repos e versionamento\n- DBFS e armazenamento\n\n### Perguntas tÃ­picas:\n- Onde os dados sÃ£o armazenados?\n- Quando usar Job vs All-Purpose cluster?\n- O que Ã© um workspace?`,
      {es: 'â­ La arquitectura de 2 planos es pregunta casi segura.', en: 'â­ The 2-plane architecture is almost certainly a question.', pt: 'â­ A arquitetura de 2 planos Ã© quase certa de cair.'}, 'âœ… Â¿PodÃ©s explicar Control Plane vs Data Plane?', 'âœ… Can you explain Control Plane vs Data Plane?', 'âœ… VocÃª consegue explicar Control Plane vs Data Plane?', 25, 20),
    createStep('db-12-3', 'Temas Clave: ELT con Spark', 'Key Topics: ELT with Spark', 'TÃ³picos Chave: ELT com Spark', 'El tema mÃ¡s importante del examen.', 'The most important exam topic.', 'O tÃ³pico mais importante do exame.',
      `## ELT con Spark (29%)\n\n### Debes saber:\n- Leer/escribir datos (CSV, JSON, Parquet, Delta)\n- Transformaciones (filter, select, join, groupBy)\n- Window functions\n- SQL vs DataFrame API\n- Schema enforcement\n\n### Preguntas tÃ­picas:\n- Â¿CÃ³mo leer un CSV con header?\n- Â¿Diferencia entre filter y where?\n- Â¿QuÃ© hace coalesce vs repartition?\n- Â¿CÃ³mo hacer un LEFT JOIN?`,
      `## ELT with Spark (29%)\n\n### You must know:\n- Read/write data (CSV, JSON, Parquet, Delta)\n- Transformations (filter, select, join, groupBy)\n- Window functions\n- SQL vs DataFrame API\n- Schema enforcement\n\n### Typical questions:\n- How to read a CSV with header?\n- Difference between filter and where?\n- What does coalesce vs repartition do?\n- How to do a LEFT JOIN?`,
      `## ELT com Spark (29%)\n\n### VocÃª deve saber:\n- Ler/escrever dados (CSV, JSON, Parquet, Delta)\n- TransformaÃ§Ãµes (filter, select, join, groupBy)\n- Window functions\n- SQL vs DataFrame API\n- Schema enforcement\n\n### Perguntas tÃ­picas:\n- Como ler um CSV com header?\n- DiferenÃ§a entre filter e where?\n- O que faz coalesce vs repartition?\n- Como fazer um LEFT JOIN?`,
      {es: 'âš¡ PracticÃ¡ mucho cÃ³digo. Este tema es 90% prÃ¡ctica.', en: 'âš¡ Practice a lot of code. This topic is 90% practice.', pt: 'âš¡ Pratique muito cÃ³digo. Este tÃ³pico Ã© 90% prÃ¡tica.'}, 'âœ… Â¿PodÃ©s escribir un ETL de memoria?', 'âœ… Can you write an ETL from memory?', 'âœ… VocÃª consegue escrever um ETL de memÃ³ria?', 30, 25),
    createStep('db-12-4', 'Temas Clave: Incremental Processing', 'Key Topics: Incremental Processing', 'TÃ³picos Chave: Processamento Incremental', 'Delta Lake y procesamiento incremental.', 'Delta Lake and incremental processing.', 'Delta Lake e processamento incremental.',
      `## Incremental Processing (22%)\n\n### Debes saber:\n- Delta Lake (ACID, time travel, MERGE)\n- Structured Streaming\n- Auto Loader (cloudFiles)\n- Change Data Feed\n- checkpointLocation\n\n### Preguntas tÃ­picas:\n- Â¿CÃ³mo hacer time travel a versiÃ³n anterior?\n- Â¿QuÃ© hace MERGE?\n- Â¿Diferencia entre append y complete mode?\n- Â¿QuÃ© es un checkpoint?`,
      `## Incremental Processing (22%)\n\n### You must know:\n- Delta Lake (ACID, time travel, MERGE)\n- Structured Streaming\n- Auto Loader (cloudFiles)\n- Change Data Feed\n- checkpointLocation\n\n### Typical questions:\n- How to time travel to previous version?\n- What does MERGE do?\n- Difference between append and complete mode?\n- What is a checkpoint?`,
      `## Processamento Incremental (22%)\n\n### VocÃª deve saber:\n- Delta Lake (ACID, time travel, MERGE)\n- Structured Streaming\n- Auto Loader (cloudFiles)\n- Change Data Feed\n- checkpointLocation\n\n### Perguntas tÃ­picas:\n- Como fazer time travel para versÃ£o anterior?\n- O que faz MERGE?\n- DiferenÃ§a entre append e complete mode?\n- O que Ã© um checkpoint?`,
      {es: 'ğŸ”· Delta Lake es el diferenciador de Databricks. SabÃ©lo bien.', en: 'ğŸ”· Delta Lake is Databricks\' differentiator. Know it well.', pt: 'ğŸ”· Delta Lake Ã© o diferencial do Databricks. Saiba bem.'}, 'âœ… Â¿PodÃ©s explicar los 3 output modes de streaming?', 'âœ… Can you explain the 3 streaming output modes?', 'âœ… VocÃª consegue explicar os 3 output modes de streaming?', 30, 25),
    createStep('db-12-5', 'Temas Clave: Production Pipelines', 'Key Topics: Production Pipelines', 'TÃ³picos Chave: Pipelines de ProduÃ§Ã£o', 'Workflows, DLT, y jobs.', 'Workflows, DLT, and jobs.', 'Workflows, DLT e jobs.',
      `## Production Pipelines (16%)\n\n### Debes saber:\n- Workflows (crear, schedule, dependencies)\n- Delta Live Tables (DLT)\n- Expectations (expect, expect_or_drop, expect_or_fail)\n- Job clusters\n- Notifications\n\n### Preguntas tÃ­picas:\n- Â¿CÃ³mo crear un job multi-task?\n- Â¿Diferencia entre expect y expect_or_fail?\n- Â¿QuÃ© pasa si una expectation falla?`,
      `## Production Pipelines (16%)\n\n### You must know:\n- Workflows (create, schedule, dependencies)\n- Delta Live Tables (DLT)\n- Expectations (expect, expect_or_drop, expect_or_fail)\n- Job clusters\n- Notifications\n\n### Typical questions:\n- How to create a multi-task job?\n- Difference between expect and expect_or_fail?\n- What happens if an expectation fails?`,
      `## Pipelines de ProduÃ§Ã£o (16%)\n\n### VocÃª deve saber:\n- Workflows (criar, schedule, dependÃªncias)\n- Delta Live Tables (DLT)\n- Expectations (expect, expect_or_drop, expect_or_fail)\n- Job clusters\n- NotificaÃ§Ãµes\n\n### Perguntas tÃ­picas:\n- Como criar um job multi-task?\n- DiferenÃ§a entre expect e expect_or_fail?\n- O que acontece se uma expectation falha?`,
      {es: 'ğŸ“Š Las 3 expectations de DLT son pregunta casi segura.', en: 'ğŸ“Š The 3 DLT expectations are almost certain questions.', pt: 'ğŸ“Š As 3 expectations do DLT sÃ£o quase certas de cair.'}, 'âœ… Â¿CuÃ¡l es la diferencia entre las 3 expectations?', 'âœ… What\'s the difference between the 3 expectations?', 'âœ… Qual Ã© a diferenÃ§a entre as 3 expectations?', 25, 20),
    createStep('db-12-6', 'Temas Clave: Data Governance', 'Key Topics: Data Governance', 'TÃ³picos Chave: GovernanÃ§a de Dados', 'Unity Catalog y permisos.', 'Unity Catalog and permissions.', 'Unity Catalog e permissÃµes.',
      `## Data Governance (9%)\n\n### Debes saber:\n- Unity Catalog (catalog > schema > table)\n- Permisos (GRANT, REVOKE)\n- Managed vs External tables\n- Data lineage\n- Audit logs\n\n### Preguntas tÃ­picas:\n- Â¿QuÃ© pasa al hacer DROP en managed vs external?\n- Â¿CÃ³mo dar SELECT a un grupo?\n- Â¿QuÃ© es un metastore?`,
      `## Data Governance (9%)\n\n### You must know:\n- Unity Catalog (catalog > schema > table)\n- Permissions (GRANT, REVOKE)\n- Managed vs External tables\n- Data lineage\n- Audit logs\n\n### Typical questions:\n- What happens when DROP on managed vs external?\n- How to give SELECT to a group?\n- What is a metastore?`,
      `## GovernanÃ§a de Dados (9%)\n\n### VocÃª deve saber:\n- Unity Catalog (catalog > schema > table)\n- PermissÃµes (GRANT, REVOKE)\n- Managed vs External tables\n- Data lineage\n- Audit logs\n\n### Perguntas tÃ­picas:\n- O que acontece ao fazer DROP em managed vs external?\n- Como dar SELECT a um grupo?\n- O que Ã© um metastore?`,
      {es: 'ğŸ” Managed vs External es pregunta frecuente.', en: 'ğŸ” Managed vs External is a frequent question.', pt: 'ğŸ” Managed vs External Ã© pergunta frequente.'}, 'âœ… Â¿QuÃ© pasa al hacer DROP TABLE en cada tipo?', 'âœ… What happens when you DROP TABLE on each type?', 'âœ… O que acontece ao fazer DROP TABLE em cada tipo?', 25, 20),
    createStep('db-12-7', 'Practice Tests y Recursos', 'Practice Tests and Resources', 'Testes PrÃ¡ticos e Recursos', 'Recursos para prepararte.', 'Resources to prepare.', 'Recursos para se preparar.',
      `## Recursos de Estudio\n\n### Oficiales:\n- Databricks Academy (gratis)\n- Practice Exam oficial\n- DocumentaciÃ³n\n\n### Comunidad:\n- Udemy: "Databricks Certified Data Engineer Associate"\n- YouTube: Databricks channel\n- Reddit: r/databricks\n\n### Tips:\n- HacÃ© TODOS los labs de Databricks Academy\n- PracticÃ¡ en Free Edition (gratis)\n- TomÃ¡ el practice exam 2+ veces\n- LeÃ© documentaciÃ³n de Delta Lake`,
      `## Study Resources\n\n### Official:\n- Databricks Academy (free)\n- Official Practice Exam\n- Documentation\n\n### Community:\n- Udemy: "Databricks Certified Data Engineer Associate"\n- YouTube: Databricks channel\n- Reddit: r/databricks\n\n### Tips:\n- Do ALL Databricks Academy labs\n- Practice in Free Edition (free)\n- Take practice exam 2+ times\n- Read Delta Lake documentation`,
      `## Recursos de Estudo\n\n### Oficiais:\n- Databricks Academy (grÃ¡tis)\n- Practice Exam oficial\n- DocumentaÃ§Ã£o\n\n### Comunidade:\n- Udemy: "Databricks Certified Data Engineer Associate"\n- YouTube: Canal Databricks\n- Reddit: r/databricks\n\n### Dicas:\n- FaÃ§a TODOS os labs do Databricks Academy\n- Pratique no Free Edition (grÃ¡tis)\n- FaÃ§a o practice exam 2+ vezes\n- Leia documentaÃ§Ã£o do Delta Lake`,
      {es: 'ğŸ“š Databricks Academy es el recurso #1. Es gratis y oficial.', en: 'ğŸ“š Databricks Academy is resource #1. It\'s free and official.', pt: 'ğŸ“š Databricks Academy Ã© o recurso #1. Ã‰ grÃ¡tis e oficial.'}, 'âœ… Â¿Te registraste en Databricks Academy?', 'âœ… Did you register on Databricks Academy?', 'âœ… VocÃª se registrou no Databricks Academy?', 20, 15),
    createStep('db-12-8', 'Estrategia de Examen', 'Exam Strategy', 'EstratÃ©gia de Exame', 'Tips para el dÃ­a del examen.', 'Tips for exam day.', 'Dicas para o dia do exame.',
      `## DÃ­a del Examen\n\n### Antes:\n- DormÃ­ bien\n- ProbÃ¡ tu setup (cÃ¡mara, micrÃ³fono)\n- Ambiente silencioso\n- Documento de identidad listo\n\n### Durante:\n- 2 min por pregunta mÃ¡ximo\n- MarcÃ¡ las difÃ­ciles y seguÃ­\n- Lee TODA la pregunta\n- BuscÃ¡ keywords en opciones\n- Si no sabÃ©s, eliminÃ¡ opciones\n\n### DespuÃ©s:\n- Resultado inmediato\n- Badge en 24-48hs\n- AgregÃ¡ a LinkedIn!\n\n### Â¡Ã‰XITOS! ğŸ‰`,
      `## Exam Day\n\n### Before:\n- Sleep well\n- Test your setup (camera, microphone)\n- Quiet environment\n- ID document ready\n\n### During:\n- 2 min per question max\n- Mark difficult ones and continue\n- Read the ENTIRE question\n- Look for keywords in options\n- If unsure, eliminate options\n\n### After:\n- Immediate result\n- Badge in 24-48h\n- Add to LinkedIn!\n\n### GOOD LUCK! ğŸ‰`,
      `## Dia do Exame\n\n### Antes:\n- Durma bem\n- Teste seu setup (cÃ¢mera, microfone)\n- Ambiente silencioso\n- Documento de identidade pronto\n\n### Durante:\n- 2 min por questÃ£o mÃ¡ximo\n- Marque as difÃ­ceis e continue\n- Leia a questÃ£o INTEIRA\n- Busque keywords nas opÃ§Ãµes\n- Se nÃ£o souber, elimine opÃ§Ãµes\n\n### Depois:\n- Resultado imediato\n- Badge em 24-48h\n- Adicione ao LinkedIn!\n\n### BOA SORTE! ğŸ‰`,
      {es: 'ğŸ¯ Lee toda la pregunta. Muchos errores vienen de no leer bien.', en: 'ğŸ¯ Read the whole question. Many errors come from not reading well.', pt: 'ğŸ¯ Leia toda a questÃ£o. Muitos erros vÃªm de nÃ£o ler bem.'}, 'ğŸ† Â¡EstÃ¡s listo para certificarte!', 'ğŸ† You\'re ready to get certified!', 'ğŸ† VocÃª estÃ¡ pronto para se certificar!', 30, 20)
  ]
};

// Export all phases as an array for easy use in the index
export const DATABRICKS_PHASES_9_12_FINAL: DatabricksPhase[] = [
  PHASE_9_SQL_WAREHOUSE,
  PHASE_10_MLFLOW,
  PHASE_11_BEST_PRACTICES,
  PHASE_12_CERTIFICATION
];
