/**
 * FASE 7: Databricks Workflows
 * OrquestaciÃ³n y automatizaciÃ³n de pipelines de datos
 */

import { DatabricksPhase } from '../types';

export const PHASE_7_WORKFLOWS: DatabricksPhase = {
  id: 'db-phase-7',
  number: 7,
  title: { es: 'Databricks Workflows', en: 'Databricks Workflows', pt: 'Databricks Workflows' },
  subtitle: { es: 'OrquestaciÃ³n de pipelines', en: 'Pipeline orchestration', pt: 'OrquestraÃ§Ã£o de pipelines' },
  description: { 
    es: 'Workflows es el orquestador nativo de Databricks. Aprende a crear Jobs, configurar schedules, manejar dependencias entre tareas y monitorear ejecuciones. Es la forma de llevar tus notebooks a producciÃ³n.',
    en: 'Workflows is Databricks native orchestrator. Learn to create Jobs, configure schedules, manage task dependencies and monitor executions.',
    pt: 'Workflows Ã© o orquestrador nativo do Databricks. Aprenda a criar Jobs, configurar schedules, gerenciar dependÃªncias entre tarefas e monitorar execuÃ§Ãµes.'
  },
  icon: 'âš™ï¸',
  color: 'blue',
  estimatedDays: '3-4 dÃ­as',
  steps: [
    {
      id: 'db-7-1',
      title: { es: 'Â¿QuÃ© son los Databricks Workflows?', en: 'What are Databricks Workflows?', pt: 'O que sÃ£o Databricks Workflows?' },
      description: { es: 'IntroducciÃ³n al sistema de orquestaciÃ³n nativo de Databricks.', en: 'Introduction to Databricks native orchestration system.', pt: 'IntroduÃ§Ã£o ao sistema de orquestraÃ§Ã£o nativo do Databricks.' },
      theory: {
        es: `## Databricks Workflows: Tu Pipeline en ProducciÃ³n

Workflows es el sistema de **orquestaciÃ³n nativo** de Databricks que te permite:
- Programar notebooks para ejecutarse automÃ¡ticamente
- Crear pipelines con mÃºltiples tareas dependientes
- Monitorear y recibir alertas de fallos
- Reintentar automÃ¡ticamente tareas fallidas

### Concepto Clave: Job vs Task

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           JOB                                â”‚
â”‚        "Pipeline de procesamiento de ventas"                 â”‚
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚   â”‚  TASK 1  â”‚â”€â”€â”€â–¶â”‚  TASK 2  â”‚â”€â”€â”€â–¶â”‚  TASK 3  â”‚             â”‚
â”‚   â”‚  extract â”‚    â”‚ transform â”‚    â”‚   load   â”‚             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                              â”‚
â”‚   Schedule: Diario a las 6:00 AM                            â”‚
â”‚   Cluster: Shared pool                                       â”‚
â”‚   Alertas: Slack #data-alerts                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

JOB = Contenedor que agrupa tareas relacionadas
TASK = Una unidad de trabajo (notebook, script, JAR, etc.)
\`\`\`

### Tipos de Tasks Disponibles

| Tipo | QuÃ© ejecuta | Uso comÃºn |
|------|-------------|-----------|
| **Notebook** | Un notebook de Databricks | Transformaciones con PySpark |
| **Python script** | Archivo .py | Scripts standalone |
| **SQL** | Query SQL | Agregaciones, reporting |
| **dbt** | Proyecto dbt | Transformaciones SQL |
| **JAR** | AplicaciÃ³n Java/Scala | Jobs legacy o especializados |
| **DLT Pipeline** | Pipeline de Delta Live Tables | ETL declarativo |
| **Spark Submit** | Job de Spark genÃ©rico | Migraciones desde otros clusters |

### Workflows vs Otras Herramientas

| Feature | Workflows | Airflow | Luigi |
|---------|-----------|---------|-------|
| Setup | Zero (nativo) | Medio | Medio |
| IntegraciÃ³n Databricks | Perfecta | Requiere provider | Manual |
| UI de monitoreo | Excelente | Buena | BÃ¡sica |
| Costo | Incluido | Separado | Separado |
| Multi-cloud | SÃ­ | SÃ­ | SÃ­ |

### Â¿CuÃ¡ndo usar Workflows?

**âœ… Usa Workflows cuando:**
- Tu pipeline es 100% en Databricks
- Necesitas setup rÃ¡pido sin infraestructura adicional
- Quieres integraciÃ³n nativa con Delta Lake, Unity Catalog
- Tu equipo no tiene experiencia con Airflow

**âŒ Considera Airflow cuando:**
- Necesitas orquestar servicios fuera de Databricks
- Tu organizaciÃ³n ya tiene Airflow establecido
- Necesitas DAGs muy complejos con lÃ³gica Python`,
        en: `## Databricks Workflows: Your Pipeline in Production

Workflows is Databricks **native orchestration** system.

### Key Concepts

- **Job**: Container that groups related tasks
- **Task**: Single unit of work (notebook, script, etc.)
- **Schedule**: When the job runs (cron, trigger, manual)`,
        pt: `## Databricks Workflows: Seu Pipeline em ProduÃ§Ã£o

Workflows Ã© o sistema de **orquestraÃ§Ã£o nativo** do Databricks.

### Conceitos Chave

- **Job**: Container que agrupa tarefas relacionadas
- **Task**: Unidade de trabalho (notebook, script, etc.)
- **Schedule**: Quando o job executa`
      },
      practicalTips: [
        { es: 'ğŸ’¡ Workflows es la forma mÃ¡s simple de llevar un notebook a producciÃ³n en Databricks.', en: 'ğŸ’¡ Workflows is the simplest way to take a notebook to production in Databricks.', pt: 'ğŸ’¡ Workflows Ã© a forma mais simples de levar um notebook para produÃ§Ã£o no Databricks.' },
        { es: 'âš ï¸ Si ya usas Airflow, considera el operador DatabricksSubmitRunOperator en vez de migrar todo.', en: 'âš ï¸ If you already use Airflow, consider DatabricksSubmitRunOperator instead of migrating everything.', pt: 'âš ï¸ Se jÃ¡ usa Airflow, considere o DatabricksSubmitRunOperator em vez de migrar tudo.' }
      ],
      externalLinks: [
        { title: 'Workflows Overview', url: 'https://docs.databricks.com/workflows/index.html', type: 'docs' },
        { title: 'Jobs vs Tasks', url: 'https://docs.databricks.com/workflows/jobs/jobs.html', type: 'docs' }
      ],
      checkpoint: { es: 'ğŸ¤” Â¿EntendÃ©s la diferencia entre Job y Task?', en: 'ğŸ¤” Do you understand the difference between Job and Task?', pt: 'ğŸ¤” VocÃª entende a diferenÃ§a entre Job e Task?' },
      xpReward: 20,
      estimatedMinutes: 20
    },
    {
      id: 'db-7-2',
      title: { es: 'Crear tu Primer Job', en: 'Create Your First Job', pt: 'Criar seu Primeiro Job' },
      description: { es: 'Paso a paso para crear un Job simple con un notebook.', en: 'Step by step to create a simple Job with a notebook.', pt: 'Passo a passo para criar um Job simples com um notebook.' },
      theory: {
        es: `## Crear un Job Paso a Paso

### OpciÃ³n 1: Desde la UI (Recomendado para empezar)

\`\`\`
1. Ir a "Workflows" en el sidebar izquierdo
2. Click en "Create Job"
3. Nombrar el job: "mi_primer_job"
4. Agregar una task:
   - Type: Notebook
   - Source: Seleccionar tu notebook
   - Cluster: Seleccionar o crear uno
5. Click en "Create"
\`\`\`

### OpciÃ³n 2: Desde el Notebook (RÃ¡pido para testing)

\`\`\`
1. Abrir tu notebook
2. Click en "Schedule" (arriba a la derecha)
3. Configurar schedule
4. Databricks crea el job automÃ¡ticamente
\`\`\`

### OpciÃ³n 3: Con la API/CLI (Para automatizaciÃ³n)

\`\`\`python
# Usando databricks-sdk
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.jobs import Task, NotebookTask

w = WorkspaceClient()

job = w.jobs.create(
    name="mi_primer_job_api",
    tasks=[
        Task(
            task_key="extract_data",
            notebook_task=NotebookTask(
                notebook_path="/Repos/mi-repo/notebooks/extract",
                base_parameters={"date": "{{start_date}}"}
            ),
            existing_cluster_id="0123-456789-abcdefg"
        )
    ]
)
print(f"Job creado con ID: {job.job_id}")
\`\`\`

### ConfiguraciÃ³n del Cluster para Jobs

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  OPCIONES DE CLUSTER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. JOB CLUSTER (Recomendado para producciÃ³n)               â”‚
â”‚     - Se crea al iniciar el job, se destruye al terminar    â”‚
â”‚     - MÃ¡s econÃ³mico para jobs esporÃ¡dicos                   â”‚
â”‚     - Aislamiento total                                      â”‚
â”‚                                                              â”‚
â”‚  2. EXISTING CLUSTER                                         â”‚
â”‚     - Usa un cluster ya corriendo                           â”‚
â”‚     - MÃ¡s rÃ¡pido (no hay startup time)                      â”‚
â”‚     - Riesgo de contenciÃ³n de recursos                      â”‚
â”‚                                                              â”‚
â”‚  3. JOB CLUSTER POOL                                         â”‚
â”‚     - Instancias pre-calentadas                             â”‚
â”‚     - Balance entre costo y velocidad                       â”‚
â”‚     - Ideal para jobs frecuentes                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Pasar ParÃ¡metros al Notebook

\`\`\`python
# En el notebook, recibir parÃ¡metros:
dbutils.widgets.text("date", "2024-01-01", "Fecha a procesar")
dbutils.widgets.text("environment", "dev", "Ambiente")

# Obtener valores
date = dbutils.widgets.get("date")
env = dbutils.widgets.get("environment")

print(f"Procesando {date} en ambiente {env}")

# En el Job, configurar base_parameters:
# {"date": "2024-01-15", "environment": "prod"}
\`\`\`

### Validar que el Job Funciona

\`\`\`
1. Click en "Run Now" para ejecutar manualmente
2. Ir a "Runs" para ver el estado
3. Click en la run para ver logs
4. Verificar que terminÃ³ con estado "Succeeded"
\`\`\``,
        en: `## Create a Job Step by Step

### From UI
1. Go to "Workflows"
2. Click "Create Job"
3. Add a task (Notebook type)
4. Select cluster
5. Create

### Pass Parameters
\`\`\`python
dbutils.widgets.text("date", "2024-01-01")
date = dbutils.widgets.get("date")
\`\`\``,
        pt: `## Criar um Job Passo a Passo

### Pela UI
1. Ir a "Workflows"
2. Click "Create Job"
3. Adicionar task (tipo Notebook)
4. Selecionar cluster
5. Criar`
      },
      practicalTips: [
        { es: 'ğŸš€ Para desarrollo usa "Existing Cluster", para producciÃ³n usa "Job Cluster".', en: 'ğŸš€ For development use "Existing Cluster", for production use "Job Cluster".', pt: 'ğŸš€ Para desenvolvimento use "Existing Cluster", para produÃ§Ã£o use "Job Cluster".' },
        { es: 'ğŸ’¡ Siempre usa parÃ¡metros (widgets) en vez de hardcodear valores - hace el notebook reutilizable.', en: 'ğŸ’¡ Always use parameters (widgets) instead of hardcoding values - makes the notebook reusable.', pt: 'ğŸ’¡ Sempre use parÃ¢metros (widgets) em vez de hardcodear valores - torna o notebook reutilizÃ¡vel.' }
      ],
      externalLinks: [
        { title: 'Create a Job', url: 'https://docs.databricks.com/workflows/jobs/create-run-jobs.html', type: 'docs' }
      ],
      checkpoint: { es: 'âœ… Â¿Creaste un job y lo ejecutaste manualmente?', en: 'âœ… Did you create a job and run it manually?', pt: 'âœ… VocÃª criou um job e executou manualmente?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-7-3',
      title: { es: 'Multi-Task Jobs y Dependencias', en: 'Multi-Task Jobs and Dependencies', pt: 'Jobs Multi-Task e DependÃªncias' },
      description: { es: 'Crea pipelines complejos con mÃºltiples tareas que dependen entre sÃ­.', en: 'Create complex pipelines with multiple dependent tasks.', pt: 'Crie pipelines complexos com mÃºltiplas tarefas dependentes.' },
      theory: {
        es: `## Multi-Task Jobs: Pipelines Reales

La mayorÃ­a de pipelines de datos tienen mÃºltiples pasos que deben ejecutarse en orden.

### Ejemplo: Pipeline ETL Completo

\`\`\`
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  extract_sales   â”‚
                    â”‚  (notebook)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚              â”‚
              â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ clean_sales â”‚  â”‚clean_productsâ”‚  â”‚clean_customersâ”‚
    â”‚ (notebook)  â”‚  â”‚ (notebook)   â”‚  â”‚  (notebook)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                 â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ join_and_agg   â”‚
                   â”‚  (notebook)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  write_gold    â”‚
                   â”‚   (notebook)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ notify_slack   â”‚
                   â”‚  (python)      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Configurar Dependencias en la UI

\`\`\`
1. Agregar todas las tasks al job
2. En cada task, secciÃ³n "Depends on"
3. Seleccionar las tasks que deben completarse antes
4. Databricks dibuja el DAG automÃ¡ticamente
\`\`\`

### Configurar con JSON/API

\`\`\`json
{
  "name": "sales_pipeline",
  "tasks": [
    {
      "task_key": "extract_sales",
      "notebook_task": {
        "notebook_path": "/pipelines/extract_sales"
      },
      "new_cluster": { "spark_version": "14.3.x-scala2.12", "num_workers": 2 }
    },
    {
      "task_key": "clean_sales",
      "depends_on": [{"task_key": "extract_sales"}],
      "notebook_task": {
        "notebook_path": "/pipelines/clean_sales"
      }
    },
    {
      "task_key": "clean_products",
      "depends_on": [{"task_key": "extract_sales"}],
      "notebook_task": {
        "notebook_path": "/pipelines/clean_products"
      }
    },
    {
      "task_key": "join_and_agg",
      "depends_on": [
        {"task_key": "clean_sales"},
        {"task_key": "clean_products"}
      ],
      "notebook_task": {
        "notebook_path": "/pipelines/join_and_aggregate"
      }
    }
  ]
}
\`\`\`

### Pasar Datos entre Tasks

**OpciÃ³n 1: Task Values (PequeÃ±os)**
\`\`\`python
# En task 1: Guardar valor
dbutils.jobs.taskValues.set(key="rows_processed", value=10000)
dbutils.jobs.taskValues.set(key="output_path", value="s3://bucket/output/")

# En task 2: Leer valor
rows = dbutils.jobs.taskValues.get(taskKey="extract_sales", key="rows_processed")
path = dbutils.jobs.taskValues.get(taskKey="extract_sales", key="output_path")
\`\`\`

**OpciÃ³n 2: Delta Tables (Grandes)**
\`\`\`python
# En task 1: Escribir a Delta
df.write.format("delta").mode("overwrite").saveAsTable("temp.extract_output")

# En task 2: Leer de Delta
df = spark.table("temp.extract_output")
\`\`\`

### EjecuciÃ³n en Paralelo vs Secuencial

\`\`\`
SECUENCIAL: A â†’ B â†’ C (cada uno espera al anterior)
PARALELO: A â†’ (B, C, D en paralelo) â†’ E

Tip: Las tasks sin dependencias entre sÃ­ corren en paralelo automÃ¡ticamente.
\`\`\``,
        en: `## Multi-Task Jobs: Real Pipelines

Configure dependencies to create complex pipelines.

\`\`\`json
{
  "tasks": [
    {"task_key": "extract", ...},
    {"task_key": "transform", "depends_on": [{"task_key": "extract"}]},
    {"task_key": "load", "depends_on": [{"task_key": "transform"}]}
  ]
}
\`\`\`

### Pass Data Between Tasks
\`\`\`python
# Set value
dbutils.jobs.taskValues.set(key="result", value=100)
# Get value
dbutils.jobs.taskValues.get(taskKey="task1", key="result")
\`\`\``,
        pt: `## Jobs Multi-Task: Pipelines Reais

Configure dependÃªncias para criar pipelines complexos.

\`\`\`python
# Passar dados entre tasks
dbutils.jobs.taskValues.set(key="result", value=100)
dbutils.jobs.taskValues.get(taskKey="task1", key="result")
\`\`\``
      },
      practicalTips: [
        { es: 'ğŸ”€ Tasks sin dependencias corren en PARALELO - aprovecha esto para acelerar pipelines.', en: 'ğŸ”€ Tasks without dependencies run in PARALLEL - leverage this to speed up pipelines.', pt: 'ğŸ”€ Tasks sem dependÃªncias rodam em PARALELO - aproveite isso para acelerar pipelines.' },
        { es: 'ğŸ“Š Usa Task Values para metadatos pequeÃ±os, Delta Tables para datos grandes.', en: 'ğŸ“Š Use Task Values for small metadata, Delta Tables for large data.', pt: 'ğŸ“Š Use Task Values para metadados pequenos, Delta Tables para dados grandes.' }
      ],
      externalLinks: [
        { title: 'Multi-Task Jobs', url: 'https://docs.databricks.com/workflows/jobs/jobs.html#add-tasks-to-jobs', type: 'docs' }
      ],
      checkpoint: { es: 'âœ… Â¿Creaste un job con al menos 3 tasks dependientes?', en: 'âœ… Did you create a job with at least 3 dependent tasks?', pt: 'âœ… VocÃª criou um job com pelo menos 3 tasks dependentes?' },
      xpReward: 35,
      estimatedMinutes: 40
    },
    {
      id: 'db-7-4',
      title: { es: 'Schedules y Triggers', en: 'Schedules and Triggers', pt: 'Schedules e Triggers' },
      description: { es: 'Configura cuÃ¡ndo se ejecutan tus jobs: por horario, eventos o triggers.', en: 'Configure when your jobs run: by schedule, events or triggers.', pt: 'Configure quando seus jobs executam: por schedule, eventos ou triggers.' },
      theory: {
        es: `## Schedules: CuÃ¡ndo Ejecutar

### Tipos de Triggers

| Tipo | Uso | Ejemplo |
|------|-----|---------|
| **Manual** | Testing, ad-hoc | Click en "Run Now" |
| **Scheduled** | Batch regular | Todos los dÃ­as a las 6am |
| **File Arrival** | Event-driven | Cuando llega archivo a S3 |
| **Continuous** | Streaming-like | Re-ejecutar apenas termina |

### Configurar Schedule con Cron

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minuto (0-59)
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hora (0-23)
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ dÃ­a del mes (1-31)
â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ mes (1-12)
â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ dÃ­a de la semana (0-6, 0=Domingo)
â”‚ â”‚ â”‚ â”‚ â”‚
* * * * *
\`\`\`

**Ejemplos Comunes:**

\`\`\`
# Todos los dÃ­as a las 6:00 AM
0 6 * * *

# Cada hora
0 * * * *

# Lunes a Viernes a las 9:00 AM
0 9 * * 1-5

# Primer dÃ­a de cada mes a medianoche
0 0 1 * *

# Cada 15 minutos
*/15 * * * *

# Cada 6 horas (0:00, 6:00, 12:00, 18:00)
0 */6 * * *
\`\`\`

### Timezone Importante

\`\`\`python
# En la UI, seleccionar timezone explÃ­citamente
# Por ejemplo: America/Argentina/Buenos_Aires

# âš ï¸ Default es UTC - cuidado con jobs de fin de dÃ­a
# Si quieres que corra a las 23:59 hora local, calcula el UTC
\`\`\`

### File Arrival Trigger

\`\`\`
1. En Job settings â†’ Trigger type: "File arrival"
2. Configurar:
   - URL: s3://bucket/landing/
   - Formato esperado: *.csv
3. Cuando llega un archivo, el job se dispara automÃ¡ticamente

Ãštil para: Archivos de proveedores externos, uploads manuales, etc.
\`\`\`

### Continuous Trigger

\`\`\`
El job se re-ejecuta inmediatamente despuÃ©s de terminar.
Simula streaming con batch micro-batches.

ConfiguraciÃ³n:
- Trigger type: "Continuous"
- Pause duration: 60 segundos (espera entre runs)

âš ï¸ Cuidado con costos - el cluster corre constantemente
\`\`\`

### MÃºltiples Schedules

\`\`\`
Un job puede tener mÃºltiples schedules:

Job: "sales_pipeline"
â”œâ”€â”€ Schedule 1: 0 6 * * * (daily completo)
â”œâ”€â”€ Schedule 2: 0 12 * * * (refresh de mediodÃ­a)
â””â”€â”€ Schedule 3: 0 */1 * * * (incremental cada hora)

Cada schedule puede tener parÃ¡metros diferentes.
\`\`\``,
        en: `## Schedules: When to Execute

### Cron Examples
\`\`\`
0 6 * * *     # Daily at 6am
0 * * * *     # Every hour
0 9 * * 1-5   # Weekdays at 9am
*/15 * * * *  # Every 15 minutes
\`\`\`

### Trigger Types
- Manual: Run Now button
- Scheduled: Cron expression
- File Arrival: When file lands in S3
- Continuous: Re-run when complete`,
        pt: `## Schedules: Quando Executar

### Exemplos de Cron
\`\`\`
0 6 * * *     # DiÃ¡rio Ã s 6am
0 * * * *     # A cada hora
0 9 * * 1-5   # Dias Ãºteis Ã s 9am
\`\`\``
      },
      practicalTips: [
        { es: 'â° SIEMPRE especifica timezone explÃ­citamente. El default UTC causa muchos bugs.', en: 'â° ALWAYS specify timezone explicitly. Default UTC causes many bugs.', pt: 'â° SEMPRE especifique timezone explicitamente. Default UTC causa muitos bugs.' },
        { es: 'ğŸ”„ Para jobs que deben correr "despuÃ©s del anterior", usa Continuous en vez de cron muy frecuente.', en: 'ğŸ”„ For jobs that must run "after the previous one", use Continuous instead of very frequent cron.', pt: 'ğŸ”„ Para jobs que devem rodar "depois do anterior", use Continuous em vez de cron muito frequente.' }
      ],
      externalLinks: [
        { title: 'Schedule Jobs', url: 'https://docs.databricks.com/workflows/jobs/schedule-jobs.html', type: 'docs' },
        { title: 'Crontab Guru', url: 'https://crontab.guru/', type: 'tool' }
      ],
      checkpoint: { es: 'âœ… Â¿Configuraste un schedule con el timezone correcto de tu regiÃ³n?', en: 'âœ… Did you configure a schedule with the correct timezone for your region?', pt: 'âœ… VocÃª configurou um schedule com o timezone correto da sua regiÃ£o?' },
      xpReward: 25,
      estimatedMinutes: 25
    },
    {
      id: 'db-7-5',
      title: { es: 'Manejo de Errores y Reintentos', en: 'Error Handling and Retries', pt: 'Manejo de Erros e Retentativas' },
      description: { es: 'Configura quÃ© hacer cuando una task falla: reintentar, alertar, compensar.', en: 'Configure what to do when a task fails: retry, alert, compensate.', pt: 'Configure o que fazer quando uma task falha: retentar, alertar, compensar.' },
      theory: {
        es: `## Manejo de Errores: Pipelines Robustos

### ConfiguraciÃ³n de Reintentos

\`\`\`
Por cada task, puedes configurar:
- Max retries: CuÃ¡ntas veces reintentar (ej: 3)
- Min retry interval: Espera entre reintentos (ej: 30 segundos)
- Max retry interval: Espera mÃ¡xima (ej: 5 minutos)
- Retry on timeout: Reintentar si excede tiempo lÃ­mite

Ejemplo:
- Intento 1: Falla â†’ espera 30s
- Intento 2: Falla â†’ espera 1min
- Intento 3: Falla â†’ espera 2min
- Intento 4: Falla â†’ JOB FAILED
\`\`\`

### Timeouts

\`\`\`python
# Task level timeout (mata la task si tarda demasiado)
"timeout_seconds": 3600  # 1 hora mÃ¡ximo

# Job level timeout (mata todo el job)
"timeout_seconds": 14400  # 4 horas mÃ¡ximo

# âš ï¸ Siempre configura timeouts para evitar jobs colgados que cuestan $$
\`\`\`

### On-Failure Tasks

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   extract    â”‚â”€â”€â”€â–¶â”‚  transform   â”‚â”€â”€â”€â–¶â”‚     load     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ FALLA
                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ notify_error â”‚  â—€â”€â”€ Task que corre SOLO si falla
                    â”‚  (on_failure) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

En JSON:
\`\`\`json
{
  "task_key": "notify_error",
  "run_if": "AT_LEAST_ONE_FAILED",
  "notebook_task": {...},
  "depends_on": [
    {"task_key": "transform"}
  ]
}
\`\`\`

### Run Conditions

| run_if | CuÃ¡ndo ejecuta |
|--------|----------------|
| ALL_SUCCESS | Todas las dependencias exitosas (default) |
| AT_LEAST_ONE_SUCCESS | Al menos una exitosa |
| NONE_FAILED | Ninguna fallÃ³ (incluye skipped) |
| ALL_DONE | Todas terminaron (success o failed) |
| AT_LEAST_ONE_FAILED | Al menos una fallÃ³ |

### Ejemplo: Pipeline con CompensaciÃ³n

\`\`\`
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   extract    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
              â–¼                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  transform   â”‚            â”‚  on_extract  â”‚
    â”‚ (ALL_SUCCESS)â”‚            â”‚  _failed     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜            â”‚(AT_LEAST_ONE â”‚
            â”‚                   â”‚   _FAILED)   â”‚
            â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚ EnvÃ­a alerta,
    â”‚     load     â”‚            â”‚ rollback,
    â”‚ (ALL_SUCCESS)â”‚            â”‚ etc.
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   on_success â”‚
    â”‚  (ALL_SUCCESS)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\``,
        en: `## Error Handling: Robust Pipelines

### Retries
- Max retries: 3
- Retry interval: 30s to 5min

### Timeouts
\`\`\`json
{"timeout_seconds": 3600}  // 1 hour max
\`\`\`

### On-Failure Tasks
\`\`\`json
{"run_if": "AT_LEAST_ONE_FAILED", ...}
\`\`\``,
        pt: `## Manejo de Erros: Pipelines Robustos

### Retentativas
- Max retries: 3
- Intervalo: 30s a 5min

### Timeouts
\`\`\`json
{"timeout_seconds": 3600}
\`\`\``
      },
      practicalTips: [
        { es: 'â±ï¸ SIEMPRE configura timeout. Un job colgado puede costarte cientos de dÃ³lares.', en: 'â±ï¸ ALWAYS configure timeout. A hanging job can cost you hundreds of dollars.', pt: 'â±ï¸ SEMPRE configure timeout. Um job travado pode custar centenas de dÃ³lares.' },
        { es: 'ğŸ”„ Los reintentos son buenos para errores transitorios (red, API). No para bugs en cÃ³digo.', en: 'ğŸ”„ Retries are good for transient errors (network, API). Not for code bugs.', pt: 'ğŸ”„ Retentativas sÃ£o boas para erros transitÃ³rios (rede, API). NÃ£o para bugs no cÃ³digo.' }
      ],
      externalLinks: [
        { title: 'Task Runs', url: 'https://docs.databricks.com/workflows/jobs/jobs.html#configure-task-runs', type: 'docs' }
      ],
      checkpoint: { es: 'âœ… Â¿Configuraste reintentos y una task on_failure para alertas?', en: 'âœ… Did you configure retries and an on_failure task for alerts?', pt: 'âœ… VocÃª configurou retentativas e uma task on_failure para alertas?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-7-6',
      title: { es: 'Alertas y Notificaciones', en: 'Alerts and Notifications', pt: 'Alertas e NotificaÃ§Ãµes' },
      description: { es: 'Configura alertas por email, Slack, webhooks cuando tus jobs fallan o tardan demasiado.', en: 'Configure email, Slack, webhook alerts when your jobs fail or take too long.', pt: 'Configure alertas por email, Slack, webhooks quando seus jobs falham ou demoram demais.' },
      theory: {
        es: `## Alertas: Enterarse de Problemas RÃ¡pido

### Tipos de Alertas Disponibles

| Evento | CuÃ¡ndo se dispara |
|--------|-------------------|
| On Start | Cuando el job comienza |
| On Success | Cuando termina exitosamente |
| On Failure | Cuando falla |
| On Duration Warning | Cuando excede tiempo esperado |

### Configurar Alertas por Email

\`\`\`
En Job settings â†’ Email notifications:
- On start: [devops@empresa.com]
- On success: [] (vacÃ­o - no notificar)
- On failure: [team@empresa.com, oncall@empresa.com]
- On duration threshold exceeded: [team@empresa.com]
  - Threshold: 2 hours
\`\`\`

### Configurar Alertas por Webhook (Slack, Teams, PagerDuty)

\`\`\`
1. En Job settings â†’ Webhook notifications
2. Agregar URL del webhook:
   - Slack: https://hooks.slack.com/services/XXX
   - Teams: https://outlook.office.com/webhook/XXX
3. Seleccionar eventos

El payload enviado incluye:
- job_id, run_id
- Estado (SUCCESS, FAILED, etc)
- DuraciÃ³n
- Link a la run
\`\`\`

### Crear Alert Custom con Notebook

\`\`\`python
# Task: send_slack_alert (run_if: AT_LEAST_ONE_FAILED)

import requests

def send_slack_alert(channel, message):
    webhook_url = dbutils.secrets.get("alerts", "slack_webhook")
    
    payload = {
        "channel": channel,
        "username": "Databricks Bot",
        "icon_emoji": ":warning:",
        "attachments": [{
            "color": "danger",
            "title": "Job Failed!",
            "text": message,
            "fields": [
                {"title": "Job", "value": dbutils.widgets.get("job_name"), "short": True},
                {"title": "Run ID", "value": dbutils.widgets.get("run_id"), "short": True}
            ]
        }]
    }
    
    requests.post(webhook_url, json=payload)

# Obtener info de la run fallida
failed_tasks = dbutils.jobs.taskValues.get(taskKey="check_status", key="failed_tasks")
send_slack_alert("#data-alerts", f"Tasks fallidas: {failed_tasks}")
\`\`\`

### Ejemplo: Alerta con Contexto Rico

\`\`\`python
# En tu task principal, guardar mÃ©tricas para la alerta
try:
    # Tu cÃ³digo de procesamiento
    rows_processed = df.count()
    dbutils.jobs.taskValues.set(key="rows_processed", value=rows_processed)
    dbutils.jobs.taskValues.set(key="status", value="success")
except Exception as e:
    dbutils.jobs.taskValues.set(key="error_message", value=str(e))
    dbutils.jobs.taskValues.set(key="status", value="failed")
    raise

# En task de alerta, construir mensaje rico
status = dbutils.jobs.taskValues.get(taskKey="main_task", key="status")
if status == "failed":
    error = dbutils.jobs.taskValues.get(taskKey="main_task", key="error_message")
    # Enviar alerta con el error especÃ­fico
\`\`\``,
        en: `## Alerts: Know About Problems Fast

### Alert Types
- On Start, On Success, On Failure
- On Duration Warning

### Webhook (Slack, Teams)
Add webhook URL in Job settings â†’ Webhook notifications

### Custom Alert
\`\`\`python
import requests
requests.post(webhook_url, json={"text": "Job failed!"})
\`\`\``,
        pt: `## Alertas: Saber de Problemas RÃ¡pido

### Tipos de Alerta
- On Start, On Success, On Failure
- On Duration Warning

### Webhook (Slack, Teams)
Adicionar URL do webhook em Job settings`
      },
      practicalTips: [
        { es: 'ğŸ”” Alerta en On Failure es obligatoria. Sin esto, un pipeline roto puede pasar dÃ­as sin notarse.', en: 'ğŸ”” On Failure alert is mandatory. Without this, a broken pipeline can go days unnoticed.', pt: 'ğŸ”” Alerta em On Failure Ã© obrigatÃ³rio. Sem isso, um pipeline quebrado pode passar dias despercebido.' },
        { es: 'â° Duration Warning ayuda a detectar pipelines que se estÃ¡n degradando antes de que fallen.', en: 'â° Duration Warning helps detect pipelines degrading before they fail.', pt: 'â° Duration Warning ajuda a detectar pipelines degradando antes de falharem.' }
      ],
      externalLinks: [
        { title: 'Notifications', url: 'https://docs.databricks.com/workflows/jobs/job-notifications.html', type: 'docs' }
      ],
      checkpoint: { es: 'âœ… Â¿Configuraste alertas de failure que llegan a tu equipo?', en: 'âœ… Did you configure failure alerts that reach your team?', pt: 'âœ… VocÃª configurou alertas de failure que chegam ao seu time?' },
      xpReward: 25,
      estimatedMinutes: 25
    },
    {
      id: 'db-7-7',
      title: { es: 'Monitoreo y Debugging', en: 'Monitoring and Debugging', pt: 'Monitoramento e Debugging' },
      description: { es: 'Aprende a diagnosticar problemas en jobs: logs, mÃ©tricas, Spark UI.', en: 'Learn to diagnose job problems: logs, metrics, Spark UI.', pt: 'Aprenda a diagnosticar problemas em jobs: logs, mÃ©tricas, Spark UI.' },
      theory: {
        es: `## Monitoreo: Entender quÃ© Pasa

### Dashboard de Runs

\`\`\`
En Workflows â†’ Tu Job â†’ Runs:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run History                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Run ID â”‚ Start Time   â”‚ Duration â”‚ Status                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 12345  â”‚ 2024-01-15   â”‚ 45m      â”‚ âœ… Succeeded            â”‚
â”‚ 12344  â”‚ 2024-01-14   â”‚ 1h 20m   â”‚ âš ï¸ Succeeded (slow)     â”‚
â”‚ 12343  â”‚ 2024-01-13   â”‚ 15m      â”‚ âŒ Failed               â”‚
â”‚ 12342  â”‚ 2024-01-12   â”‚ 42m      â”‚ âœ… Succeeded            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Investigar una Run Fallida

\`\`\`
1. Click en la run fallida
2. Ver quÃ© task fallÃ³ (marcada en rojo)
3. Click en la task para ver:
   - Output del notebook
   - Logs del driver
   - Stack trace del error
4. Click en "Spark UI" para mÃ©tricas detalladas
\`\`\`

### Spark UI: El Microscopio de Performance

\`\`\`
Spark UI te muestra:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JOBS TAB                                                    â”‚
â”‚ - Cada "job" de Spark (acciÃ³n que dispara ejecuciÃ³n)        â”‚
â”‚ - Tiempo por job                                            â”‚
â”‚ - Stages completados                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ STAGES TAB                                                  â”‚
â”‚ - Detalle de cada stage                                     â”‚
â”‚ - Input/Output size                                         â”‚
â”‚ - Shuffle read/write                                        â”‚
â”‚ - Tiempo por tarea                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ STORAGE TAB                                                 â”‚
â”‚ - DataFrames cacheados                                      â”‚
â”‚ - Uso de memoria                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SQL TAB                                                     â”‚
â”‚ - Plan de ejecuciÃ³n de cada query                           â”‚
â”‚ - Tiempo por operaciÃ³n                                      â”‚
â”‚ - Rows procesadas                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### MÃ©tricas Clave a Monitorear

| MÃ©trica | QuÃ© indica | AcciÃ³n si es alto |
|---------|-----------|-------------------|
| Shuffle Write | Datos movidos entre nodos | Reparticionar, broadcast joins |
| Spill | Datos que no cupieron en RAM | MÃ¡s memoria o menos datos |
| GC Time | Tiempo en garbage collection | Optimizar memoria, serializaciÃ³n |
| Skew | Particiones desbalanceadas | Salting, repartition |

### Logging Custom

\`\`\`python
# En tu notebook, agregar logs para debugging
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("my_pipeline")

logger.info(f"Starting processing for date: {date}")
logger.info(f"Input records: {df.count()}")

# Estos logs aparecen en el output del notebook
# y en los logs del driver
\`\`\`

### Comparar Runs

\`\`\`
Para identificar degradaciÃ³n:
1. Seleccionar 2 runs
2. Click "Compare"
3. Ver diferencias en:
   - DuraciÃ³n total
   - DuraciÃ³n por task
   - Recursos usados

Ãštil para: "Â¿Por quÃ© hoy tardÃ³ el doble?"
\`\`\``,
        en: `## Monitoring: Understand What's Happening

### Investigate Failed Run
1. Click on failed run
2. See which task failed
3. Check logs and stack trace
4. Open Spark UI for details

### Key Metrics
- Shuffle Write: Data moved between nodes
- Spill: Data that didn't fit in RAM
- GC Time: Garbage collection time`,
        pt: `## Monitoramento: Entender o que Acontece

### Investigar Run Fallida
1. Click na run fallida
2. Ver qual task falhou
3. Verificar logs e stack trace
4. Abrir Spark UI para detalhes`
      },
      practicalTips: [
        { es: 'ğŸ“Š Revisa el Spark UI de jobs exitosos tambiÃ©n - puedes encontrar optimizaciones antes de que sea un problema.', en: 'ğŸ“Š Review Spark UI of successful jobs too - you can find optimizations before they become problems.', pt: 'ğŸ“Š Revise o Spark UI de jobs bem-sucedidos tambÃ©m - vocÃª pode encontrar otimizaÃ§Ãµes antes que virem problemas.' },
        { es: 'ğŸ“ˆ Crea dashboards de duraciÃ³n histÃ³rica para detectar degradaciÃ³n gradual.', en: 'ğŸ“ˆ Create historical duration dashboards to detect gradual degradation.', pt: 'ğŸ“ˆ Crie dashboards de duraÃ§Ã£o histÃ³rica para detectar degradaÃ§Ã£o gradual.' }
      ],
      externalLinks: [
        { title: 'Spark UI', url: 'https://docs.databricks.com/clusters/spark-ui.html', type: 'docs' },
        { title: 'Debug Jobs', url: 'https://docs.databricks.com/workflows/jobs/debug-jobs.html', type: 'docs' }
      ],
      checkpoint: { es: 'âœ… Â¿Usaste Spark UI para investigar una run lenta?', en: 'âœ… Did you use Spark UI to investigate a slow run?', pt: 'âœ… VocÃª usou Spark UI para investigar uma run lenta?' },
      xpReward: 30,
      estimatedMinutes: 30
    },
    {
      id: 'db-7-8',
      title: { es: 'Proyecto: Pipeline de ProducciÃ³n', en: 'Project: Production Pipeline', pt: 'Projeto: Pipeline de ProduÃ§Ã£o' },
      description: { es: 'Construye un pipeline completo con mÃºltiples tasks, schedule, reintentos y alertas.', en: 'Build a complete pipeline with multiple tasks, schedule, retries and alerts.', pt: 'Construa um pipeline completo com mÃºltiplas tasks, schedule, retentativas e alertas.' },
      theory: {
        es: `## Proyecto: Pipeline de Ventas Production-Ready

Vas a crear un pipeline ETL completo que simula uno de producciÃ³n real.

### Arquitectura del Pipeline

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SALES DAILY PIPELINE                        â”‚
â”‚                  Schedule: 0 6 * * *                         â”‚
â”‚                  Timezone: America/Argentina/Buenos_Aires    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚  check_sourceâ”‚  â† Verifica que los datos fuente existen â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚          â”‚                                                   â”‚
â”‚          â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚extract_sales â”‚  â† Lee de S3/API, escribe bronze         â”‚
â”‚   â”‚  (retries: 3)â”‚                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚          â”‚                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                                            â”‚
â”‚    â–¼           â–¼                                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚ â”‚ clean  â”‚ â”‚ clean  â”‚  â† En paralelo                        â”‚
â”‚ â”‚ orders â”‚ â”‚productsâ”‚                                       â”‚
â”‚ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                       â”‚
â”‚     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚           â–¼                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚join_and_calc â”‚  â† Calcula mÃ©tricas, escribe silver      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚          â”‚                                                   â”‚
â”‚          â–¼                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚ write_gold   â”‚  â† Escribe tablas gold para BI           â”‚
â”‚   â”‚(timeout: 30m)â”‚                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚          â”‚                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”                                            â”‚
â”‚    â–¼           â–¼                                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚
â”‚ â”‚on_succ â”‚ â”‚on_fail â”‚  â† Notificaciones                     â”‚
â”‚ â”‚(Slack) â”‚ â”‚(Slack+ â”‚                                       â”‚
â”‚ â”‚        â”‚ â”‚PagerD) â”‚                                       â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Checklist de ImplementaciÃ³n

**Tasks:**
- [ ] check_source: Verifica que los archivos/tablas fuente existen
- [ ] extract_sales: Lee datos, maneja errores, escribe a bronze
- [ ] clean_orders: Limpia y valida Ã³rdenes
- [ ] clean_products: Limpia y valida productos
- [ ] join_and_calc: Join + cÃ¡lculo de mÃ©tricas
- [ ] write_gold: Escribe tablas finales
- [ ] notify_success: Slack con resumen
- [ ] notify_failure: Slack + PagerDuty con detalles

**ConfiguraciÃ³n:**
- [ ] Schedule diario a las 6am tu timezone
- [ ] Reintentos: 3 para extract, 2 para el resto
- [ ] Timeout: 30min por task, 3h total
- [ ] Job cluster con autoscaling 2-8 workers
- [ ] Tags: team=data, env=prod, domain=sales

**ParÃ¡metros:**
- [ ] date: Fecha a procesar (default: yesterday)
- [ ] environment: dev/staging/prod
- [ ] full_refresh: true/false

### CÃ³digo Base para Notebooks

**check_source.py:**
\`\`\`python
# Verificar que los datos fuente existen
date = dbutils.widgets.get("date")

source_path = f"s3://raw-data/sales/{date}/"
if not dbutils.fs.ls(source_path):
    raise Exception(f"Source data not found for {date}")

# Guardar info para siguiente task
dbutils.jobs.taskValues.set(key="source_path", value=source_path)
dbutils.jobs.taskValues.set(key="source_exists", value=True)
\`\`\`

**extract_sales.py:**
\`\`\`python
date = dbutils.widgets.get("date")
source_path = dbutils.jobs.taskValues.get(taskKey="check_source", key="source_path")

# Leer datos
df = spark.read.format("json").load(source_path)

# Validaciones bÃ¡sicas
if df.count() == 0:
    raise Exception("Empty source file!")

# Escribir a bronze
df.write.format("delta").mode("overwrite").saveAsTable(f"bronze.sales_{date.replace('-','')}")

dbutils.jobs.taskValues.set(key="rows_extracted", value=df.count())
\`\`\`

### Criterios de Ã‰xito

âœ… El pipeline corre exitosamente de principio a fin
âœ… Si falla extract, reintenta 3 veces antes de fallar definitivamente  
âœ… Las tasks paralelas (clean_*) corren en paralelo
âœ… Las alertas llegan correctamente a Slack
âœ… Los logs son suficientes para debuggear problemas
âœ… Las mÃ©tricas (rows procesadas) se reportan`,
        en: `## Project: Production-Ready Sales Pipeline

Build a complete ETL pipeline with:
- Multiple dependent tasks
- Daily schedule
- Retries and timeouts
- Success/failure notifications

See Spanish version for full architecture diagram and checklist.`,
        pt: `## Projeto: Pipeline de Vendas Production-Ready

Construa um pipeline ETL completo com:
- MÃºltiplas tasks dependentes
- Schedule diÃ¡rio
- Retentativas e timeouts
- NotificaÃ§Ãµes de sucesso/falha`
      },
      practicalTips: [
        { es: 'ğŸ—ï¸ Este proyecto es perfecto para tu portfolio de Data Engineer.', en: 'ğŸ—ï¸ This project is perfect for your Data Engineer portfolio.', pt: 'ğŸ—ï¸ Este projeto Ã© perfeito para seu portfÃ³lio de Data Engineer.' },
        { es: 'ğŸ“ Documenta las decisiones de diseÃ±o (por quÃ© 3 reintentos, por quÃ© ese timeout, etc).', en: 'ğŸ“ Document design decisions (why 3 retries, why that timeout, etc).', pt: 'ğŸ“ Documente as decisÃµes de design (por que 3 retentativas, por que esse timeout, etc).' }
      ],
      externalLinks: [
        { title: 'Best Practices', url: 'https://docs.databricks.com/workflows/jobs/jobs-best-practices.html', type: 'docs' }
      ],
      checkpoint: { es: 'ğŸ† Â¿Tu pipeline corre diariamente sin intervenciÃ³n manual y alerta cuando falla?', en: 'ğŸ† Does your pipeline run daily without manual intervention and alert when it fails?', pt: 'ğŸ† Seu pipeline roda diariamente sem intervenÃ§Ã£o manual e alerta quando falha?' },
      xpReward: 80,
      estimatedMinutes: 120
    }
  ]
};
