/**
 * FASE 7: Workflows & Jobs
 * 9 pasos para automatizar pipelines
 */

import { DatabricksPhase } from '../types';

export const PHASE_7_WORKFLOWS: DatabricksPhase = {
  id: 'db-phase-7',
  number: 7,
  title: { es: 'Workflows & Jobs', en: 'Workflows & Jobs', pt: 'Workflows & Jobs' },
  subtitle: { es: 'Automatizaci√≥n de pipelines', en: 'Pipeline automation', pt: 'Automa√ß√£o de pipelines' },
  description: { es: 'Workflows te permite orquestar y automatizar tus pipelines de datos en producci√≥n.', en: 'Workflows lets you orchestrate and automate your data pipelines in production.', pt: 'Workflows permite orquestrar e automatizar seus pipelines de dados em produ√ß√£o.' },
  icon: '‚öôÔ∏è',
  color: 'green',
  estimatedDays: '4-5 d√≠as',
  steps: [
    { id: 'db-7-1', title: { es: 'Introducci√≥n a Databricks Workflows', en: 'Introduction to Databricks Workflows', pt: 'Introdu√ß√£o ao Databricks Workflows' }, description: { es: 'Qu√© son los Workflows y c√≥mo funcionan.', en: 'What are Workflows and how they work.', pt: 'O que s√£o Workflows e como funcionam.' },
      theory: { es: `## Databricks Workflows\n\nWorkflows es la soluci√≥n nativa de Databricks para orquestar pipelines.\n\n### Componentes:\n- **Job**: Un pipeline completo\n- **Task**: Una unidad de trabajo (notebook, script, etc.)\n- **Trigger**: Cu√°ndo ejecutar (schedule, manual, API)\n- **Cluster**: D√≥nde ejecutar\n\n### Tipos de Tasks:\n- Notebook\n- Python script\n- SQL\n- dbt\n- Delta Live Tables\n- JAR/Wheel\n\n### Ventajas sobre Airflow:\n- Integraci√≥n nativa con Databricks\n- Sin infraestructura adicional\n- Spark UI integrado\n- Clusters ef√≠meros autom√°ticos`, en: `## Databricks Workflows\n\nWorkflows is Databricks' native solution for orchestrating pipelines.\n\n### Components:\n- **Job**: A complete pipeline\n- **Task**: A unit of work (notebook, script, etc.)\n- **Trigger**: When to run (schedule, manual, API)\n- **Cluster**: Where to run\n\n### Task Types:\n- Notebook\n- Python script\n- SQL\n- dbt\n- Delta Live Tables\n- JAR/Wheel\n\n### Advantages over Airflow:\n- Native Databricks integration\n- No additional infrastructure\n- Integrated Spark UI\n- Automatic ephemeral clusters`, pt: `## Databricks Workflows\n\nWorkflows √© a solu√ß√£o nativa do Databricks para orquestrar pipelines.\n\n### Componentes:\n- **Job**: Um pipeline completo\n- **Task**: Uma unidade de trabalho (notebook, script, etc.)\n- **Trigger**: Quando executar (schedule, manual, API)\n- **Cluster**: Onde executar\n\n### Tipos de Tasks:\n- Notebook\n- Python script\n- SQL\n- dbt\n- Delta Live Tables\n- JAR/Wheel\n\n### Vantagens sobre Airflow:\n- Integra√ß√£o nativa com Databricks\n- Sem infraestrutura adicional\n- Spark UI integrado\n- Clusters ef√™meros autom√°ticos` },
      practicalTips: [{ es: 'üí° Workflows es m√°s simple que Airflow para proyectos 100% Databricks.', en: 'üí° Workflows is simpler than Airflow for 100% Databricks projects.', pt: 'üí° Workflows √© mais simples que Airflow para projetos 100% Databricks.' }],
      externalLinks: [{ title: 'Workflows', url: 'https://docs.databricks.com/workflows/index.html', type: 'docs' }],
      checkpoint: { es: 'ü§î ¬øCu√°ndo usar√≠as Workflows vs Airflow?', en: 'ü§î When would you use Workflows vs Airflow?', pt: 'ü§î Quando voc√™ usaria Workflows vs Airflow?' },
      xpReward: 20, estimatedMinutes: 15 },
    { id: 'db-7-2', title: { es: 'Crear tu Primer Job', en: 'Create Your First Job', pt: 'Criar seu Primeiro Job' }, description: { es: 'Paso a paso para crear un job desde la UI.', en: 'Step by step to create a job from the UI.', pt: 'Passo a passo para criar um job pela UI.' },
      theory: { es: `## Crear Job en UI\n\n### Pasos:\n1. **Workflows** > **Create Job**\n2. **Job name**: "mi-primer-etl"\n3. **Add task**:\n   - Task name: "extract"\n   - Type: Notebook\n   - Source: Workspace\n   - Path: /Users/.../mi_notebook\n4. **Cluster**: Job cluster (m√°s barato)\n5. **Save**\n\n### Configurar Cluster:\n\`\`\`json\n{\n  "num_workers": 2,\n  "spark_version": "13.3.x-scala2.12",\n  "node_type_id": "i3.xlarge"\n}\n\`\`\`\n\n### Ejecutar:\n- Run Now (manual)\n- Add Trigger (programado)`, en: `## Create Job in UI\n\n### Steps:\n1. **Workflows** > **Create Job**\n2. **Job name**: "my-first-etl"\n3. **Add task**:\n   - Task name: "extract"\n   - Type: Notebook\n   - Source: Workspace\n   - Path: /Users/.../my_notebook\n4. **Cluster**: Job cluster (cheaper)\n5. **Save**\n\n### Configure Cluster:\n\`\`\`json\n{\n  "num_workers": 2,\n  "spark_version": "13.3.x-scala2.12",\n  "node_type_id": "i3.xlarge"\n}\n\`\`\`\n\n### Run:\n- Run Now (manual)\n- Add Trigger (scheduled)`, pt: `## Criar Job na UI\n\n### Passos:\n1. **Workflows** > **Create Job**\n2. **Job name**: "meu-primeiro-etl"\n3. **Add task**:\n   - Task name: "extract"\n   - Type: Notebook\n   - Source: Workspace\n   - Path: /Users/.../meu_notebook\n4. **Cluster**: Job cluster (mais barato)\n5. **Save**\n\n### Configurar Cluster:\n\`\`\`json\n{\n  "num_workers": 2,\n  "spark_version": "13.3.x-scala2.12",\n  "node_type_id": "i3.xlarge"\n}\n\`\`\`\n\n### Executar:\n- Run Now (manual)\n- Add Trigger (programado)` },
      practicalTips: [{ es: 'üí∞ Siempre us√° Job Clusters para jobs programados. Cuestan ~50% menos.', en: 'üí∞ Always use Job Clusters for scheduled jobs. They cost ~50% less.', pt: 'üí∞ Sempre use Job Clusters para jobs programados. Custam ~50% menos.' }],
      externalLinks: [{ title: 'Create Job', url: 'https://docs.databricks.com/workflows/jobs/create-run-jobs.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øCreaste y ejecutaste tu primer job?', en: '‚úÖ Did you create and run your first job?', pt: '‚úÖ Voc√™ criou e executou seu primeiro job?' },
      xpReward: 30, estimatedMinutes: 25 },
    { id: 'db-7-3', title: { es: 'Multi-Task Jobs: DAGs', en: 'Multi-Task Jobs: DAGs', pt: 'Multi-Task Jobs: DAGs' }, description: { es: 'Cre√° pipelines complejos con m√∫ltiples tasks y dependencias.', en: 'Create complex pipelines with multiple tasks and dependencies.', pt: 'Crie pipelines complexos com m√∫ltiplas tasks e depend√™ncias.' },
      theory: { es: `## DAGs en Workflows\n\n### Estructura t√≠pica:\n\`\`\`\n       ‚îå‚îÄ‚ñ∫ transform_ventas ‚îÄ‚îê\nextract‚î§                     ‚îú‚îÄ‚ñ∫ load\n       ‚îî‚îÄ‚ñ∫ transform_users ‚îÄ‚îÄ‚îò\n\`\`\`\n\n### Configurar dependencias:\n1. Crear task "extract"\n2. Crear task "transform_ventas"\n   - Depends on: extract\n3. Crear task "transform_users"\n   - Depends on: extract\n4. Crear task "load"\n   - Depends on: transform_ventas, transform_users\n\n### Pasar datos entre tasks:\n\`\`\`python\n# En task 1: guardar resultado\ndbutils.jobs.taskValues.set(key="count", value=df.count())\n\n# En task 2: leer resultado\ncount = dbutils.jobs.taskValues.get(\n    taskKey="task1", \n    key="count"\n)\n\`\`\``, en: `## DAGs in Workflows\n\n### Typical structure:\n\`\`\`\n       ‚îå‚îÄ‚ñ∫ transform_sales ‚îÄ‚îÄ‚îê\nextract‚î§                     ‚îú‚îÄ‚ñ∫ load\n       ‚îî‚îÄ‚ñ∫ transform_users ‚îÄ‚îÄ‚îò\n\`\`\`\n\n### Configure dependencies:\n1. Create task "extract"\n2. Create task "transform_sales"\n   - Depends on: extract\n3. Create task "transform_users"\n   - Depends on: extract\n4. Create task "load"\n   - Depends on: transform_sales, transform_users\n\n### Pass data between tasks:\n\`\`\`python\n# In task 1: save result\ndbutils.jobs.taskValues.set(key="count", value=df.count())\n\n# In task 2: read result\ncount = dbutils.jobs.taskValues.get(\n    taskKey="task1", \n    key="count"\n)\n\`\`\``, pt: `## DAGs no Workflows\n\n### Estrutura t√≠pica:\n\`\`\`\n       ‚îå‚îÄ‚ñ∫ transform_vendas ‚îÄ‚îê\nextract‚î§                     ‚îú‚îÄ‚ñ∫ load\n       ‚îî‚îÄ‚ñ∫ transform_users ‚îÄ‚îÄ‚îò\n\`\`\`\n\n### Configurar depend√™ncias:\n1. Criar task "extract"\n2. Criar task "transform_vendas"\n   - Depends on: extract\n3. Criar task "transform_users"\n   - Depends on: extract\n4. Criar task "load"\n   - Depends on: transform_vendas, transform_users\n\n### Passar dados entre tasks:\n\`\`\`python\n# Na task 1: salvar resultado\ndbutils.jobs.taskValues.set(key="count", value=df.count())\n\n# Na task 2: ler resultado\ncount = dbutils.jobs.taskValues.get(\n    taskKey="task1", \n    key="count"\n)\n\`\`\`` },
      practicalTips: [{ es: 'üîÑ Tasks en paralelo comparten el mismo cluster, ahorrando tiempo y dinero.', en: 'üîÑ Parallel tasks share the same cluster, saving time and money.', pt: 'üîÑ Tasks em paralelo compartilham o mesmo cluster, economizando tempo e dinheiro.' }],
      externalLinks: [{ title: 'Multi-task Jobs', url: 'https://docs.databricks.com/workflows/jobs/jobs.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øCreaste un job con al menos 3 tasks y dependencias?', en: '‚úÖ Did you create a job with at least 3 tasks and dependencies?', pt: '‚úÖ Voc√™ criou um job com pelo menos 3 tasks e depend√™ncias?' },
      xpReward: 35, estimatedMinutes: 30 },
    { id: 'db-7-4', title: { es: 'Schedules y Triggers', en: 'Schedules and Triggers', pt: 'Schedules e Triggers' }, description: { es: 'Program√° jobs para que corran autom√°ticamente.', en: 'Schedule jobs to run automatically.', pt: 'Programe jobs para rodar automaticamente.' },
      theory: { es: `## Triggers\n\n### Tipos:\n1. **Scheduled**: Cron expression\n2. **Continuous**: Siempre corriendo\n3. **File arrival**: Cuando llegan archivos\n\n### Schedule (Cron):\n\`\`\`\n# Todos los d√≠as a las 3am\n0 3 * * *\n\n# Cada hora\n0 * * * *\n\n# Lunes a viernes 8am\n0 8 * * 1-5\n\n# Cada 15 minutos\n*/15 * * * *\n\`\`\`\n\n### File Arrival Trigger:\n\`\`\`json\n{\n  "file_arrival": {\n    "url": "s3://bucket/path/",\n    "min_time_between_triggers": "1 minute"\n  }\n}\n\`\`\`\n\n### Timezone:\nSiempre especific√° timezone: "America/Argentina/Buenos_Aires"`, en: `## Triggers\n\n### Types:\n1. **Scheduled**: Cron expression\n2. **Continuous**: Always running\n3. **File arrival**: When files arrive\n\n### Schedule (Cron):\n\`\`\`\n# Every day at 3am\n0 3 * * *\n\n# Every hour\n0 * * * *\n\n# Monday to Friday 8am\n0 8 * * 1-5\n\n# Every 15 minutes\n*/15 * * * *\n\`\`\`\n\n### File Arrival Trigger:\n\`\`\`json\n{\n  "file_arrival": {\n    "url": "s3://bucket/path/",\n    "min_time_between_triggers": "1 minute"\n  }\n}\n\`\`\`\n\n### Timezone:\nAlways specify timezone: "America/New_York"`, pt: `## Triggers\n\n### Tipos:\n1. **Scheduled**: Cron expression\n2. **Continuous**: Sempre rodando\n3. **File arrival**: Quando arquivos chegam\n\n### Schedule (Cron):\n\`\`\`\n# Todos os dias √†s 3am\n0 3 * * *\n\n# A cada hora\n0 * * * *\n\n# Segunda a sexta 8am\n0 8 * * 1-5\n\n# A cada 15 minutos\n*/15 * * * *\n\`\`\`\n\n### File Arrival Trigger:\n\`\`\`json\n{\n  "file_arrival": {\n    "url": "s3://bucket/path/",\n    "min_time_between_triggers": "1 minute"\n  }\n}\n\`\`\`\n\n### Timezone:\nSempre especifique timezone: "America/Sao_Paulo"` },
      practicalTips: [{ es: '‚è∞ Us√° crontab.guru para validar expresiones cron.', en: '‚è∞ Use crontab.guru to validate cron expressions.', pt: '‚è∞ Use crontab.guru para validar express√µes cron.' }],
      externalLinks: [{ title: 'Triggers', url: 'https://docs.databricks.com/workflows/jobs/schedule-jobs.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øProgramaste un job para correr cada hora?', en: '‚úÖ Did you schedule a job to run every hour?', pt: '‚úÖ Voc√™ programou um job para rodar a cada hora?' },
      xpReward: 25, estimatedMinutes: 20 },
    { id: 'db-7-5', title: { es: 'Parameters y Widgets', en: 'Parameters and Widgets', pt: 'Parameters e Widgets' }, description: { es: 'Pas√° par√°metros din√°micos a tus jobs.', en: 'Pass dynamic parameters to your jobs.', pt: 'Passe par√¢metros din√¢micos para seus jobs.' },
      theory: { es: `## Par√°metros\n\n### Definir en Job:\n\`\`\`json\n{\n  "parameters": {\n    "fecha": "2024-01-15",\n    "ambiente": "prod"\n  }\n}\n\`\`\`\n\n### Usar en Notebook:\n\`\`\`python\n# Con widgets\ndbutils.widgets.text("fecha", "2024-01-01", "Fecha")\ndbutils.widgets.dropdown("ambiente", "dev", ["dev", "prod"])\n\n# Obtener valor\nfecha = dbutils.widgets.get("fecha")\n\n# En SQL\n-- SELECT * FROM ventas WHERE fecha = '{{fecha}}'\n\`\`\`\n\n### Dynamic Values:\n\`\`\`json\n{\n  "parameters": {\n    "fecha": "{{job.start_time.date}}"\n  }\n}\n\`\`\``, en: `## Parameters\n\n### Define in Job:\n\`\`\`json\n{\n  "parameters": {\n    "date": "2024-01-15",\n    "environment": "prod"\n  }\n}\n\`\`\`\n\n### Use in Notebook:\n\`\`\`python\n# With widgets\ndbutils.widgets.text("date", "2024-01-01", "Date")\ndbutils.widgets.dropdown("environment", "dev", ["dev", "prod"])\n\n# Get value\ndate = dbutils.widgets.get("date")\n\n# In SQL\n-- SELECT * FROM sales WHERE date = '{{date}}'\n\`\`\`\n\n### Dynamic Values:\n\`\`\`json\n{\n  "parameters": {\n    "date": "{{job.start_time.date}}"\n  }\n}\n\`\`\``, pt: `## Par√¢metros\n\n### Definir no Job:\n\`\`\`json\n{\n  "parameters": {\n    "data": "2024-01-15",\n    "ambiente": "prod"\n  }\n}\n\`\`\`\n\n### Usar no Notebook:\n\`\`\`python\n# Com widgets\ndbutils.widgets.text("data", "2024-01-01", "Data")\ndbutils.widgets.dropdown("ambiente", "dev", ["dev", "prod"])\n\n# Obter valor\ndata = dbutils.widgets.get("data")\n\n# Em SQL\n-- SELECT * FROM vendas WHERE data = '{{data}}'\n\`\`\`\n\n### Valores Din√¢micos:\n\`\`\`json\n{\n  "parameters": {\n    "data": "{{job.start_time.date}}"\n  }\n}\n\`\`\`` },
      practicalTips: [{ es: 'üí° Us√° widgets para hacer notebooks reutilizables entre ambientes.', en: 'üí° Use widgets to make notebooks reusable between environments.', pt: 'üí° Use widgets para fazer notebooks reutiliz√°veis entre ambientes.' }],
      externalLinks: [{ title: 'Parameters', url: 'https://docs.databricks.com/workflows/jobs/jobs.html#pass-parameters-to-a-job', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øCreaste un job con par√°metros din√°micos?', en: '‚úÖ Did you create a job with dynamic parameters?', pt: '‚úÖ Voc√™ criou um job com par√¢metros din√¢micos?' },
      xpReward: 25, estimatedMinutes: 20 },
    { id: 'db-7-6', title: { es: 'Notificaciones y Alertas', en: 'Notifications and Alerts', pt: 'Notifica√ß√µes e Alertas' }, description: { es: 'Configur√° alertas para √©xitos, fallos y duraciones.', en: 'Configure alerts for successes, failures, and durations.', pt: 'Configure alertas para sucessos, falhas e dura√ß√µes.' },
      theory: { es: `## Notificaciones\n\n### Configurar en Job:\n\`\`\`json\n{\n  "email_notifications": {\n    "on_start": ["equipo@empresa.com"],\n    "on_success": ["equipo@empresa.com"],\n    "on_failure": ["oncall@empresa.com"],\n    "on_duration_warning_threshold_exceeded": ["equipo@empresa.com"]\n  },\n  "webhook_notifications": {\n    "on_failure": [{\n      "id": "slack_webhook_id"\n    }]\n  }\n}\n\`\`\`\n\n### Integrar con Slack:\n1. Crear Slack Webhook\n2. Configurar en Databricks > Settings > Notifications\n3. Seleccionar en Job`, en: `## Notifications\n\n### Configure in Job:\n\`\`\`json\n{\n  "email_notifications": {\n    "on_start": ["team@company.com"],\n    "on_success": ["team@company.com"],\n    "on_failure": ["oncall@company.com"],\n    "on_duration_warning_threshold_exceeded": ["team@company.com"]\n  },\n  "webhook_notifications": {\n    "on_failure": [{\n      "id": "slack_webhook_id"\n    }]\n  }\n}\n\`\`\`\n\n### Integrate with Slack:\n1. Create Slack Webhook\n2. Configure in Databricks > Settings > Notifications\n3. Select in Job`, pt: `## Notifica√ß√µes\n\n### Configurar no Job:\n\`\`\`json\n{\n  "email_notifications": {\n    "on_start": ["equipe@empresa.com"],\n    "on_success": ["equipe@empresa.com"],\n    "on_failure": ["oncall@empresa.com"],\n    "on_duration_warning_threshold_exceeded": ["equipe@empresa.com"]\n  },\n  "webhook_notifications": {\n    "on_failure": [{\n      "id": "slack_webhook_id"\n    }]\n  }\n}\n\`\`\`\n\n### Integrar com Slack:\n1. Criar Slack Webhook\n2. Configurar em Databricks > Settings > Notifications\n3. Selecionar no Job` },
      practicalTips: [{ es: 'üö® Siempre configur√° alertas de fallo. Te van a salvar muchas veces.', en: 'üö® Always configure failure alerts. They\'ll save you many times.', pt: 'üö® Sempre configure alertas de falha. V√£o te salvar muitas vezes.' }],
      externalLinks: [{ title: 'Notifications', url: 'https://docs.databricks.com/workflows/jobs/job-notifications.html', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øConfiguraste notificaciones de fallo por email?', en: '‚úÖ Did you configure failure notifications by email?', pt: '‚úÖ Voc√™ configurou notifica√ß√µes de falha por email?' },
      xpReward: 20, estimatedMinutes: 15 },
    { id: 'db-7-7', title: { es: 'Retries y Error Handling', en: 'Retries and Error Handling', pt: 'Retries e Error Handling' }, description: { es: 'Hac√© tus jobs resilientes a fallos temporales.', en: 'Make your jobs resilient to temporary failures.', pt: 'Fa√ßa seus jobs resilientes a falhas tempor√°rias.' },
      theory: { es: `## Manejo de Errores\n\n### Retries autom√°ticos:\n\`\`\`json\n{\n  "max_retries": 3,\n  "min_retry_interval_millis": 60000,\n  "retry_on_timeout": true\n}\n\`\`\`\n\n### Timeout:\n\`\`\`json\n{\n  "timeout_seconds": 3600  // 1 hora\n}\n\`\`\`\n\n### En c√≥digo:\n\`\`\`python\ntry:\n    # operaci√≥n riesgosa\n    result = risky_operation()\nexcept Exception as e:\n    # loggear error\n    print(f"Error: {e}")\n    # notificar\n    dbutils.notebook.exit(f"FAILED: {e}")\n\`\`\`\n\n### Task-level handling:\n- Cada task puede tener su propia config de retry\n- Tasks fallidas no bloquean otras si no son dependencias`, en: `## Error Handling\n\n### Automatic retries:\n\`\`\`json\n{\n  "max_retries": 3,\n  "min_retry_interval_millis": 60000,\n  "retry_on_timeout": true\n}\n\`\`\`\n\n### Timeout:\n\`\`\`json\n{\n  "timeout_seconds": 3600  // 1 hour\n}\n\`\`\`\n\n### In code:\n\`\`\`python\ntry:\n    # risky operation\n    result = risky_operation()\nexcept Exception as e:\n    # log error\n    print(f"Error: {e}")\n    # notify\n    dbutils.notebook.exit(f"FAILED: {e}")\n\`\`\`\n\n### Task-level handling:\n- Each task can have its own retry config\n- Failed tasks don't block others if not dependencies`, pt: `## Tratamento de Erros\n\n### Retries autom√°ticos:\n\`\`\`json\n{\n  "max_retries": 3,\n  "min_retry_interval_millis": 60000,\n  "retry_on_timeout": true\n}\n\`\`\`\n\n### Timeout:\n\`\`\`json\n{\n  "timeout_seconds": 3600  // 1 hora\n}\n\`\`\`\n\n### No c√≥digo:\n\`\`\`python\ntry:\n    # opera√ß√£o arriscada\n    result = risky_operation()\nexcept Exception as e:\n    # logar erro\n    print(f"Error: {e}")\n    # notificar\n    dbutils.notebook.exit(f"FAILED: {e}")\n\`\`\`\n\n### Task-level handling:\n- Cada task pode ter sua pr√≥pria config de retry\n- Tasks falhadas n√£o bloqueiam outras se n√£o s√£o depend√™ncias` },
      practicalTips: [{ es: 'üîÑ 3 retries con backoff exponencial es un buen default.', en: 'üîÑ 3 retries with exponential backoff is a good default.', pt: 'üîÑ 3 retries com backoff exponencial √© um bom padr√£o.' }],
      externalLinks: [{ title: 'Retries', url: 'https://docs.databricks.com/workflows/jobs/jobs.html#configure-task-retry-policy', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øConfiguraste retries y timeout en tu job?', en: '‚úÖ Did you configure retries and timeout in your job?', pt: '‚úÖ Voc√™ configurou retries e timeout no seu job?' },
      xpReward: 25, estimatedMinutes: 20 },
    { id: 'db-7-8', title: { es: 'Jobs API: Automatizaci√≥n', en: 'Jobs API: Automation', pt: 'Jobs API: Automa√ß√£o' }, description: { es: 'Cre√° y ejecut√° jobs program√°ticamente via API.', en: 'Create and run jobs programmatically via API.', pt: 'Crie e execute jobs programaticamente via API.' },
      theory: { es: `## Jobs API\n\n### Crear Job:\n\`\`\`python\nimport requests\n\nurl = "https://WORKSPACE/api/2.1/jobs/create"\nheaders = {"Authorization": f"Bearer {TOKEN}"}\n\njob_config = {\n    "name": "mi-job-api",\n    "tasks": [{\n        "task_key": "main",\n        "notebook_task": {\n            "notebook_path": "/path/to/notebook"\n        },\n        "new_cluster": {...}\n    }]\n}\n\nresponse = requests.post(url, json=job_config, headers=headers)\njob_id = response.json()["job_id"]\n\`\`\`\n\n### Ejecutar Job:\n\`\`\`python\nurl = f"https://WORKSPACE/api/2.1/jobs/run-now"\nresponse = requests.post(url, \n    json={"job_id": job_id},\n    headers=headers\n)\nrun_id = response.json()["run_id"]\n\`\`\`\n\n### Ver estado:\n\`\`\`python\nurl = f"https://WORKSPACE/api/2.1/jobs/runs/get?run_id={run_id}"\nresponse = requests.get(url, headers=headers)\nstatus = response.json()["state"]["life_cycle_state"]\n\`\`\``, en: `## Jobs API\n\n### Create Job:\n\`\`\`python\nimport requests\n\nurl = "https://WORKSPACE/api/2.1/jobs/create"\nheaders = {"Authorization": f"Bearer {TOKEN}"}\n\njob_config = {\n    "name": "my-api-job",\n    "tasks": [{\n        "task_key": "main",\n        "notebook_task": {\n            "notebook_path": "/path/to/notebook"\n        },\n        "new_cluster": {...}\n    }]\n}\n\nresponse = requests.post(url, json=job_config, headers=headers)\njob_id = response.json()["job_id"]\n\`\`\`\n\n### Run Job:\n\`\`\`python\nurl = f"https://WORKSPACE/api/2.1/jobs/run-now"\nresponse = requests.post(url, \n    json={"job_id": job_id},\n    headers=headers\n)\nrun_id = response.json()["run_id"]\n\`\`\`\n\n### Check status:\n\`\`\`python\nurl = f"https://WORKSPACE/api/2.1/jobs/runs/get?run_id={run_id}"\nresponse = requests.get(url, headers=headers)\nstatus = response.json()["state"]["life_cycle_state"]\n\`\`\``, pt: `## Jobs API\n\n### Criar Job:\n\`\`\`python\nimport requests\n\nurl = "https://WORKSPACE/api/2.1/jobs/create"\nheaders = {"Authorization": f"Bearer {TOKEN}"}\n\njob_config = {\n    "name": "meu-job-api",\n    "tasks": [{\n        "task_key": "main",\n        "notebook_task": {\n            "notebook_path": "/path/to/notebook"\n        },\n        "new_cluster": {...}\n    }]\n}\n\nresponse = requests.post(url, json=job_config, headers=headers)\njob_id = response.json()["job_id"]\n\`\`\`\n\n### Executar Job:\n\`\`\`python\nurl = f"https://WORKSPACE/api/2.1/jobs/run-now"\nresponse = requests.post(url, \n    json={"job_id": job_id},\n    headers=headers\n)\nrun_id = response.json()["run_id"]\n\`\`\`\n\n### Ver status:\n\`\`\`python\nurl = f"https://WORKSPACE/api/2.1/jobs/runs/get?run_id={run_id}"\nresponse = requests.get(url, headers=headers)\nstatus = response.json()["state"]["life_cycle_state"]\n\`\`\`` },
      practicalTips: [{ es: 'üîß Us√° la API para CI/CD y deployment automatizado de jobs.', en: 'üîß Use the API for CI/CD and automated job deployment.', pt: 'üîß Use a API para CI/CD e deployment automatizado de jobs.' }],
      externalLinks: [{ title: 'Jobs API', url: 'https://docs.databricks.com/api/workspace/jobs', type: 'docs' }],
      checkpoint: { es: '‚úÖ ¬øEjecutaste un job via API?', en: '‚úÖ Did you run a job via API?', pt: '‚úÖ Voc√™ executou um job via API?' },
      xpReward: 30, estimatedMinutes: 30 },
    { id: 'db-7-9', title: { es: 'Proyecto: Pipeline ETL Completo', en: 'Project: Complete ETL Pipeline', pt: 'Projeto: Pipeline ETL Completo' }, description: { es: 'Constru√≠ un pipeline de producci√≥n completo.', en: 'Build a complete production pipeline.', pt: 'Construa um pipeline de produ√ß√£o completo.' },
      theory: { es: `## Proyecto: ETL Pipeline\n\n### Arquitectura:\n\`\`\`\nextract_raw ‚Üí validate ‚Üí transform_bronze ‚Üí transform_silver ‚Üí load_gold\n                ‚Üì (fallo)\n            send_alert\n\`\`\`\n\n### Checklist:\n- [ ] Job con 5+ tasks\n- [ ] Dependencias configuradas\n- [ ] Par√°metros para fecha/ambiente\n- [ ] Retries y timeouts\n- [ ] Notificaciones email/Slack\n- [ ] Schedule diario\n- [ ] Job cluster optimizado\n- [ ] Documentaci√≥n del pipeline`, en: `## Project: ETL Pipeline\n\n### Architecture:\n\`\`\`\nextract_raw ‚Üí validate ‚Üí transform_bronze ‚Üí transform_silver ‚Üí load_gold\n                ‚Üì (failure)\n            send_alert\n\`\`\`\n\n### Checklist:\n- [ ] Job with 5+ tasks\n- [ ] Dependencies configured\n- [ ] Parameters for date/environment\n- [ ] Retries and timeouts\n- [ ] Email/Slack notifications\n- [ ] Daily schedule\n- [ ] Optimized job cluster\n- [ ] Pipeline documentation`, pt: `## Projeto: Pipeline ETL\n\n### Arquitetura:\n\`\`\`\nextract_raw ‚Üí validate ‚Üí transform_bronze ‚Üí transform_silver ‚Üí load_gold\n                ‚Üì (falha)\n            send_alert\n\`\`\`\n\n### Checklist:\n- [ ] Job com 5+ tasks\n- [ ] Depend√™ncias configuradas\n- [ ] Par√¢metros para data/ambiente\n- [ ] Retries e timeouts\n- [ ] Notifica√ß√µes email/Slack\n- [ ] Schedule di√°rio\n- [ ] Job cluster otimizado\n- [ ] Documenta√ß√£o do pipeline` },
      practicalTips: [{ es: 'üèÜ Este proyecto es digno de portfolio. Documentalo bien!', en: 'üèÜ This project is portfolio-worthy. Document it well!', pt: 'üèÜ Este projeto √© digno de portf√≥lio. Documente bem!' }],
      externalLinks: [{ title: 'Workflow Patterns', url: 'https://docs.databricks.com/workflows/index.html', type: 'docs' }],
      checkpoint: { es: 'üèÜ ¬øTu pipeline corre autom√°ticamente todos los d√≠as?', en: 'üèÜ Does your pipeline run automatically every day?', pt: 'üèÜ Seu pipeline roda automaticamente todos os dias?' },
      xpReward: 100, estimatedMinutes: 90 }
  ]
};


