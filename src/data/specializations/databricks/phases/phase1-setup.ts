/**
 * FASE 1: Setup & Fundamentos de Databricks
 * 10 pasos detallados para comenzar con Databricks
 */

import { DatabricksPhase } from '../types';

export const PHASE_1_SETUP: DatabricksPhase = {
  id: 'db-phase-1',
  number: 1,
  title: {
    es: 'Setup & Fundamentos',
    en: 'Setup & Fundamentals',
    pt: 'Setup & Fundamentos'
  },
  subtitle: {
    es: 'Tu primer contacto con Databricks',
    en: 'Your first contact with Databricks',
    pt: 'Seu primeiro contato com Databricks'
  },
  description: {
    es: 'AprenderÃ¡s a crear tu cuenta gratuita en Databricks Community Edition, entender la arquitectura bÃ¡sica, y ejecutar tu primer notebook. Esta fase sienta las bases para todo lo que viene.',
    en: 'You will learn to create your free account on Databricks Community Edition, understand the basic architecture, and run your first notebook. This phase lays the foundation for everything that follows.',
    pt: 'VocÃª aprenderÃ¡ a criar sua conta gratuita no Databricks Community Edition, entender a arquitetura bÃ¡sica e executar seu primeiro notebook. Esta fase estabelece as bases para tudo o que vem a seguir.'
  },
  icon: 'ğŸš€',
  color: 'emerald',
  estimatedDays: '3-4 dÃ­as',
  steps: [
    // PASO 1.1
    {
      id: 'db-1-1',
      title: {
        es: 'Â¿QuÃ© es Databricks y por quÃ© aprenderlo?',
        en: 'What is Databricks and why learn it?',
        pt: 'O que Ã© Databricks e por que aprender?'
      },
      description: {
        es: 'EntendÃ© quÃ© problema resuelve Databricks y por quÃ© es la plataforma mÃ¡s demandada del mercado.',
        en: 'Understand what problem Databricks solves and why it is the most in-demand platform in the market.',
        pt: 'Entenda qual problema o Databricks resolve e por que Ã© a plataforma mais demandada do mercado.'
      },
      theory: {
        es: `## Â¿QuÃ© es Databricks?

Databricks es una **plataforma unificada de datos y AI** fundada por los creadores de Apache Spark. Combina:

- **Data Engineering**: Pipelines de datos escalables
- **Data Science**: Notebooks colaborativos
- **Machine Learning**: MLflow integrado
- **Data Warehousing**: SQL Analytics

### El Problema que Resuelve

Antes de Databricks, las empresas tenÃ­an:
- Data Lakes desordenados ("data swamps")
- MÃºltiples herramientas desconectadas
- Silos entre equipos de datos
- Dificultad para hacer ML en producciÃ³n

### Databricks Lakehouse

Databricks introdujo el concepto de **Lakehouse**, que combina:
- La flexibilidad de un Data Lake (almacena cualquier dato)
- La confiabilidad de un Data Warehouse (ACID, schema enforcement)

### Â¿Por quÃ© es tan demandado?

1. **Empresas top lo usan**: Shell, Comcast, Regeneron, HSBC
2. **Salarios altos**: DE con Databricks ganan 20-30% mÃ¡s
3. **Crecimiento**: #1 en Gartner Magic Quadrant
4. **Ecosistema**: Spark + Delta Lake + MLflow + Unity Catalog`,
        en: `## What is Databricks?

Databricks is a **unified data and AI platform** founded by the creators of Apache Spark. It combines:

- **Data Engineering**: Scalable data pipelines
- **Data Science**: Collaborative notebooks
- **Machine Learning**: Integrated MLflow
- **Data Warehousing**: SQL Analytics

### The Problem it Solves

Before Databricks, companies had:
- Messy Data Lakes ("data swamps")
- Multiple disconnected tools
- Silos between data teams
- Difficulty putting ML in production

### Databricks Lakehouse

Databricks introduced the **Lakehouse** concept, combining:
- The flexibility of a Data Lake (stores any data)
- The reliability of a Data Warehouse (ACID, schema enforcement)

### Why is it so in-demand?

1. **Top companies use it**: Shell, Comcast, Regeneron, HSBC
2. **High salaries**: DEs with Databricks earn 20-30% more
3. **Growth**: #1 in Gartner Magic Quadrant
4. **Ecosystem**: Spark + Delta Lake + MLflow + Unity Catalog`,
        pt: `## O que Ã© Databricks?

Databricks Ã© uma **plataforma unificada de dados e IA** fundada pelos criadores do Apache Spark. Combina:

- **Data Engineering**: Pipelines de dados escalÃ¡veis
- **Data Science**: Notebooks colaborativos
- **Machine Learning**: MLflow integrado
- **Data Warehousing**: SQL Analytics

### O Problema que Resolve

Antes do Databricks, as empresas tinham:
- Data Lakes desorganizados ("data swamps")
- MÃºltiplas ferramentas desconectadas
- Silos entre equipes de dados
- Dificuldade para colocar ML em produÃ§Ã£o

### Databricks Lakehouse

Databricks introduziu o conceito de **Lakehouse**, que combina:
- A flexibilidade de um Data Lake (armazena qualquer dado)
- A confiabilidade de um Data Warehouse (ACID, schema enforcement)

### Por que Ã© tÃ£o demandado?

1. **Empresas top usam**: Shell, Comcast, Regeneron, HSBC
2. **SalÃ¡rios altos**: DEs com Databricks ganham 20-30% mais
3. **Crescimento**: #1 no Gartner Magic Quadrant
4. **Ecossistema**: Spark + Delta Lake + MLflow + Unity Catalog`
      },
      practicalTips: [
        {
          es: 'ğŸ’¡ Databricks Community Edition es GRATIS y suficiente para aprender todo lo bÃ¡sico e intermedio.',
          en: 'ğŸ’¡ Databricks Community Edition is FREE and sufficient to learn all basic and intermediate concepts.',
          pt: 'ğŸ’¡ Databricks Community Edition Ã© GRATUITO e suficiente para aprender todos os conceitos bÃ¡sicos e intermediÃ¡rios.'
        },
        {
          es: 'ğŸ¯ AgregÃ¡ "Databricks" a tu LinkedIn ahora mismo. Es un keyword que atrae recruiters.',
          en: 'ğŸ¯ Add "Databricks" to your LinkedIn right now. It\'s a keyword that attracts recruiters.',
          pt: 'ğŸ¯ Adicione "Databricks" ao seu LinkedIn agora mesmo. Ã‰ uma keyword que atrai recrutadores.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Official Website',
          url: 'https://www.databricks.com/',
          type: 'docs'
        },
        {
          title: 'What is a Data Lakehouse? (Official Docs)',
          url: 'https://www.databricks.com/glossary/data-lakehouse',
          type: 'docs'
        },
        {
          title: 'The Data Lakehouse Architecture',
          url: 'https://www.databricks.com/product/data-lakehouse',
          type: 'article'
        }
      ],
      checkpoint: {
        es: 'ğŸ¤” Â¿PodÃ©s explicar en una oraciÃ³n quÃ© es un Lakehouse y por quÃ© es mejor que un Data Lake tradicional?',
        en: 'ğŸ¤” Can you explain in one sentence what a Lakehouse is and why it\'s better than a traditional Data Lake?',
        pt: 'ğŸ¤” VocÃª consegue explicar em uma frase o que Ã© um Lakehouse e por que Ã© melhor que um Data Lake tradicional?'
      },
      xpReward: 15,
      estimatedMinutes: 20
    },
    // PASO 1.2
    {
      id: 'db-1-2',
      title: {
        es: 'Crear cuenta en Databricks Community Edition',
        en: 'Create Databricks Community Edition account',
        pt: 'Criar conta no Databricks Community Edition'
      },
      description: {
        es: 'Registrate gratis y configurÃ¡ tu primer workspace de Databricks.',
        en: 'Register for free and set up your first Databricks workspace.',
        pt: 'Registre-se gratuitamente e configure seu primeiro workspace do Databricks.'
      },
      theory: {
        es: `## Databricks Community Edition

**Community Edition** es la versiÃ³n gratuita de Databricks, perfecta para aprender:

### QuÃ© incluye (GRATIS):
- âœ… Workspace completo
- âœ… Notebooks ilimitados
- âœ… Cluster de 15GB RAM
- âœ… Delta Lake
- âœ… MLflow bÃ¡sico
- âœ… Datasets de ejemplo

### QuÃ© NO incluye:
- âŒ Unity Catalog (governance)
- âŒ Jobs scheduling avanzado
- âŒ Clusters grandes
- âŒ Soporte enterprise

### Paso a paso para crear la cuenta:

1. **Ir a**: https://community.cloud.databricks.com/
2. **Click en "Get started for free"**
3. **Completar el formulario**:
   - Email (usÃ¡ uno profesional)
   - Nombre completo
   - Empresa (podÃ©s poner "Learning")
   - PaÃ­s
4. **Verificar email**
5. **Crear password** (mÃ­nimo 8 caracteres, 1 nÃºmero, 1 mayÃºscula)
6. **Â¡Listo!** Ya tenÃ©s tu workspace

### Tips importantes:
- El cluster se apaga automÃ¡ticamente despuÃ©s de 2 horas de inactividad
- Los datos persisten aunque el cluster estÃ© apagado
- PodÃ©s tener mÃºltiples notebooks`,
        en: `## Databricks Community Edition

**Community Edition** is the free version of Databricks, perfect for learning:

### What's included (FREE):
- âœ… Full workspace
- âœ… Unlimited notebooks
- âœ… 15GB RAM cluster
- âœ… Delta Lake
- âœ… Basic MLflow
- âœ… Sample datasets

### What's NOT included:
- âŒ Unity Catalog (governance)
- âŒ Advanced job scheduling
- âŒ Large clusters
- âŒ Enterprise support

### Step by step to create account:

1. **Go to**: https://community.cloud.databricks.com/
2. **Click "Get started for free"**
3. **Fill the form**:
   - Email (use a professional one)
   - Full name
   - Company (you can put "Learning")
   - Country
4. **Verify email**
5. **Create password** (min 8 chars, 1 number, 1 uppercase)
6. **Done!** You have your workspace

### Important tips:
- The cluster auto-shuts down after 2 hours of inactivity
- Data persists even when cluster is off
- You can have multiple notebooks`,
        pt: `## Databricks Community Edition

**Community Edition** Ã© a versÃ£o gratuita do Databricks, perfeita para aprender:

### O que inclui (GRÃTIS):
- âœ… Workspace completo
- âœ… Notebooks ilimitados
- âœ… Cluster de 15GB RAM
- âœ… Delta Lake
- âœ… MLflow bÃ¡sico
- âœ… Datasets de exemplo

### O que NÃƒO inclui:
- âŒ Unity Catalog (governance)
- âŒ Jobs scheduling avanÃ§ado
- âŒ Clusters grandes
- âŒ Suporte enterprise

### Passo a passo para criar conta:

1. **Ir para**: https://community.cloud.databricks.com/
2. **Clicar em "Get started for free"**
3. **Preencher o formulÃ¡rio**:
   - Email (use um profissional)
   - Nome completo
   - Empresa (pode colocar "Learning")
   - PaÃ­s
4. **Verificar email**
5. **Criar senha** (mÃ­n 8 caracteres, 1 nÃºmero, 1 maiÃºscula)
6. **Pronto!** VocÃª tem seu workspace

### Dicas importantes:
- O cluster desliga automaticamente apÃ³s 2 horas de inatividade
- Os dados persistem mesmo com o cluster desligado
- VocÃª pode ter mÃºltiplos notebooks`
      },
      practicalTips: [
        {
          es: 'âš ï¸ UsÃ¡ un email que revises frecuentemente. Databricks envÃ­a notificaciones importantes.',
          en: 'âš ï¸ Use an email you check frequently. Databricks sends important notifications.',
          pt: 'âš ï¸ Use um email que vocÃª verifica frequentemente. Databricks envia notificaÃ§Ãµes importantes.'
        },
        {
          es: 'ğŸ’¡ GuardÃ¡ tus credenciales en un password manager. Las vas a necesitar seguido.',
          en: 'ğŸ’¡ Save your credentials in a password manager. You\'ll need them often.',
          pt: 'ğŸ’¡ Salve suas credenciais em um gerenciador de senhas. VocÃª vai precisar delas frequentemente.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Community Edition Signup',
          url: 'https://community.cloud.databricks.com/',
          type: 'tool'
        },
        {
          title: 'Community Edition vs Full Version',
          url: 'https://docs.databricks.com/getting-started/community-edition.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Ya tenÃ©s tu cuenta creada y pudiste entrar al workspace?',
        en: 'âœ… Do you have your account created and were you able to enter the workspace?',
        pt: 'âœ… VocÃª jÃ¡ tem sua conta criada e conseguiu entrar no workspace?'
      },
      xpReward: 20,
      estimatedMinutes: 15
    },
    // PASO 1.3
    {
      id: 'db-1-3',
      title: {
        es: 'Tour por el Workspace de Databricks',
        en: 'Tour of the Databricks Workspace',
        pt: 'Tour pelo Workspace do Databricks'
      },
      description: {
        es: 'ConocÃ© cada secciÃ³n del workspace y entendÃ© para quÃ© sirve cada una.',
        en: 'Get to know each section of the workspace and understand what each one is for.',
        pt: 'ConheÃ§a cada seÃ§Ã£o do workspace e entenda para que serve cada uma.'
      },
      theory: {
        es: `## El Workspace de Databricks

Cuando entrÃ¡s a Databricks, ves una interfaz con varios elementos. Vamos a recorrerlos:

### ğŸ  Home (Inicio)
Tu pÃ¡gina principal. Muestra:
- Notebooks recientes
- Clusters activos
- Accesos rÃ¡pidos

### ğŸ“ Workspace
El "explorador de archivos" de Databricks:
- **Users/tu_usuario/**: Tu carpeta personal
- **Shared/**: Carpetas compartidas con el equipo
- **Repos/**: IntegraciÃ³n con Git

### ğŸ”§ Compute (Clusters)
Donde creÃ¡s y gestionÃ¡s clusters:
- Ver clusters activos
- Crear nuevos clusters
- Configurar recursos

### ğŸ“Š Data
Explora tus datos:
- **Databases**: Bases de datos Hive
- **Tables**: Tablas registradas
- **DBFS**: Databricks File System

### ğŸ”„ Workflows (Jobs)
AutomatizaciÃ³n y scheduling:
- Crear Jobs
- Ver ejecuciones
- Configurar triggers

### ğŸ§ª Machine Learning
Herramientas de ML:
- Experiments (MLflow)
- Models (Registry)
- Feature Store

### âš™ï¸ Settings
ConfiguraciÃ³n:
- Admin Console
- User Settings
- Workspace Settings

### NavegaciÃ³n por teclado:
- \`Ctrl + Shift + P\`: Command palette
- \`Ctrl + Alt + N\`: Nuevo notebook`,
        en: `## The Databricks Workspace

When you enter Databricks, you see an interface with several elements. Let's go through them:

### ğŸ  Home
Your main page. Shows:
- Recent notebooks
- Active clusters
- Quick access

### ğŸ“ Workspace
Databricks "file explorer":
- **Users/your_user/**: Your personal folder
- **Shared/**: Folders shared with team
- **Repos/**: Git integration

### ğŸ”§ Compute (Clusters)
Where you create and manage clusters:
- View active clusters
- Create new clusters
- Configure resources

### ğŸ“Š Data
Explore your data:
- **Databases**: Hive databases
- **Tables**: Registered tables
- **DBFS**: Databricks File System

### ğŸ”„ Workflows (Jobs)
Automation and scheduling:
- Create Jobs
- View executions
- Configure triggers

### ğŸ§ª Machine Learning
ML tools:
- Experiments (MLflow)
- Models (Registry)
- Feature Store

### âš™ï¸ Settings
Configuration:
- Admin Console
- User Settings
- Workspace Settings

### Keyboard navigation:
- \`Ctrl + Shift + P\`: Command palette
- \`Ctrl + Alt + N\`: New notebook`,
        pt: `## O Workspace do Databricks

Quando vocÃª entra no Databricks, vÃª uma interface com vÃ¡rios elementos. Vamos percorrÃª-los:

### ğŸ  Home (InÃ­cio)
Sua pÃ¡gina principal. Mostra:
- Notebooks recentes
- Clusters ativos
- Acessos rÃ¡pidos

### ğŸ“ Workspace
O "explorador de arquivos" do Databricks:
- **Users/seu_usuario/**: Sua pasta pessoal
- **Shared/**: Pastas compartilhadas com a equipe
- **Repos/**: IntegraÃ§Ã£o com Git

### ğŸ”§ Compute (Clusters)
Onde vocÃª cria e gerencia clusters:
- Ver clusters ativos
- Criar novos clusters
- Configurar recursos

### ğŸ“Š Data
Explore seus dados:
- **Databases**: Bancos de dados Hive
- **Tables**: Tabelas registradas
- **DBFS**: Databricks File System

### ğŸ”„ Workflows (Jobs)
AutomaÃ§Ã£o e scheduling:
- Criar Jobs
- Ver execuÃ§Ãµes
- Configurar triggers

### ğŸ§ª Machine Learning
Ferramentas de ML:
- Experiments (MLflow)
- Models (Registry)
- Feature Store

### âš™ï¸ Settings
ConfiguraÃ§Ã£o:
- Admin Console
- User Settings
- Workspace Settings

### NavegaÃ§Ã£o por teclado:
- \`Ctrl + Shift + P\`: Command palette
- \`Ctrl + Alt + N\`: Novo notebook`
      },
      practicalTips: [
        {
          es: 'ğŸ¯ ExplorÃ¡ cada secciÃ³n sin miedo. No podÃ©s romper nada en Community Edition.',
          en: 'ğŸ¯ Explore each section without fear. You can\'t break anything in Community Edition.',
          pt: 'ğŸ¯ Explore cada seÃ§Ã£o sem medo. VocÃª nÃ£o pode quebrar nada no Community Edition.'
        },
        {
          es: 'ğŸ’¡ CreÃ¡ una carpeta personal dentro de Workspace > Users > tu_usuario para organizar tus notebooks.',
          en: 'ğŸ’¡ Create a personal folder inside Workspace > Users > your_user to organize your notebooks.',
          pt: 'ğŸ’¡ Crie uma pasta pessoal dentro de Workspace > Users > seu_usuario para organizar seus notebooks.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Workspace Overview',
          url: 'https://docs.databricks.com/workspace/index.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'ğŸ¤” Â¿PodÃ©s encontrar dÃ³nde se crean los clusters y dÃ³nde se guardan los notebooks?',
        en: 'ğŸ¤” Can you find where clusters are created and where notebooks are saved?',
        pt: 'ğŸ¤” VocÃª consegue encontrar onde os clusters sÃ£o criados e onde os notebooks sÃ£o salvos?'
      },
      xpReward: 15,
      estimatedMinutes: 15
    },
    // PASO 1.4
    {
      id: 'db-1-4',
      title: {
        es: 'Crear tu primer Cluster',
        en: 'Create your first Cluster',
        pt: 'Criar seu primeiro Cluster'
      },
      description: {
        es: 'Un cluster es el "motor" que ejecuta tu cÃ³digo. AprendÃ© a crear y configurar uno.',
        en: 'A cluster is the "engine" that runs your code. Learn to create and configure one.',
        pt: 'Um cluster Ã© o "motor" que executa seu cÃ³digo. Aprenda a criar e configurar um.'
      },
      theory: {
        es: `## Â¿QuÃ© es un Cluster en Databricks?

Un **cluster** es un conjunto de mÃ¡quinas virtuales que ejecutan Apache Spark. Es donde corre tu cÃ³digo.

### AnatomÃ­a de un Cluster:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DRIVER NODE               â”‚
â”‚  (Coordina el trabajo, tu notebook) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Worker â”‚   â”‚ Worker  â”‚   â”‚ Worker  â”‚
â”‚ Node  â”‚   â”‚  Node   â”‚   â”‚  Node   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### En Community Edition:
- Solo podÃ©s crear **1 cluster**
- ConfiguraciÃ³n fija: **15GB RAM, 2 cores**
- Se apaga despuÃ©s de **2 horas** de inactividad
- Databricks Runtime: versiÃ³n de Spark + bibliotecas

### Crear un Cluster - Paso a paso:

1. Ir a **Compute** en el menÃº lateral
2. Click en **Create Cluster**
3. **Cluster Name**: Ponele un nombre (ej: "mi-cluster-aprendizaje")
4. **Cluster Mode**: Standard (Ãºnica opciÃ³n en CE)
5. **Databricks Runtime**: Elegir la Ãºltima LTS (Long Term Support)
   - Ej: "13.3 LTS (Spark 3.4.1, Scala 2.12)"
6. Click en **Create Cluster**
7. Esperar 3-5 minutos a que inicie

### Estados del Cluster:
- ğŸŸ¡ **Pending**: Iniciando
- ğŸŸ¢ **Running**: Listo para usar
- ğŸ”´ **Terminated**: Apagado
- ğŸŸ  **Restarting**: Reiniciando`,
        en: `## What is a Cluster in Databricks?

A **cluster** is a set of virtual machines that run Apache Spark. It's where your code runs.

### Anatomy of a Cluster:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DRIVER NODE               â”‚
â”‚  (Coordinates work, your notebook)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Worker â”‚   â”‚ Worker  â”‚   â”‚ Worker  â”‚
â”‚ Node  â”‚   â”‚  Node   â”‚   â”‚  Node   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### In Community Edition:
- You can only create **1 cluster**
- Fixed configuration: **15GB RAM, 2 cores**
- Auto-shuts down after **2 hours** of inactivity
- Databricks Runtime: Spark version + libraries

### Create a Cluster - Step by step:

1. Go to **Compute** in the sidebar
2. Click **Create Cluster**
3. **Cluster Name**: Give it a name (e.g., "my-learning-cluster")
4. **Cluster Mode**: Standard (only option in CE)
5. **Databricks Runtime**: Choose latest LTS (Long Term Support)
   - E.g., "13.3 LTS (Spark 3.4.1, Scala 2.12)"
6. Click **Create Cluster**
7. Wait 3-5 minutes for it to start

### Cluster States:
- ğŸŸ¡ **Pending**: Starting
- ğŸŸ¢ **Running**: Ready to use
- ğŸ”´ **Terminated**: Shut down
- ğŸŸ  **Restarting**: Restarting`,
        pt: `## O que Ã© um Cluster no Databricks?

Um **cluster** Ã© um conjunto de mÃ¡quinas virtuais que executam Apache Spark. Ã‰ onde seu cÃ³digo roda.

### Anatomia de um Cluster:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DRIVER NODE               â”‚
â”‚  (Coordena o trabalho, seu notebook)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚Worker â”‚   â”‚ Worker  â”‚   â”‚ Worker  â”‚
â”‚ Node  â”‚   â”‚  Node   â”‚   â”‚  Node   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### No Community Edition:
- VocÃª sÃ³ pode criar **1 cluster**
- ConfiguraÃ§Ã£o fixa: **15GB RAM, 2 cores**
- Desliga automaticamente apÃ³s **2 horas** de inatividade
- Databricks Runtime: versÃ£o do Spark + bibliotecas

### Criar um Cluster - Passo a passo:

1. Ir para **Compute** no menu lateral
2. Clicar em **Create Cluster**
3. **Cluster Name**: DÃª um nome (ex: "meu-cluster-aprendizado")
4. **Cluster Mode**: Standard (Ãºnica opÃ§Ã£o no CE)
5. **Databricks Runtime**: Escolher a Ãºltima LTS (Long Term Support)
   - Ex: "13.3 LTS (Spark 3.4.1, Scala 2.12)"
6. Clicar em **Create Cluster**
7. Esperar 3-5 minutos para iniciar

### Estados do Cluster:
- ğŸŸ¡ **Pending**: Iniciando
- ğŸŸ¢ **Running**: Pronto para usar
- ğŸ”´ **Terminated**: Desligado
- ğŸŸ  **Restarting**: Reiniciando`
      },
      practicalTips: [
        {
          es: 'â° El cluster tarda ~5 minutos en iniciar. AprovechÃ¡ para leer el siguiente paso.',
          en: 'â° The cluster takes ~5 minutes to start. Use the time to read the next step.',
          pt: 'â° O cluster leva ~5 minutos para iniciar. Aproveite para ler o prÃ³ximo passo.'
        },
        {
          es: 'ğŸ’° En versiÃ³n paga, siempre usÃ¡ "Terminate after X minutes of inactivity" para ahorrar costos.',
          en: 'ğŸ’° In paid version, always use "Terminate after X minutes of inactivity" to save costs.',
          pt: 'ğŸ’° Na versÃ£o paga, sempre use "Terminate after X minutes of inactivity" para economizar custos.'
        },
        {
          es: 'ğŸ”„ Si el cluster se apaga, podÃ©s reiniciarlo desde la misma pÃ¡gina de Compute.',
          en: 'ğŸ”„ If the cluster shuts down, you can restart it from the same Compute page.',
          pt: 'ğŸ”„ Se o cluster desligar, vocÃª pode reiniciÃ¡-lo da mesma pÃ¡gina de Compute.'
        }
      ],
      externalLinks: [
        {
          title: 'Cluster Configuration Best Practices',
          url: 'https://docs.databricks.com/clusters/configure.html',
          type: 'docs'
        },
        {
          title: 'Databricks Runtime Versions',
          url: 'https://docs.databricks.com/release-notes/runtime/releases.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Tu cluster estÃ¡ en estado "Running" (verde)?',
        en: 'âœ… Is your cluster in "Running" state (green)?',
        pt: 'âœ… Seu cluster estÃ¡ no estado "Running" (verde)?'
      },
      xpReward: 25,
      estimatedMinutes: 20
    },
    // PASO 1.5
    {
      id: 'db-1-5',
      title: {
        es: 'Tu primer Notebook en Databricks',
        en: 'Your first Notebook in Databricks',
        pt: 'Seu primeiro Notebook no Databricks'
      },
      description: {
        es: 'Los notebooks son donde escribÃ­s y ejecutÃ¡s cÃ³digo. CreÃ¡ tu primero y ejecutÃ¡ cÃ³digo Python y SQL.',
        en: 'Notebooks are where you write and run code. Create your first one and run Python and SQL code.',
        pt: 'Os notebooks sÃ£o onde vocÃª escreve e executa cÃ³digo. Crie seu primeiro e execute cÃ³digo Python e SQL.'
      },
      theory: {
        es: `## Notebooks en Databricks

Un notebook es un documento interactivo con **celdas** que pueden contener:
- CÃ³digo (Python, SQL, Scala, R)
- Markdown (documentaciÃ³n)
- Visualizaciones

### Crear un Notebook:

1. Ir a **Workspace** > **Users** > tu usuario
2. Click derecho > **Create** > **Notebook**
3. **Name**: "01-Mi-Primer-Notebook"
4. **Default Language**: Python
5. **Cluster**: Seleccionar tu cluster
6. Click **Create**

### AnatomÃ­a del Notebook:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ““ 01-Mi-Primer-Notebook    [Attach â–¼]â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [+ Code] [+ Text] [+ SQL]             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ # Celda 1 (Python)               â”‚  â”‚
â”‚  â”‚ print("Hola Databricks!")        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ %sql                             â”‚  â”‚
â”‚  â”‚ SELECT "Hola desde SQL"          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Magic Commands:

PodÃ©s cambiar el lenguaje de una celda individual:
- \`%python\` - Ejecutar Python
- \`%sql\` - Ejecutar SQL
- \`%scala\` - Ejecutar Scala
- \`%r\` - Ejecutar R
- \`%md\` - Markdown (documentaciÃ³n)
- \`%sh\` - Shell commands
- \`%fs\` - Comandos de DBFS

### Atajos de teclado:
- \`Shift + Enter\`: Ejecutar celda y avanzar
- \`Ctrl + Enter\`: Ejecutar celda sin avanzar
- \`Esc + A\`: Insertar celda arriba
- \`Esc + B\`: Insertar celda abajo`,
        en: `## Notebooks in Databricks

A notebook is an interactive document with **cells** that can contain:
- Code (Python, SQL, Scala, R)
- Markdown (documentation)
- Visualizations

### Create a Notebook:

1. Go to **Workspace** > **Users** > your user
2. Right click > **Create** > **Notebook**
3. **Name**: "01-My-First-Notebook"
4. **Default Language**: Python
5. **Cluster**: Select your cluster
6. Click **Create**

### Notebook Anatomy:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ““ 01-My-First-Notebook    [Attach â–¼] â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [+ Code] [+ Text] [+ SQL]             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ # Cell 1 (Python)                â”‚  â”‚
â”‚  â”‚ print("Hello Databricks!")       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ %sql                             â”‚  â”‚
â”‚  â”‚ SELECT "Hello from SQL"          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Magic Commands:

You can change the language of an individual cell:
- \`%python\` - Run Python
- \`%sql\` - Run SQL
- \`%scala\` - Run Scala
- \`%r\` - Run R
- \`%md\` - Markdown (documentation)
- \`%sh\` - Shell commands
- \`%fs\` - DBFS commands

### Keyboard shortcuts:
- \`Shift + Enter\`: Run cell and advance
- \`Ctrl + Enter\`: Run cell without advancing
- \`Esc + A\`: Insert cell above
- \`Esc + B\`: Insert cell below`,
        pt: `## Notebooks no Databricks

Um notebook Ã© um documento interativo com **cÃ©lulas** que podem conter:
- CÃ³digo (Python, SQL, Scala, R)
- Markdown (documentaÃ§Ã£o)
- VisualizaÃ§Ãµes

### Criar um Notebook:

1. Ir para **Workspace** > **Users** > seu usuÃ¡rio
2. Clique direito > **Create** > **Notebook**
3. **Name**: "01-Meu-Primeiro-Notebook"
4. **Default Language**: Python
5. **Cluster**: Selecionar seu cluster
6. Clicar em **Create**

### Anatomia do Notebook:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ““ 01-Meu-Primeiro-Notebook [Attach â–¼]â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [+ Code] [+ Text] [+ SQL]             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ # CÃ©lula 1 (Python)              â”‚  â”‚
â”‚  â”‚ print("OlÃ¡ Databricks!")         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ %sql                             â”‚  â”‚
â”‚  â”‚ SELECT "OlÃ¡ do SQL"              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Magic Commands:

VocÃª pode mudar a linguagem de uma cÃ©lula individual:
- \`%python\` - Executar Python
- \`%sql\` - Executar SQL
- \`%scala\` - Executar Scala
- \`%r\` - Executar R
- \`%md\` - Markdown (documentaÃ§Ã£o)
- \`%sh\` - Comandos Shell
- \`%fs\` - Comandos DBFS

### Atalhos de teclado:
- \`Shift + Enter\`: Executar cÃ©lula e avanÃ§ar
- \`Ctrl + Enter\`: Executar cÃ©lula sem avanÃ§ar
- \`Esc + A\`: Inserir cÃ©lula acima
- \`Esc + B\`: Inserir cÃ©lula abaixo`
      },
      codeExample: {
        language: 'python',
        code: `# Celda 1: Python bÃ¡sico
print("ğŸ‰ Â¡Hola Databricks!")

# Celda 2: Ver versiÃ³n de Spark
spark.version

# Celda 3: Crear un DataFrame simple
data = [("Ana", 25), ("Bob", 30), ("Carlos", 35)]
df = spark.createDataFrame(data, ["nombre", "edad"])
df.show()

# Celda 4: SQL (usar %sql al inicio de la celda)
# %sql
# SELECT * FROM VALUES 
#   ('Ana', 25), 
#   ('Bob', 30), 
#   ('Carlos', 35) 
# AS tabla(nombre, edad)`,
        explanation: {
          es: 'Este cÃ³digo muestra las operaciones bÃ¡sicas: imprimir, ver la versiÃ³n de Spark, crear un DataFrame, y ejecutar SQL.',
          en: 'This code shows basic operations: printing, checking Spark version, creating a DataFrame, and running SQL.',
          pt: 'Este cÃ³digo mostra operaÃ§Ãµes bÃ¡sicas: imprimir, verificar versÃ£o do Spark, criar um DataFrame e executar SQL.'
        }
      },
      practicalTips: [
        {
          es: 'ğŸ“ Siempre documentÃ¡ tu notebook con celdas Markdown (%md). Tu yo del futuro te lo agradecerÃ¡.',
          en: 'ğŸ“ Always document your notebook with Markdown cells (%md). Your future self will thank you.',
          pt: 'ğŸ“ Sempre documente seu notebook com cÃ©lulas Markdown (%md). Seu eu do futuro vai agradecer.'
        },
        {
          es: 'ğŸ’¡ UsÃ¡ nombres descriptivos para tus notebooks: "01-ExploraciÃ³n-Datos", "02-Limpieza", etc.',
          en: 'ğŸ’¡ Use descriptive names for your notebooks: "01-Data-Exploration", "02-Cleaning", etc.',
          pt: 'ğŸ’¡ Use nomes descritivos para seus notebooks: "01-ExploraÃ§Ã£o-Dados", "02-Limpeza", etc.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Notebooks Guide',
          url: 'https://docs.databricks.com/notebooks/index.html',
          type: 'docs'
        },
        {
          title: 'Notebook Keyboard Shortcuts',
          url: 'https://docs.databricks.com/notebooks/notebooks-manage.html#keyboard-shortcuts',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Pudiste ejecutar cÃ³digo Python y SQL en tu notebook?',
        en: 'âœ… Were you able to run Python and SQL code in your notebook?',
        pt: 'âœ… VocÃª conseguiu executar cÃ³digo Python e SQL no seu notebook?'
      },
      xpReward: 30,
      estimatedMinutes: 25
    },
    // PASO 1.6
    {
      id: 'db-1-6',
      title: {
        es: 'DBFS: El Sistema de Archivos de Databricks',
        en: 'DBFS: The Databricks File System',
        pt: 'DBFS: O Sistema de Arquivos do Databricks'
      },
      description: {
        es: 'DBFS es donde Databricks almacena datos. AprendÃ© a navegarlo y subir archivos.',
        en: 'DBFS is where Databricks stores data. Learn to navigate it and upload files.',
        pt: 'DBFS Ã© onde o Databricks armazena dados. Aprenda a navegar e fazer upload de arquivos.'
      },
      theory: {
        es: `## DBFS - Databricks File System

DBFS es una capa de abstracciÃ³n sobre el almacenamiento en la nube (S3, ADLS, GCS).

### Estructura de DBFS:

\`\`\`
dbfs:/
â”œâ”€â”€ FileStore/          # Archivos subidos por usuarios
â”‚   â”œâ”€â”€ tables/         # Datos de tablas
â”‚   â””â”€â”€ shared_uploads/ # Uploads compartidos
â”œâ”€â”€ databricks-datasets/ # Datasets de ejemplo (gratis!)
â”œâ”€â”€ user/               # Carpetas de usuarios
â””â”€â”€ mnt/                # Puntos de montaje (cloud storage)
\`\`\`

### Comandos para explorar DBFS:

\`\`\`python
# Listar contenido
%fs ls /

# Ver datasets de ejemplo
%fs ls /databricks-datasets/

# Crear carpeta
%fs mkdirs /FileStore/mi-proyecto/

# Copiar archivo
%fs cp /source/file.csv /destination/file.csv

# Ver contenido de archivo
%fs head /databricks-datasets/README.md
\`\`\`

### Desde Python:
\`\`\`python
# Usando dbutils
dbutils.fs.ls("/databricks-datasets/")

# Leer archivo como texto
dbutils.fs.head("/databricks-datasets/README.md")

# Copiar archivo
dbutils.fs.cp("source", "destination")
\`\`\`

### Datasets de Ejemplo Disponibles:

| Dataset | DescripciÃ³n | TamaÃ±o |
|---------|-------------|--------|
| /databricks-datasets/samples/population-vs-price/ | Datos de ciudades | ~10KB |
| /databricks-datasets/nyctaxi/ | Taxis de NYC | ~100MB |
| /databricks-datasets/wine-quality/ | Calidad de vinos | ~200KB |
| /databricks-datasets/COVID/ | Datos COVID-19 | ~50MB |
| /databricks-datasets/amazon/ | Reviews Amazon | ~1GB |`,
        en: `## DBFS - Databricks File System

DBFS is an abstraction layer over cloud storage (S3, ADLS, GCS).

### DBFS Structure:

\`\`\`
dbfs:/
â”œâ”€â”€ FileStore/          # User-uploaded files
â”‚   â”œâ”€â”€ tables/         # Table data
â”‚   â””â”€â”€ shared_uploads/ # Shared uploads
â”œâ”€â”€ databricks-datasets/ # Sample datasets (free!)
â”œâ”€â”€ user/               # User folders
â””â”€â”€ mnt/                # Mount points (cloud storage)
\`\`\`

### Commands to explore DBFS:

\`\`\`python
# List contents
%fs ls /

# View sample datasets
%fs ls /databricks-datasets/

# Create folder
%fs mkdirs /FileStore/my-project/

# Copy file
%fs cp /source/file.csv /destination/file.csv

# View file contents
%fs head /databricks-datasets/README.md
\`\`\`

### From Python:
\`\`\`python
# Using dbutils
dbutils.fs.ls("/databricks-datasets/")

# Read file as text
dbutils.fs.head("/databricks-datasets/README.md")

# Copy file
dbutils.fs.cp("source", "destination")
\`\`\`

### Available Sample Datasets:

| Dataset | Description | Size |
|---------|-------------|------|
| /databricks-datasets/samples/population-vs-price/ | City data | ~10KB |
| /databricks-datasets/nyctaxi/ | NYC Taxis | ~100MB |
| /databricks-datasets/wine-quality/ | Wine quality | ~200KB |
| /databricks-datasets/COVID/ | COVID-19 data | ~50MB |
| /databricks-datasets/amazon/ | Amazon reviews | ~1GB |`,
        pt: `## DBFS - Databricks File System

DBFS Ã© uma camada de abstraÃ§Ã£o sobre o armazenamento em nuvem (S3, ADLS, GCS).

### Estrutura do DBFS:

\`\`\`
dbfs:/
â”œâ”€â”€ FileStore/          # Arquivos enviados por usuÃ¡rios
â”‚   â”œâ”€â”€ tables/         # Dados de tabelas
â”‚   â””â”€â”€ shared_uploads/ # Uploads compartilhados
â”œâ”€â”€ databricks-datasets/ # Datasets de exemplo (grÃ¡tis!)
â”œâ”€â”€ user/               # Pastas de usuÃ¡rios
â””â”€â”€ mnt/                # Pontos de montagem (cloud storage)
\`\`\`

### Comandos para explorar DBFS:

\`\`\`python
# Listar conteÃºdo
%fs ls /

# Ver datasets de exemplo
%fs ls /databricks-datasets/

# Criar pasta
%fs mkdirs /FileStore/meu-projeto/

# Copiar arquivo
%fs cp /source/file.csv /destination/file.csv

# Ver conteÃºdo do arquivo
%fs head /databricks-datasets/README.md
\`\`\`

### Do Python:
\`\`\`python
# Usando dbutils
dbutils.fs.ls("/databricks-datasets/")

# Ler arquivo como texto
dbutils.fs.head("/databricks-datasets/README.md")

# Copiar arquivo
dbutils.fs.cp("source", "destination")
\`\`\`

### Datasets de Exemplo DisponÃ­veis:

| Dataset | DescriÃ§Ã£o | Tamanho |
|---------|-----------|---------|
| /databricks-datasets/samples/population-vs-price/ | Dados de cidades | ~10KB |
| /databricks-datasets/nyctaxi/ | TÃ¡xis NYC | ~100MB |
| /databricks-datasets/wine-quality/ | Qualidade de vinhos | ~200KB |
| /databricks-datasets/COVID/ | Dados COVID-19 | ~50MB |
| /databricks-datasets/amazon/ | Reviews Amazon | ~1GB |`
      },
      codeExample: {
        language: 'python',
        code: `# Explorar DBFS desde el notebook

# 1. Listar datasets de ejemplo
display(dbutils.fs.ls("/databricks-datasets/"))

# 2. Ver contenido de un archivo
print(dbutils.fs.head("/databricks-datasets/README.md", 500))

# 3. Crear tu carpeta de trabajo
dbutils.fs.mkdirs("/FileStore/mi-proyecto/")

# 4. Verificar que se creÃ³
display(dbutils.fs.ls("/FileStore/"))

# 5. Cargar un dataset de ejemplo en DataFrame
df = spark.read.csv(
    "/databricks-datasets/samples/population-vs-price/data_geo.csv",
    header=True,
    inferSchema=True
)
display(df)`,
        explanation: {
          es: 'Este cÃ³digo muestra cÃ³mo navegar DBFS, crear carpetas y cargar datos de los datasets de ejemplo.',
          en: 'This code shows how to navigate DBFS, create folders, and load data from sample datasets.',
          pt: 'Este cÃ³digo mostra como navegar no DBFS, criar pastas e carregar dados dos datasets de exemplo.'
        }
      },
      practicalTips: [
        {
          es: 'ğŸ“‚ Los datasets en /databricks-datasets/ son perfectos para practicar sin tener que subir datos propios.',
          en: 'ğŸ“‚ The datasets in /databricks-datasets/ are perfect for practicing without uploading your own data.',
          pt: 'ğŸ“‚ Os datasets em /databricks-datasets/ sÃ£o perfeitos para praticar sem precisar fazer upload de dados prÃ³prios.'
        },
        {
          es: 'âš ï¸ En Community Edition, los datos en DBFS se eliminan si no usÃ¡s tu cuenta por 14 dÃ­as.',
          en: 'âš ï¸ In Community Edition, DBFS data is deleted if you don\'t use your account for 14 days.',
          pt: 'âš ï¸ No Community Edition, os dados no DBFS sÃ£o deletados se vocÃª nÃ£o usar sua conta por 14 dias.'
        }
      ],
      externalLinks: [
        {
          title: 'DBFS Documentation',
          url: 'https://docs.databricks.com/dbfs/index.html',
          type: 'docs'
        },
        {
          title: 'Sample Datasets',
          url: 'https://docs.databricks.com/dbfs/databricks-datasets.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Pudiste listar los datasets de ejemplo y cargar uno en un DataFrame?',
        en: 'âœ… Were you able to list the sample datasets and load one into a DataFrame?',
        pt: 'âœ… VocÃª conseguiu listar os datasets de exemplo e carregar um em um DataFrame?'
      },
      xpReward: 25,
      estimatedMinutes: 20
    },
    // PASO 1.7
    {
      id: 'db-1-7',
      title: {
        es: 'Subir tus propios datos a Databricks',
        en: 'Upload your own data to Databricks',
        pt: 'Fazer upload dos seus prÃ³prios dados para o Databricks'
      },
      description: {
        es: 'AprendÃ© a subir archivos CSV, JSON y Parquet desde tu computadora.',
        en: 'Learn to upload CSV, JSON and Parquet files from your computer.',
        pt: 'Aprenda a fazer upload de arquivos CSV, JSON e Parquet do seu computador.'
      },
      theory: {
        es: `## Subir Datos a Databricks

Hay varias formas de subir datos:

### MÃ©todo 1: UI de Databricks (mÃ¡s fÃ¡cil)

1. Ir a **Data** en el menÃº lateral
2. Click en **Create Table**
3. **Drop files to upload** o click para seleccionar
4. Elegir opciones:
   - Crear tabla o solo subir archivo
   - Nombre de la tabla
   - Tipo de archivo (CSV, JSON, Parquet, etc.)

### MÃ©todo 2: Arrastrar al Notebook

1. Abrir un notebook
2. Arrastrar archivo desde tu computadora al notebook
3. Databricks genera cÃ³digo automÃ¡ticamente

### MÃ©todo 3: dbutils (programÃ¡tico)

\`\`\`python
# DespuÃ©s de subir por UI, el archivo queda en:
# /FileStore/tables/tu_archivo.csv

# Leerlo
df = spark.read.csv(
    "/FileStore/tables/tu_archivo.csv",
    header=True,
    inferSchema=True
)
\`\`\`

### MÃ©todo 4: Desde URL externa

\`\`\`python
# Descargar desde internet
import urllib.request

url = "https://example.com/data.csv"
local_path = "/tmp/data.csv"
dbfs_path = "/FileStore/data.csv"

# Descargar
urllib.request.urlretrieve(url, local_path)

# Copiar a DBFS
dbutils.fs.cp(f"file:{local_path}", f"dbfs:{dbfs_path}")
\`\`\`

### Formatos soportados:
| Formato | ExtensiÃ³n | Uso recomendado |
|---------|-----------|-----------------|
| CSV | .csv | Datos pequeÃ±os, legibles |
| JSON | .json | APIs, documentos |
| Parquet | .parquet | ProducciÃ³n, performance |
| Delta | .delta | Databricks native |
| Avro | .avro | Streaming |
| ORC | .orc | Hive compatibility |`,
        en: `## Upload Data to Databricks

There are several ways to upload data:

### Method 1: Databricks UI (easiest)

1. Go to **Data** in the sidebar
2. Click **Create Table**
3. **Drop files to upload** or click to select
4. Choose options:
   - Create table or just upload file
   - Table name
   - File type (CSV, JSON, Parquet, etc.)

### Method 2: Drag to Notebook

1. Open a notebook
2. Drag file from your computer to the notebook
3. Databricks generates code automatically

### Method 3: dbutils (programmatic)

\`\`\`python
# After uploading via UI, the file is at:
# /FileStore/tables/your_file.csv

# Read it
df = spark.read.csv(
    "/FileStore/tables/your_file.csv",
    header=True,
    inferSchema=True
)
\`\`\`

### Method 4: From external URL

\`\`\`python
# Download from internet
import urllib.request

url = "https://example.com/data.csv"
local_path = "/tmp/data.csv"
dbfs_path = "/FileStore/data.csv"

# Download
urllib.request.urlretrieve(url, local_path)

# Copy to DBFS
dbutils.fs.cp(f"file:{local_path}", f"dbfs:{dbfs_path}")
\`\`\`

### Supported formats:
| Format | Extension | Recommended use |
|--------|-----------|-----------------|
| CSV | .csv | Small data, readable |
| JSON | .json | APIs, documents |
| Parquet | .parquet | Production, performance |
| Delta | .delta | Databricks native |
| Avro | .avro | Streaming |
| ORC | .orc | Hive compatibility |`,
        pt: `## Fazer Upload de Dados para o Databricks

Existem vÃ¡rias formas de fazer upload de dados:

### MÃ©todo 1: UI do Databricks (mais fÃ¡cil)

1. Ir para **Data** no menu lateral
2. Clicar em **Create Table**
3. **Drop files to upload** ou clicar para selecionar
4. Escolher opÃ§Ãµes:
   - Criar tabela ou apenas fazer upload
   - Nome da tabela
   - Tipo de arquivo (CSV, JSON, Parquet, etc.)

### MÃ©todo 2: Arrastar para o Notebook

1. Abrir um notebook
2. Arrastar arquivo do seu computador para o notebook
3. Databricks gera cÃ³digo automaticamente

### MÃ©todo 3: dbutils (programÃ¡tico)

\`\`\`python
# ApÃ³s fazer upload pela UI, o arquivo fica em:
# /FileStore/tables/seu_arquivo.csv

# Ler
df = spark.read.csv(
    "/FileStore/tables/seu_arquivo.csv",
    header=True,
    inferSchema=True
)
\`\`\`

### MÃ©todo 4: De URL externa

\`\`\`python
# Baixar da internet
import urllib.request

url = "https://example.com/data.csv"
local_path = "/tmp/data.csv"
dbfs_path = "/FileStore/data.csv"

# Baixar
urllib.request.urlretrieve(url, local_path)

# Copiar para DBFS
dbutils.fs.cp(f"file:{local_path}", f"dbfs:{dbfs_path}")
\`\`\`

### Formatos suportados:
| Formato | ExtensÃ£o | Uso recomendado |
|---------|----------|-----------------|
| CSV | .csv | Dados pequenos, legÃ­veis |
| JSON | .json | APIs, documentos |
| Parquet | .parquet | ProduÃ§Ã£o, performance |
| Delta | .delta | Databricks nativo |
| Avro | .avro | Streaming |
| ORC | .orc | Compatibilidade Hive |`
      },
      practicalTips: [
        {
          es: 'ğŸ“Š Siempre que puedas, convertÃ­ tus datos a Parquet o Delta. Son mucho mÃ¡s eficientes.',
          en: 'ğŸ“Š Whenever possible, convert your data to Parquet or Delta. They are much more efficient.',
          pt: 'ğŸ“Š Sempre que puder, converta seus dados para Parquet ou Delta. SÃ£o muito mais eficientes.'
        },
        {
          es: 'ğŸ’¡ El lÃ­mite de upload en Community Edition es ~2GB por archivo.',
          en: 'ğŸ’¡ The upload limit in Community Edition is ~2GB per file.',
          pt: 'ğŸ’¡ O limite de upload no Community Edition Ã© ~2GB por arquivo.'
        }
      ],
      externalLinks: [
        {
          title: 'Importing Data Documentation',
          url: 'https://docs.databricks.com/data/data.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Pudiste subir un archivo CSV propio y leerlo como DataFrame?',
        en: 'âœ… Were you able to upload your own CSV file and read it as a DataFrame?',
        pt: 'âœ… VocÃª conseguiu fazer upload de um arquivo CSV prÃ³prio e lÃª-lo como DataFrame?'
      },
      xpReward: 20,
      estimatedMinutes: 20
    },
    // PASO 1.8
    {
      id: 'db-1-8',
      title: {
        es: 'Visualizaciones Nativas en Databricks',
        en: 'Native Visualizations in Databricks',
        pt: 'VisualizaÃ§Ãµes Nativas no Databricks'
      },
      description: {
        es: 'Databricks tiene visualizaciones built-in increÃ­bles. AprendÃ© a usarlas.',
        en: 'Databricks has amazing built-in visualizations. Learn to use them.',
        pt: 'O Databricks tem visualizaÃ§Ãµes built-in incrÃ­veis. Aprenda a usÃ¡-las.'
      },
      theory: {
        es: `## Visualizaciones en Databricks

Databricks tiene un sistema de visualizaciÃ³n muy poderoso integrado.

### Usar display() en vez de show()

\`\`\`python
# âŒ Esto muestra texto plano
df.show()

# âœ… Esto muestra una tabla interactiva con opciones de visualizaciÃ³n
display(df)
\`\`\`

### Tipos de VisualizaciÃ³n Disponibles:

DespuÃ©s de ejecutar \`display(df)\`, podÃ©s hacer click en el Ã­cono de grÃ¡fico para:

| Tipo | Uso |
|------|-----|
| ğŸ“Š Bar Chart | Comparar categorÃ­as |
| ğŸ“ˆ Line Chart | Tendencias temporales |
| ğŸ¥§ Pie Chart | Proporciones |
| ğŸ“‰ Area Chart | VolÃºmenes acumulados |
| ğŸ—ºï¸ Map | Datos geogrÃ¡ficos |
| ğŸ“‹ Pivot Table | AnÃ¡lisis multidimensional |
| ğŸ¯ Scatter Plot | Correlaciones |

### Configurar VisualizaciÃ³n:

1. Ejecutar \`display(df)\`
2. Click en **+** al lado de "Table"
3. Elegir tipo de grÃ¡fico
4. Configurar:
   - Keys (eje X)
   - Values (eje Y)
   - Series groupings
   - Aggregations

### Guardar VisualizaciÃ³n:

Las visualizaciones se guardan con el notebook. PodÃ©s:
- Renombrarlas
- Tener mÃºltiples por celda
- Exportarlas como imagen`,
        en: `## Visualizations in Databricks

Databricks has a very powerful integrated visualization system.

### Use display() instead of show()

\`\`\`python
# âŒ This shows plain text
df.show()

# âœ… This shows an interactive table with visualization options
display(df)
\`\`\`

### Available Visualization Types:

After running \`display(df)\`, you can click the chart icon for:

| Type | Use |
|------|-----|
| ğŸ“Š Bar Chart | Compare categories |
| ğŸ“ˆ Line Chart | Time trends |
| ğŸ¥§ Pie Chart | Proportions |
| ğŸ“‰ Area Chart | Cumulative volumes |
| ğŸ—ºï¸ Map | Geographic data |
| ğŸ“‹ Pivot Table | Multidimensional analysis |
| ğŸ¯ Scatter Plot | Correlations |

### Configure Visualization:

1. Run \`display(df)\`
2. Click **+** next to "Table"
3. Choose chart type
4. Configure:
   - Keys (X axis)
   - Values (Y axis)
   - Series groupings
   - Aggregations

### Save Visualization:

Visualizations are saved with the notebook. You can:
- Rename them
- Have multiple per cell
- Export as image`,
        pt: `## VisualizaÃ§Ãµes no Databricks

O Databricks tem um sistema de visualizaÃ§Ã£o muito poderoso integrado.

### Usar display() em vez de show()

\`\`\`python
# âŒ Isso mostra texto simples
df.show()

# âœ… Isso mostra uma tabela interativa com opÃ§Ãµes de visualizaÃ§Ã£o
display(df)
\`\`\`

### Tipos de VisualizaÃ§Ã£o DisponÃ­veis:

ApÃ³s executar \`display(df)\`, vocÃª pode clicar no Ã­cone de grÃ¡fico para:

| Tipo | Uso |
|------|-----|
| ğŸ“Š Bar Chart | Comparar categorias |
| ğŸ“ˆ Line Chart | TendÃªncias temporais |
| ğŸ¥§ Pie Chart | ProporÃ§Ãµes |
| ğŸ“‰ Area Chart | Volumes acumulados |
| ğŸ—ºï¸ Map | Dados geogrÃ¡ficos |
| ğŸ“‹ Pivot Table | AnÃ¡lise multidimensional |
| ğŸ¯ Scatter Plot | CorrelaÃ§Ãµes |

### Configurar VisualizaÃ§Ã£o:

1. Executar \`display(df)\`
2. Clicar em **+** ao lado de "Table"
3. Escolher tipo de grÃ¡fico
4. Configurar:
   - Keys (eixo X)
   - Values (eixo Y)
   - Series groupings
   - Aggregations

### Salvar VisualizaÃ§Ã£o:

As visualizaÃ§Ãµes sÃ£o salvas com o notebook. VocÃª pode:
- RenomeÃ¡-las
- Ter mÃºltiplas por cÃ©lula
- ExportÃ¡-las como imagem`
      },
      codeExample: {
        language: 'python',
        code: `# Cargar datos de ejemplo para visualizar
df = spark.read.csv(
    "/databricks-datasets/samples/population-vs-price/data_geo.csv",
    header=True,
    inferSchema=True
)

# Ver con display (interactivo)
display(df)

# Tip: DespuÃ©s de ejecutar, click en el Ã­cono de grÃ¡fico
# para crear visualizaciones sin cÃ³digo

# TambiÃ©n podÃ©s agregar tÃ­tulos con displayHTML
displayHTML("<h2>ğŸ™ï¸ AnÃ¡lisis de Ciudades</h2>")
display(df)`,
        explanation: {
          es: 'UsÃ¡ display() para ver tablas interactivas. Luego podÃ©s crear grÃ¡ficos con clicks.',
          en: 'Use display() to see interactive tables. Then you can create charts with clicks.',
          pt: 'Use display() para ver tabelas interativas. Depois vocÃª pode criar grÃ¡ficos com cliques.'
        }
      },
      practicalTips: [
        {
          es: 'ğŸ¨ Databricks guarda las visualizaciones automÃ¡ticamente. No perdÃ©s tu trabajo.',
          en: 'ğŸ¨ Databricks saves visualizations automatically. You won\'t lose your work.',
          pt: 'ğŸ¨ O Databricks salva as visualizaÃ§Ãµes automaticamente. VocÃª nÃ£o perde seu trabalho.'
        },
        {
          es: 'ğŸ’¡ UsÃ¡ displayHTML() para agregar tÃ­tulos y formato entre visualizaciones.',
          en: 'ğŸ’¡ Use displayHTML() to add titles and formatting between visualizations.',
          pt: 'ğŸ’¡ Use displayHTML() para adicionar tÃ­tulos e formataÃ§Ã£o entre visualizaÃ§Ãµes.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Visualizations',
          url: 'https://docs.databricks.com/visualizations/index.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Creaste al menos un grÃ¡fico de barras y uno de lÃ­neas usando display()?',
        en: 'âœ… Did you create at least one bar chart and one line chart using display()?',
        pt: 'âœ… VocÃª criou pelo menos um grÃ¡fico de barras e um de linhas usando display()?'
      },
      xpReward: 20,
      estimatedMinutes: 20
    },
    // PASO 1.9
    {
      id: 'db-1-9',
      title: {
        es: 'dbutils: La Navaja Suiza de Databricks',
        en: 'dbutils: The Swiss Army Knife of Databricks',
        pt: 'dbutils: O Canivete SuÃ­Ã§o do Databricks'
      },
      description: {
        es: 'dbutils es una utilidad poderosa para trabajar con archivos, secrets, widgets y mÃ¡s.',
        en: 'dbutils is a powerful utility for working with files, secrets, widgets and more.',
        pt: 'dbutils Ã© um utilitÃ¡rio poderoso para trabalhar com arquivos, secrets, widgets e mais.'
      },
      theory: {
        es: `## dbutils - Databricks Utilities

\`dbutils\` es un objeto que viene pre-cargado en Databricks con utilidades muy Ãºtiles.

### MÃ³dulos principales:

### 1. dbutils.fs - Sistema de archivos
\`\`\`python
# Listar archivos
dbutils.fs.ls("/path/")

# Copiar
dbutils.fs.cp("source", "dest")

# Mover
dbutils.fs.mv("source", "dest")

# Eliminar
dbutils.fs.rm("path", recurse=True)

# Crear carpeta
dbutils.fs.mkdirs("path")

# Ver contenido
dbutils.fs.head("path", maxBytes=1000)
\`\`\`

### 2. dbutils.widgets - ParÃ¡metros interactivos
\`\`\`python
# Crear widget de texto
dbutils.widgets.text("nombre", "default", "Etiqueta")

# Crear dropdown
dbutils.widgets.dropdown("pais", "AR", ["AR", "MX", "CO"])

# Obtener valor
valor = dbutils.widgets.get("nombre")

# Eliminar
dbutils.widgets.remove("nombre")
dbutils.widgets.removeAll()
\`\`\`

### 3. dbutils.secrets - Credenciales seguras
\`\`\`python
# Obtener secret (configurado en Scope)
password = dbutils.secrets.get(scope="mi-scope", key="db-password")
\`\`\`

### 4. dbutils.notebook - Ejecutar otros notebooks
\`\`\`python
# Ejecutar notebook y obtener resultado
result = dbutils.notebook.run("path/notebook", timeout_seconds=60)

# Salir del notebook con valor
dbutils.notebook.exit("valor_de_retorno")
\`\`\`

### 5. dbutils.library - Instalar bibliotecas
\`\`\`python
# Instalar desde PyPI
dbutils.library.installPyPI("pandas")

# Reiniciar Python
dbutils.library.restartPython()
\`\`\`

### Ver ayuda:
\`\`\`python
dbutils.help()
dbutils.fs.help()
\`\`\``,
        en: `## dbutils - Databricks Utilities

\`dbutils\` is a pre-loaded object in Databricks with very useful utilities.

### Main modules:

### 1. dbutils.fs - File system
\`\`\`python
# List files
dbutils.fs.ls("/path/")

# Copy
dbutils.fs.cp("source", "dest")

# Move
dbutils.fs.mv("source", "dest")

# Delete
dbutils.fs.rm("path", recurse=True)

# Create folder
dbutils.fs.mkdirs("path")

# View contents
dbutils.fs.head("path", maxBytes=1000)
\`\`\`

### 2. dbutils.widgets - Interactive parameters
\`\`\`python
# Create text widget
dbutils.widgets.text("name", "default", "Label")

# Create dropdown
dbutils.widgets.dropdown("country", "US", ["US", "MX", "CO"])

# Get value
value = dbutils.widgets.get("name")

# Remove
dbutils.widgets.remove("name")
dbutils.widgets.removeAll()
\`\`\`

### 3. dbutils.secrets - Secure credentials
\`\`\`python
# Get secret (configured in Scope)
password = dbutils.secrets.get(scope="my-scope", key="db-password")
\`\`\`

### 4. dbutils.notebook - Run other notebooks
\`\`\`python
# Run notebook and get result
result = dbutils.notebook.run("path/notebook", timeout_seconds=60)

# Exit notebook with value
dbutils.notebook.exit("return_value")
\`\`\`

### 5. dbutils.library - Install libraries
\`\`\`python
# Install from PyPI
dbutils.library.installPyPI("pandas")

# Restart Python
dbutils.library.restartPython()
\`\`\`

### View help:
\`\`\`python
dbutils.help()
dbutils.fs.help()
\`\`\``,
        pt: `## dbutils - Databricks Utilities

\`dbutils\` Ã© um objeto prÃ©-carregado no Databricks com utilidades muito Ãºteis.

### MÃ³dulos principais:

### 1. dbutils.fs - Sistema de arquivos
\`\`\`python
# Listar arquivos
dbutils.fs.ls("/path/")

# Copiar
dbutils.fs.cp("source", "dest")

# Mover
dbutils.fs.mv("source", "dest")

# Excluir
dbutils.fs.rm("path", recurse=True)

# Criar pasta
dbutils.fs.mkdirs("path")

# Ver conteÃºdo
dbutils.fs.head("path", maxBytes=1000)
\`\`\`

### 2. dbutils.widgets - ParÃ¢metros interativos
\`\`\`python
# Criar widget de texto
dbutils.widgets.text("nome", "default", "RÃ³tulo")

# Criar dropdown
dbutils.widgets.dropdown("pais", "BR", ["BR", "MX", "CO"])

# Obter valor
valor = dbutils.widgets.get("nome")

# Remover
dbutils.widgets.remove("nome")
dbutils.widgets.removeAll()
\`\`\`

### 3. dbutils.secrets - Credenciais seguras
\`\`\`python
# Obter secret (configurado no Scope)
password = dbutils.secrets.get(scope="meu-scope", key="db-password")
\`\`\`

### 4. dbutils.notebook - Executar outros notebooks
\`\`\`python
# Executar notebook e obter resultado
result = dbutils.notebook.run("path/notebook", timeout_seconds=60)

# Sair do notebook com valor
dbutils.notebook.exit("valor_de_retorno")
\`\`\`

### 5. dbutils.library - Instalar bibliotecas
\`\`\`python
# Instalar do PyPI
dbutils.library.installPyPI("pandas")

# Reiniciar Python
dbutils.library.restartPython()
\`\`\`

### Ver ajuda:
\`\`\`python
dbutils.help()
dbutils.fs.help()
\`\`\``
      },
      codeExample: {
        language: 'python',
        code: `# Ejemplos prÃ¡cticos de dbutils

# 1. Ver ayuda general
dbutils.help()

# 2. Listar datasets de ejemplo
display(dbutils.fs.ls("/databricks-datasets/"))

# 3. Crear un widget para filtrar datos
dbutils.widgets.dropdown(
    "dataset",
    "wine-quality",
    ["wine-quality", "nyctaxi", "COVID"],
    "Seleccionar Dataset"
)

# 4. Usar el valor del widget
selected = dbutils.widgets.get("dataset")
print(f"Dataset seleccionado: {selected}")

# 5. Mostrar archivos del dataset seleccionado
display(dbutils.fs.ls(f"/databricks-datasets/{selected}/"))`,
        explanation: {
          es: 'Los widgets permiten crear notebooks interactivos donde el usuario puede seleccionar parÃ¡metros sin tocar cÃ³digo.',
          en: 'Widgets allow you to create interactive notebooks where the user can select parameters without touching code.',
          pt: 'Os widgets permitem criar notebooks interativos onde o usuÃ¡rio pode selecionar parÃ¢metros sem tocar no cÃ³digo.'
        }
      },
      practicalTips: [
        {
          es: 'â­ Los widgets son ideales para crear notebooks que usen otras personas no tÃ©cnicas.',
          en: 'â­ Widgets are ideal for creating notebooks that non-technical people can use.',
          pt: 'â­ Os widgets sÃ£o ideais para criar notebooks que pessoas nÃ£o tÃ©cnicas possam usar.'
        },
        {
          es: 'ğŸ”’ Nunca hardcodees passwords en el cÃ³digo. UsÃ¡ dbutils.secrets.',
          en: 'ğŸ”’ Never hardcode passwords in code. Use dbutils.secrets.',
          pt: 'ğŸ”’ Nunca coloque senhas fixas no cÃ³digo. Use dbutils.secrets.'
        }
      ],
      externalLinks: [
        {
          title: 'dbutils Reference',
          url: 'https://docs.databricks.com/dev-tools/databricks-utils.html',
          type: 'docs'
        },
        {
          title: 'Widgets Documentation',
          url: 'https://docs.databricks.com/notebooks/widgets.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Creaste un widget dropdown y lo usaste para filtrar datos?',
        en: 'âœ… Did you create a dropdown widget and use it to filter data?',
        pt: 'âœ… VocÃª criou um widget dropdown e o usou para filtrar dados?'
      },
      xpReward: 25,
      estimatedMinutes: 25
    },
    // PASO 1.10
    {
      id: 'db-1-10',
      title: {
        es: 'Proyecto Mini: Tu Primer Pipeline Completo',
        en: 'Mini Project: Your First Complete Pipeline',
        pt: 'Mini Projeto: Seu Primeiro Pipeline Completo'
      },
      description: {
        es: 'JuntÃ¡ todo lo aprendido en un mini proyecto: cargar datos, transformar, visualizar.',
        en: 'Put everything learned together in a mini project: load data, transform, visualize.',
        pt: 'Junte tudo o que aprendeu em um mini projeto: carregar dados, transformar, visualizar.'
      },
      theory: {
        es: `## Mini Proyecto: AnÃ¡lisis de Calidad de Vinos ğŸ·

Vamos a crear un notebook completo que:
1. Cargue datos del dataset wine-quality
2. Explore y limpie los datos
3. Cree visualizaciones
4. Guarde resultados

### Estructura del Notebook:

\`\`\`
ğŸ““ 01-Analisis-Vinos
â”œâ”€â”€ ğŸ“ Markdown: TÃ­tulo y descripciÃ³n
â”œâ”€â”€ ğŸ”§ Setup: Imports y configuraciÃ³n
â”œâ”€â”€ ğŸ“¥ Ingesta: Cargar datos
â”œâ”€â”€ ğŸ” ExploraciÃ³n: AnÃ¡lisis inicial
â”œâ”€â”€ ğŸ§¹ Limpieza: Transformaciones
â”œâ”€â”€ ğŸ“Š VisualizaciÃ³n: GrÃ¡ficos
â””â”€â”€ ğŸ’¾ Guardado: Persistir resultados
\`\`\`

### Checklist del Proyecto:

- [ ] Crear notebook con nombre descriptivo
- [ ] Documentar con Markdown
- [ ] Cargar dataset wine-quality
- [ ] Explorar schema y estadÃ­sticas
- [ ] Crear al menos 3 visualizaciones
- [ ] Guardar DataFrame limpio como tabla
- [ ] Agregar conclusiones`,
        en: `## Mini Project: Wine Quality Analysis ğŸ·

We're going to create a complete notebook that:
1. Loads data from wine-quality dataset
2. Explores and cleans the data
3. Creates visualizations
4. Saves results

### Notebook Structure:

\`\`\`
ğŸ““ 01-Wine-Analysis
â”œâ”€â”€ ğŸ“ Markdown: Title and description
â”œâ”€â”€ ğŸ”§ Setup: Imports and configuration
â”œâ”€â”€ ğŸ“¥ Ingestion: Load data
â”œâ”€â”€ ğŸ” Exploration: Initial analysis
â”œâ”€â”€ ğŸ§¹ Cleaning: Transformations
â”œâ”€â”€ ğŸ“Š Visualization: Charts
â””â”€â”€ ğŸ’¾ Saving: Persist results
\`\`\`

### Project Checklist:

- [ ] Create notebook with descriptive name
- [ ] Document with Markdown
- [ ] Load wine-quality dataset
- [ ] Explore schema and statistics
- [ ] Create at least 3 visualizations
- [ ] Save clean DataFrame as table
- [ ] Add conclusions`,
        pt: `## Mini Projeto: AnÃ¡lise de Qualidade de Vinhos ğŸ·

Vamos criar um notebook completo que:
1. Carregue dados do dataset wine-quality
2. Explore e limpe os dados
3. Crie visualizaÃ§Ãµes
4. Salve resultados

### Estrutura do Notebook:

\`\`\`
ğŸ““ 01-Analise-Vinhos
â”œâ”€â”€ ğŸ“ Markdown: TÃ­tulo e descriÃ§Ã£o
â”œâ”€â”€ ğŸ”§ Setup: Imports e configuraÃ§Ã£o
â”œâ”€â”€ ğŸ“¥ IngestÃ£o: Carregar dados
â”œâ”€â”€ ğŸ” ExploraÃ§Ã£o: AnÃ¡lise inicial
â”œâ”€â”€ ğŸ§¹ Limpeza: TransformaÃ§Ãµes
â”œâ”€â”€ ğŸ“Š VisualizaÃ§Ã£o: GrÃ¡ficos
â””â”€â”€ ğŸ’¾ Salvamento: Persistir resultados
\`\`\`

### Checklist do Projeto:

- [ ] Criar notebook com nome descritivo
- [ ] Documentar com Markdown
- [ ] Carregar dataset wine-quality
- [ ] Explorar schema e estatÃ­sticas
- [ ] Criar pelo menos 3 visualizaÃ§Ãµes
- [ ] Salvar DataFrame limpo como tabela
- [ ] Adicionar conclusÃµes`
      },
      codeExample: {
        language: 'python',
        code: `# ===========================================
# ğŸ· ANÃLISIS DE CALIDAD DE VINOS
# Proyecto de la Fase 1 - Databricks
# ===========================================

# %md
# # ğŸ· AnÃ¡lisis de Calidad de Vinos
# 
# **Objetivo:** Explorar el dataset de calidad de vinos y crear visualizaciones.
# 
# **Dataset:** /databricks-datasets/wine-quality/

# Celda 1: Setup
from pyspark.sql.functions import *

# Celda 2: Cargar datos
df_red = spark.read.csv(
    "/databricks-datasets/wine-quality/winequality-red.csv",
    header=True,
    inferSchema=True,
    sep=";"
)
df_red = df_red.withColumn("wine_type", lit("red"))

df_white = spark.read.csv(
    "/databricks-datasets/wine-quality/winequality-white.csv",
    header=True,
    inferSchema=True,
    sep=";"
)
df_white = df_white.withColumn("wine_type", lit("white"))

# Unir ambos datasets
df = df_red.union(df_white)
print(f"Total de vinos: {df.count()}")

# Celda 3: ExploraciÃ³n
df.printSchema()
display(df.describe())

# Celda 4: AnÃ¡lisis de calidad
# (Crear grÃ¡fico de barras despuÃ©s de ejecutar)
quality_dist = df.groupBy("quality", "wine_type").count()
display(quality_dist)

# Celda 5: CorrelaciÃ³n alcohol vs calidad
# (Crear scatter plot)
display(df.select("alcohol", "quality", "wine_type"))

# Celda 6: Guardar como tabla
df.write.mode("overwrite").saveAsTable("wines_analysis")
print("âœ… Tabla guardada exitosamente!")`,
        explanation: {
          es: 'Este proyecto integra todo lo aprendido: cargar datos, explorar, transformar, visualizar y guardar.',
          en: 'This project integrates everything learned: loading data, exploring, transforming, visualizing and saving.',
          pt: 'Este projeto integra tudo o que foi aprendido: carregar dados, explorar, transformar, visualizar e salvar.'
        }
      },
      practicalTips: [
        {
          es: 'ğŸ““ Este notebook puede ser parte de tu portfolio. Hacelo prolijo!',
          en: 'ğŸ““ This notebook can be part of your portfolio. Make it neat!',
          pt: 'ğŸ““ Este notebook pode fazer parte do seu portfÃ³lio. FaÃ§a-o organizado!'
        },
        {
          es: 'ğŸ’¡ AgregÃ¡ tus propias conclusiones al final del notebook. Muestra pensamiento analÃ­tico.',
          en: 'ğŸ’¡ Add your own conclusions at the end of the notebook. Show analytical thinking.',
          pt: 'ğŸ’¡ Adicione suas prÃ³prias conclusÃµes no final do notebook. Mostre pensamento analÃ­tico.'
        }
      ],
      externalLinks: [
        {
          title: 'Wine Quality Dataset Info',
          url: 'https://archive.ics.uci.edu/ml/datasets/wine+quality',
          type: 'article'
        }
      ],
      checkpoint: {
        es: 'ğŸ† Â¿Completaste el notebook con al menos 3 visualizaciones y guardaste la tabla?',
        en: 'ğŸ† Did you complete the notebook with at least 3 visualizations and save the table?',
        pt: 'ğŸ† VocÃª completou o notebook com pelo menos 3 visualizaÃ§Ãµes e salvou a tabela?'
      },
      xpReward: 50,
      estimatedMinutes: 45
    }
  ]
};


