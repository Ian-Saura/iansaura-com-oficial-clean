/**
 * FASE 1: Setup & Fundamentos de Databricks
 * 10 pasos detallados para comenzar con Databricks Free Edition (2026)
 * 
 * ACTUALIZADO: Enero 2026 - MigraciÃ³n de Community Edition a Free Edition
 * - Serverless compute (no mÃ¡s clusters manuales)
 * - Databricks Assistant incluido
 * - Unity Catalog bÃ¡sico disponible
 * - Solo Python y SQL (no R ni Scala)
 */

import { DatabricksPhase } from '../types';

export const PHASE_1_SETUP: DatabricksPhase = {
  id: 'db-phase-1',
  number: 1,
  title: {
    es: 'Setup & Fundamentos',
    en: 'Setup & Fundamentals',
    pt: 'Setup & Fundamentos'
  },
  subtitle: {
    es: 'Tu primer contacto con Databricks',
    en: 'Your first contact with Databricks',
    pt: 'Seu primeiro contato com Databricks'
  },
  description: {
    es: 'AprenderÃ¡s a crear tu cuenta gratuita en Databricks Free Edition, entender la arquitectura bÃ¡sica con serverless compute, y ejecutar tu primer notebook con la ayuda del Databricks Assistant. Esta fase sienta las bases para todo lo que viene.',
    en: 'You will learn to create your free account on Databricks Free Edition, understand the basic architecture with serverless compute, and run your first notebook with the help of Databricks Assistant. This phase lays the foundation for everything that follows.',
    pt: 'VocÃª aprenderÃ¡ a criar sua conta gratuita no Databricks Free Edition, entender a arquitetura bÃ¡sica com serverless compute, e executar seu primeiro notebook com a ajuda do Databricks Assistant. Esta fase estabelece as bases para tudo o que vem a seguir.'
  },
  icon: 'ğŸš€',
  color: 'emerald',
  estimatedDays: '3-4 dÃ­as',
  steps: [
    // PASO 1.1
    {
      id: 'db-1-1',
      title: {
        es: 'Â¿QuÃ© es Databricks y por quÃ© aprenderlo?',
        en: 'What is Databricks and why learn it?',
        pt: 'O que Ã© Databricks e por que aprender?'
      },
      description: {
        es: 'EntendÃ© quÃ© problema resuelve Databricks y por quÃ© es la plataforma mÃ¡s demandada del mercado.',
        en: 'Understand what problem Databricks solves and why it is the most in-demand platform in the market.',
        pt: 'Entenda qual problema o Databricks resolve e por que Ã© a plataforma mais demandada do mercado.'
      },
      theory: {
        es: `## Â¿QuÃ© es Databricks?

Databricks es una **plataforma unificada de datos y AI** fundada por los creadores de Apache Spark. Combina:

- **Data Engineering**: Pipelines de datos escalables
- **Data Science**: Notebooks colaborativos
- **Machine Learning**: MLflow integrado
- **Data Warehousing**: SQL Analytics

### El Problema que Resuelve

Antes de Databricks, las empresas tenÃ­an:
- Data Lakes desordenados ("data swamps")
- MÃºltiples herramientas desconectadas
- Silos entre equipos de datos
- Dificultad para hacer ML en producciÃ³n

### Databricks Lakehouse

Databricks introdujo el concepto de **Lakehouse**, que combina:
- La flexibilidad de un Data Lake (almacena cualquier dato)
- La confiabilidad de un Data Warehouse (ACID, schema enforcement)

### Â¿Por quÃ© es tan demandado?

1. **Empresas top lo usan**: Shell, Comcast, Regeneron, HSBC
2. **Salarios altos**: DE con Databricks ganan 20-30% mÃ¡s
3. **Crecimiento**: #1 en Gartner Magic Quadrant
4. **Ecosistema**: Spark + Delta Lake + MLflow + Unity Catalog`,
        en: `## What is Databricks?

Databricks is a **unified data and AI platform** founded by the creators of Apache Spark. It combines:

- **Data Engineering**: Scalable data pipelines
- **Data Science**: Collaborative notebooks
- **Machine Learning**: Integrated MLflow
- **Data Warehousing**: SQL Analytics

### The Problem it Solves

Before Databricks, companies had:
- Messy Data Lakes ("data swamps")
- Multiple disconnected tools
- Silos between data teams
- Difficulty putting ML in production

### Databricks Lakehouse

Databricks introduced the **Lakehouse** concept, combining:
- The flexibility of a Data Lake (stores any data)
- The reliability of a Data Warehouse (ACID, schema enforcement)

### Why is it so in-demand?

1. **Top companies use it**: Shell, Comcast, Regeneron, HSBC
2. **High salaries**: DEs with Databricks earn 20-30% more
3. **Growth**: #1 in Gartner Magic Quadrant
4. **Ecosystem**: Spark + Delta Lake + MLflow + Unity Catalog`,
        pt: `## O que Ã© Databricks?

Databricks Ã© uma **plataforma unificada de dados e IA** fundada pelos criadores do Apache Spark. Combina:

- **Data Engineering**: Pipelines de dados escalÃ¡veis
- **Data Science**: Notebooks colaborativos
- **Machine Learning**: MLflow integrado
- **Data Warehousing**: SQL Analytics

### O Problema que Resolve

Antes do Databricks, as empresas tinham:
- Data Lakes desorganizados ("data swamps")
- MÃºltiplas ferramentas desconectadas
- Silos entre equipes de dados
- Dificuldade para colocar ML em produÃ§Ã£o

### Databricks Lakehouse

Databricks introduziu o conceito de **Lakehouse**, que combina:
- A flexibilidade de um Data Lake (armazena qualquer dado)
- A confiabilidade de um Data Warehouse (ACID, schema enforcement)

### Por que Ã© tÃ£o demandado?

1. **Empresas top usam**: Shell, Comcast, Regeneron, HSBC
2. **SalÃ¡rios altos**: DEs com Databricks ganham 20-30% mais
3. **Crescimento**: #1 no Gartner Magic Quadrant
4. **Ecossistema**: Spark + Delta Lake + MLflow + Unity Catalog`
      },
      practicalTips: [
        {
          es: 'ğŸ’¡ Databricks Free Edition es GRATIS y suficiente para aprender todo lo bÃ¡sico e intermedio. Incluye serverless compute, Delta Lake, Unity Catalog bÃ¡sico y Databricks Assistant.',
          en: 'ğŸ’¡ Databricks Free Edition is FREE and sufficient to learn all basic and intermediate concepts. Includes serverless compute, Delta Lake, basic Unity Catalog and Databricks Assistant.',
          pt: 'ğŸ’¡ Databricks Free Edition Ã© GRATUITO e suficiente para aprender todos os conceitos bÃ¡sicos e intermediÃ¡rios. Inclui serverless compute, Delta Lake, Unity Catalog bÃ¡sico e Databricks Assistant.'
        },
        {
          es: 'ğŸ¯ AgregÃ¡ "Databricks" a tu LinkedIn ahora mismo. Es un keyword que atrae recruiters.',
          en: 'ğŸ¯ Add "Databricks" to your LinkedIn right now. It\'s a keyword that attracts recruiters.',
          pt: 'ğŸ¯ Adicione "Databricks" ao seu LinkedIn agora mesmo. Ã‰ uma keyword que atrai recrutadores.'
        },
        {
          es: 'ğŸ¤– El Databricks Assistant (IA) estÃ¡ incluido gratis y te ayuda a escribir cÃ³digo, explicar errores y optimizar queries.',
          en: 'ğŸ¤– Databricks Assistant (AI) is included for free and helps you write code, explain errors and optimize queries.',
          pt: 'ğŸ¤– O Databricks Assistant (IA) estÃ¡ incluÃ­do gratuitamente e ajuda a escrever cÃ³digo, explicar erros e otimizar queries.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Official Website',
          url: 'https://www.databricks.com/',
          type: 'docs'
        },
        {
          title: 'What is a Data Lakehouse? (Official Docs)',
          url: 'https://www.databricks.com/glossary/data-lakehouse',
          type: 'docs'
        },
        {
          title: 'Databricks Free Edition',
          url: 'https://www.databricks.com/try-databricks-free',
          type: 'tool'
        }
      ],
      checkpoint: {
        es: 'ğŸ¤” Â¿PodÃ©s explicar en una oraciÃ³n quÃ© es un Lakehouse y por quÃ© es mejor que un Data Lake tradicional?',
        en: 'ğŸ¤” Can you explain in one sentence what a Lakehouse is and why it\'s better than a traditional Data Lake?',
        pt: 'ğŸ¤” VocÃª consegue explicar em uma frase o que Ã© um Lakehouse e por que Ã© melhor que um Data Lake tradicional?'
      },
      xpReward: 15,
      estimatedMinutes: 20
    },
    // PASO 1.2
    {
      id: 'db-1-2',
      title: {
        es: 'Crear cuenta en Databricks Free Edition',
        en: 'Create Databricks Free Edition account',
        pt: 'Criar conta no Databricks Free Edition'
      },
      description: {
        es: 'Registrate gratis y configurÃ¡ tu primer workspace de Databricks con serverless compute.',
        en: 'Register for free and set up your first Databricks workspace with serverless compute.',
        pt: 'Registre-se gratuitamente e configure seu primeiro workspace do Databricks com serverless compute.'
      },
      theory: {
        es: `## Databricks Free Edition (2026)

**Free Edition** es la versiÃ³n gratuita de Databricks, perfecta para aprender. Reemplaza a la antigua Community Edition con mejoras significativas:

### QuÃ© incluye (GRATIS):
- âœ… Workspace completo
- âœ… Notebooks ilimitados
- âœ… **Serverless compute** (sin crear clusters manualmente)
- âœ… Delta Lake completo
- âœ… **Unity Catalog bÃ¡sico** (governance)
- âœ… **Databricks Assistant** (IA para coding)
- âœ… Delta Live Tables (DLT)
- âœ… MLflow
- âœ… Datasets de ejemplo
- âœ… Acceso a Databricks Academy (cursos gratis)

### QuÃ© NO incluye (requiere plan pago):
- âŒ R y Scala (solo Python y SQL)
- âŒ Clusters personalizados/GPU
- âŒ MÃºltiples workspaces
- âŒ SSO/SCIM empresarial
- âŒ Acceso a internet sin restricciones
- âŒ Soporte enterprise

### Paso a paso para crear la cuenta:

1. **Ir a**: https://www.databricks.com/try-databricks-free
2. **Click en "Start for free"**
3. **Completar el formulario**:
   - Email (usÃ¡ uno profesional)
   - Nombre completo
   - PaÃ­s
4. **Verificar email**
5. **Crear password**
6. **Â¡Listo!** Ya tenÃ©s tu workspace con serverless compute

### Diferencias clave vs Community Edition (antigua):

| Aspecto | Community Edition (vieja) | Free Edition (nueva) |
|---------|--------------------------|---------------------|
| Compute | Cluster 15GB manual | Serverless automÃ¡tico |
| Unity Catalog | No disponible | Disponible (bÃ¡sico) |
| DLT | No disponible | Disponible |
| AI Assistant | No disponible | Incluido |
| Lenguajes | Python, SQL, R, Scala | Solo Python y SQL |`,
        en: `## Databricks Free Edition (2026)

**Free Edition** is the free version of Databricks, perfect for learning. It replaces the old Community Edition with significant improvements:

### What's included (FREE):
- âœ… Full workspace
- âœ… Unlimited notebooks
- âœ… **Serverless compute** (no manual cluster creation)
- âœ… Complete Delta Lake
- âœ… **Basic Unity Catalog** (governance)
- âœ… **Databricks Assistant** (AI for coding)
- âœ… Delta Live Tables (DLT)
- âœ… MLflow
- âœ… Sample datasets
- âœ… Databricks Academy access (free courses)

### What's NOT included (requires paid plan):
- âŒ R and Scala (Python and SQL only)
- âŒ Custom clusters/GPU
- âŒ Multiple workspaces
- âŒ Enterprise SSO/SCIM
- âŒ Unrestricted internet access
- âŒ Enterprise support

### Step by step to create account:

1. **Go to**: https://www.databricks.com/try-databricks-free
2. **Click "Start for free"**
3. **Fill the form**:
   - Email (use a professional one)
   - Full name
   - Country
4. **Verify email**
5. **Create password**
6. **Done!** You have your workspace with serverless compute

### Key differences vs Community Edition (old):

| Aspect | Community Edition (old) | Free Edition (new) |
|--------|------------------------|-------------------|
| Compute | 15GB manual cluster | Automatic serverless |
| Unity Catalog | Not available | Available (basic) |
| DLT | Not available | Available |
| AI Assistant | Not available | Included |
| Languages | Python, SQL, R, Scala | Python and SQL only |`,
        pt: `## Databricks Free Edition (2026)

**Free Edition** Ã© a versÃ£o gratuita do Databricks, perfeita para aprender. Substitui o antigo Community Edition com melhorias significativas:

### O que inclui (GRÃTIS):
- âœ… Workspace completo
- âœ… Notebooks ilimitados
- âœ… **Serverless compute** (sem criar clusters manualmente)
- âœ… Delta Lake completo
- âœ… **Unity Catalog bÃ¡sico** (governance)
- âœ… **Databricks Assistant** (IA para coding)
- âœ… Delta Live Tables (DLT)
- âœ… MLflow
- âœ… Datasets de exemplo
- âœ… Acesso ao Databricks Academy (cursos grÃ¡tis)

### O que NÃƒO inclui (requer plano pago):
- âŒ R e Scala (apenas Python e SQL)
- âŒ Clusters personalizados/GPU
- âŒ MÃºltiplos workspaces
- âŒ SSO/SCIM empresarial
- âŒ Acesso Ã  internet sem restriÃ§Ãµes
- âŒ Suporte enterprise

### Passo a passo para criar conta:

1. **Ir para**: https://www.databricks.com/try-databricks-free
2. **Clicar em "Start for free"**
3. **Preencher o formulÃ¡rio**:
   - Email (use um profissional)
   - Nome completo
   - PaÃ­s
4. **Verificar email**
5. **Criar senha**
6. **Pronto!** VocÃª tem seu workspace com serverless compute

### DiferenÃ§as chave vs Community Edition (antigo):

| Aspecto | Community Edition (antigo) | Free Edition (novo) |
|---------|---------------------------|-------------------|
| Compute | Cluster 15GB manual | Serverless automÃ¡tico |
| Unity Catalog | NÃ£o disponÃ­vel | DisponÃ­vel (bÃ¡sico) |
| DLT | NÃ£o disponÃ­vel | DisponÃ­vel |
| AI Assistant | NÃ£o disponÃ­vel | IncluÃ­do |
| Linguagens | Python, SQL, R, Scala | Apenas Python e SQL |`
      },
      practicalTips: [
        {
          es: 'âš ï¸ UsÃ¡ un email que revises frecuentemente. Databricks envÃ­a notificaciones importantes.',
          en: 'âš ï¸ Use an email you check frequently. Databricks sends important notifications.',
          pt: 'âš ï¸ Use um email que vocÃª verifica frequentemente. Databricks envia notificaÃ§Ãµes importantes.'
        },
        {
          es: 'ğŸ’¡ GuardÃ¡ tus credenciales en un password manager. Las vas a necesitar seguido.',
          en: 'ğŸ’¡ Save your credentials in a password manager. You\'ll need them often.',
          pt: 'ğŸ’¡ Salve suas credenciais em um gerenciador de senhas. VocÃª vai precisar delas frequentemente.'
        },
        {
          es: 'ğŸš€ No necesitÃ¡s crear un cluster - el serverless compute se activa automÃ¡ticamente cuando ejecutÃ¡s cÃ³digo.',
          en: 'ğŸš€ You don\'t need to create a cluster - serverless compute activates automatically when you run code.',
          pt: 'ğŸš€ VocÃª nÃ£o precisa criar um cluster - o serverless compute ativa automaticamente quando vocÃª executa cÃ³digo.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Free Edition Signup',
          url: 'https://www.databricks.com/try-databricks-free',
          type: 'tool'
        },
        {
          title: 'Free Edition Documentation',
          url: 'https://docs.databricks.com/en/getting-started/free-edition.html',
          type: 'docs'
        },
        {
          title: 'Free Edition Limitations',
          url: 'https://docs.databricks.com/en/getting-started/free-edition-limitations.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Ya tenÃ©s tu cuenta creada y pudiste entrar al workspace?',
        en: 'âœ… Do you have your account created and were you able to enter the workspace?',
        pt: 'âœ… VocÃª jÃ¡ tem sua conta criada e conseguiu entrar no workspace?'
      },
      xpReward: 20,
      estimatedMinutes: 15
    },
    // PASO 1.3
    {
      id: 'db-1-3',
      title: {
        es: 'Tour por el Workspace de Databricks',
        en: 'Tour of the Databricks Workspace',
        pt: 'Tour pelo Workspace do Databricks'
      },
      description: {
        es: 'ConocÃ© cada secciÃ³n del workspace y entendÃ© para quÃ© sirve cada una.',
        en: 'Get to know each section of the workspace and understand what each one is for.',
        pt: 'ConheÃ§a cada seÃ§Ã£o do workspace e entenda para que serve cada uma.'
      },
      theory: {
        es: `## El Workspace de Databricks Free Edition

Cuando entrÃ¡s a Databricks, ves una interfaz moderna con varios elementos. Vamos a recorrerlos:

### ğŸ  Home (Inicio)
Tu pÃ¡gina principal. Muestra:
- Notebooks recientes
- Accesos rÃ¡pidos a recursos
- Tutoriales y guÃ­as de inicio

### ğŸ“ Workspace
El "explorador de archivos" de Databricks:
- **Users/tu_usuario/**: Tu carpeta personal
- **Shared/**: Carpetas compartidas con el equipo
- **Repos/**: IntegraciÃ³n con Git

### ğŸ”§ Compute
En Free Edition, el compute es **serverless**:
- Se activa automÃ¡ticamente al ejecutar cÃ³digo
- No necesitÃ¡s crear ni gestionar clusters
- Los recursos se asignan segÃºn demanda

> ğŸ’¡ **Nota**: En planes pagos, aquÃ­ se crean clusters personalizados.

### ğŸ“Š Catalog (Unity Catalog)
Explora y gestiona tus datos:
- **Catalogs**: Contenedores de nivel superior
- **Schemas**: Agrupan tablas relacionadas
- **Tables/Views**: Tus datos

### ğŸ”„ Workflows (Jobs)
AutomatizaciÃ³n y scheduling:
- Crear Jobs
- Ver ejecuciones
- Configurar triggers

### ğŸ¤– Databricks Assistant
El asistente de IA integrado:
- Ayuda a escribir cÃ³digo
- Explica errores
- Optimiza queries
- Responde preguntas sobre Databricks

### ğŸ§ª Machine Learning
Herramientas de ML:
- Experiments (MLflow)
- Models (Registry)

### âš™ï¸ Settings
ConfiguraciÃ³n de tu cuenta y workspace

### NavegaciÃ³n por teclado:
- \`Ctrl + Shift + P\` o \`Cmd + Shift + P\`: Command palette
- \`Ctrl + Alt + N\`: Nuevo notebook`,
        en: `## The Databricks Free Edition Workspace

When you enter Databricks, you see a modern interface with several elements. Let's go through them:

### ğŸ  Home
Your main page. Shows:
- Recent notebooks
- Quick access to resources
- Tutorials and getting started guides

### ğŸ“ Workspace
Databricks "file explorer":
- **Users/your_user/**: Your personal folder
- **Shared/**: Folders shared with team
- **Repos/**: Git integration

### ğŸ”§ Compute
In Free Edition, compute is **serverless**:
- Activates automatically when you run code
- No need to create or manage clusters
- Resources allocated on demand

> ğŸ’¡ **Note**: In paid plans, custom clusters are created here.

### ğŸ“Š Catalog (Unity Catalog)
Explore and manage your data:
- **Catalogs**: Top-level containers
- **Schemas**: Group related tables
- **Tables/Views**: Your data

### ğŸ”„ Workflows (Jobs)
Automation and scheduling:
- Create Jobs
- View executions
- Configure triggers

### ğŸ¤– Databricks Assistant
The integrated AI assistant:
- Helps write code
- Explains errors
- Optimizes queries
- Answers questions about Databricks

### ğŸ§ª Machine Learning
ML tools:
- Experiments (MLflow)
- Models (Registry)

### âš™ï¸ Settings
Account and workspace configuration

### Keyboard navigation:
- \`Ctrl + Shift + P\` or \`Cmd + Shift + P\`: Command palette
- \`Ctrl + Alt + N\`: New notebook`,
        pt: `## O Workspace do Databricks Free Edition

Quando vocÃª entra no Databricks, vÃª uma interface moderna com vÃ¡rios elementos. Vamos percorrÃª-los:

### ğŸ  Home (InÃ­cio)
Sua pÃ¡gina principal. Mostra:
- Notebooks recentes
- Acessos rÃ¡pidos a recursos
- Tutoriais e guias de inÃ­cio

### ğŸ“ Workspace
O "explorador de arquivos" do Databricks:
- **Users/seu_usuario/**: Sua pasta pessoal
- **Shared/**: Pastas compartilhadas com a equipe
- **Repos/**: IntegraÃ§Ã£o com Git

### ğŸ”§ Compute
No Free Edition, o compute Ã© **serverless**:
- Ativa automaticamente ao executar cÃ³digo
- NÃ£o precisa criar nem gerenciar clusters
- Recursos alocados sob demanda

> ğŸ’¡ **Nota**: Em planos pagos, clusters personalizados sÃ£o criados aqui.

### ğŸ“Š Catalog (Unity Catalog)
Explore e gerencie seus dados:
- **Catalogs**: Containers de nÃ­vel superior
- **Schemas**: Agrupam tabelas relacionadas
- **Tables/Views**: Seus dados

### ğŸ”„ Workflows (Jobs)
AutomaÃ§Ã£o e scheduling:
- Criar Jobs
- Ver execuÃ§Ãµes
- Configurar triggers

### ğŸ¤– Databricks Assistant
O assistente de IA integrado:
- Ajuda a escrever cÃ³digo
- Explica erros
- Otimiza queries
- Responde perguntas sobre Databricks

### ğŸ§ª Machine Learning
Ferramentas de ML:
- Experiments (MLflow)
- Models (Registry)

### âš™ï¸ Settings
ConfiguraÃ§Ã£o da sua conta e workspace

### NavegaÃ§Ã£o por teclado:
- \`Ctrl + Shift + P\` ou \`Cmd + Shift + P\`: Command palette
- \`Ctrl + Alt + N\`: Novo notebook`
      },
      practicalTips: [
        {
          es: 'ğŸ¯ ExplorÃ¡ cada secciÃ³n sin miedo. No podÃ©s romper nada en Free Edition.',
          en: 'ğŸ¯ Explore each section without fear. You can\'t break anything in Free Edition.',
          pt: 'ğŸ¯ Explore cada seÃ§Ã£o sem medo. VocÃª nÃ£o pode quebrar nada no Free Edition.'
        },
        {
          es: 'ğŸ’¡ CreÃ¡ una carpeta personal dentro de Workspace > Users > tu_usuario para organizar tus notebooks.',
          en: 'ğŸ’¡ Create a personal folder inside Workspace > Users > your_user to organize your notebooks.',
          pt: 'ğŸ’¡ Crie uma pasta pessoal dentro de Workspace > Users > seu_usuario para organizar seus notebooks.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Workspace Overview',
          url: 'https://docs.databricks.com/workspace/index.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'ğŸ¤” Â¿PodÃ©s encontrar dÃ³nde se crean los clusters y dÃ³nde se guardan los notebooks?',
        en: 'ğŸ¤” Can you find where clusters are created and where notebooks are saved?',
        pt: 'ğŸ¤” VocÃª consegue encontrar onde os clusters sÃ£o criados e onde os notebooks sÃ£o salvos?'
      },
      xpReward: 15,
      estimatedMinutes: 15
    },
    // PASO 1.4
    {
      id: 'db-1-4',
      title: {
        es: 'Entendiendo Serverless Compute',
        en: 'Understanding Serverless Compute',
        pt: 'Entendendo Serverless Compute'
      },
      description: {
        es: 'En Free Edition, Databricks usa serverless compute. No necesitÃ¡s crear clusters - el poder de cÃ³mputo se asigna automÃ¡ticamente.',
        en: 'In Free Edition, Databricks uses serverless compute. You don\'t need to create clusters - computing power is assigned automatically.',
        pt: 'No Free Edition, o Databricks usa serverless compute. VocÃª nÃ£o precisa criar clusters - o poder de computaÃ§Ã£o Ã© atribuÃ­do automaticamente.'
      },
      theory: {
        es: `## Serverless Compute en Free Edition

En Databricks Free Edition, el compute es **serverless**. Esto significa que NO necesitÃ¡s crear ni gestionar clusters manualmente.

### Â¿CÃ³mo funciona?

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TU NOTEBOOK                          â”‚
â”‚              EscribÃ­s y ejecutÃ¡s cÃ³digo                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Click "Run"
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SERVERLESS COMPUTE                        â”‚
â”‚  â€¢ Se activa automÃ¡ticamente                            â”‚
â”‚  â€¢ Recursos asignados segÃºn necesidad                   â”‚
â”‚  â€¢ Sin configuraciÃ³n manual                             â”‚
â”‚  â€¢ Sin tiempos de espera de inicio                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Ventajas del Serverless:
- âœ… **Sin esperas**: No hay que esperar 5 min para que inicie un cluster
- âœ… **Sin configuraciÃ³n**: No elegÃ­s RAM, cores, runtime
- âœ… **Sin gestiÃ³n**: No hay que apagar/encender nada
- âœ… **Costo eficiente**: Solo pagÃ¡s por lo que usÃ¡s (en planes pagos)

### ComparaciÃ³n con Clusters Tradicionales:

| Aspecto | Clusters (Plan Pago) | Serverless (Free Edition) |
|---------|---------------------|---------------------------|
| Setup | Manual (3-5 min) | AutomÃ¡tico (~segundos) |
| ConfiguraciÃ³n | RAM, cores, runtime | Ninguna |
| GestiÃ³n | Encender/apagar | AutomÃ¡tico |
| Lenguajes | Python, SQL, R, Scala | Python, SQL |
| Costo | Por hora de cluster | Por uso |

### Lo que ves en Compute:

En Free Edition, la secciÃ³n Compute muestra:
- Tu uso de serverless
- Historial de ejecuciones
- No hay botÃ³n "Create Cluster" (no lo necesitÃ¡s)

### Â¿CÃ³mo ejecutar cÃ³digo?

1. AbrÃ­ un notebook
2. EscribÃ­ cÃ³digo en una celda
3. Click en **Run** o presionÃ¡ **Shift + Enter**
4. Â¡El serverless compute se activa automÃ¡ticamente!

> ğŸ’¡ **Para usuarios avanzados**: En planes pagos, podÃ©s crear clusters personalizados con configuraciones especÃ­ficas (GPU, mÃ¡s RAM, etc.).`,
        en: `## Serverless Compute in Free Edition

In Databricks Free Edition, compute is **serverless**. This means you DON'T need to create or manage clusters manually.

### How does it work?

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    YOUR NOTEBOOK                        â”‚
â”‚              Write and execute code                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Click "Run"
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SERVERLESS COMPUTE                        â”‚
â”‚  â€¢ Activates automatically                              â”‚
â”‚  â€¢ Resources assigned as needed                         â”‚
â”‚  â€¢ No manual configuration                              â”‚
â”‚  â€¢ No startup wait times                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Serverless Advantages:
- âœ… **No waiting**: No need to wait 5 min for cluster startup
- âœ… **No configuration**: Don't choose RAM, cores, runtime
- âœ… **No management**: Nothing to turn on/off
- âœ… **Cost efficient**: Only pay for what you use (in paid plans)

### Comparison with Traditional Clusters:

| Aspect | Clusters (Paid Plan) | Serverless (Free Edition) |
|--------|---------------------|---------------------------|
| Setup | Manual (3-5 min) | Automatic (~seconds) |
| Configuration | RAM, cores, runtime | None |
| Management | Turn on/off | Automatic |
| Languages | Python, SQL, R, Scala | Python, SQL |
| Cost | Per cluster hour | Per use |

### What you see in Compute:

In Free Edition, the Compute section shows:
- Your serverless usage
- Execution history
- No "Create Cluster" button (you don't need it)

### How to run code?

1. Open a notebook
2. Write code in a cell
3. Click **Run** or press **Shift + Enter**
4. Serverless compute activates automatically!

> ğŸ’¡ **For advanced users**: In paid plans, you can create custom clusters with specific configurations (GPU, more RAM, etc.).`,
        pt: `## Serverless Compute no Free Edition

No Databricks Free Edition, o compute Ã© **serverless**. Isso significa que vocÃª NÃƒO precisa criar nem gerenciar clusters manualmente.

### Como funciona?

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEU NOTEBOOK                         â”‚
â”‚              Escreve e executa cÃ³digo                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ Click "Run"
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               SERVERLESS COMPUTE                        â”‚
â”‚  â€¢ Ativa automaticamente                                â”‚
â”‚  â€¢ Recursos atribuÃ­dos conforme necessidade             â”‚
â”‚  â€¢ Sem configuraÃ§Ã£o manual                              â”‚
â”‚  â€¢ Sem tempos de espera de inÃ­cio                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Vantagens do Serverless:
- âœ… **Sem esperas**: NÃ£o precisa esperar 5 min para iniciar cluster
- âœ… **Sem configuraÃ§Ã£o**: NÃ£o escolhe RAM, cores, runtime
- âœ… **Sem gestÃ£o**: Nada para ligar/desligar
- âœ… **Custo eficiente**: SÃ³ paga pelo que usa (em planos pagos)

### ComparaÃ§Ã£o com Clusters Tradicionais:

| Aspecto | Clusters (Plano Pago) | Serverless (Free Edition) |
|---------|----------------------|---------------------------|
| Setup | Manual (3-5 min) | AutomÃ¡tico (~segundos) |
| ConfiguraÃ§Ã£o | RAM, cores, runtime | Nenhuma |
| GestÃ£o | Ligar/desligar | AutomÃ¡tico |
| Linguagens | Python, SQL, R, Scala | Python, SQL |
| Custo | Por hora de cluster | Por uso |

### O que vocÃª vÃª em Compute:

No Free Edition, a seÃ§Ã£o Compute mostra:
- Seu uso de serverless
- HistÃ³rico de execuÃ§Ãµes
- Sem botÃ£o "Create Cluster" (vocÃª nÃ£o precisa)

### Como executar cÃ³digo?

1. Abra um notebook
2. Escreva cÃ³digo em uma cÃ©lula
3. Clique em **Run** ou pressione **Shift + Enter**
4. O serverless compute ativa automaticamente!

> ğŸ’¡ **Para usuÃ¡rios avanÃ§ados**: Em planos pagos, vocÃª pode criar clusters personalizados com configuraÃ§Ãµes especÃ­ficas (GPU, mais RAM, etc.).`
      },
      practicalTips: [
        {
          es: 'ğŸš€ En Free Edition, solo ejecutÃ¡ tu cÃ³digo. El compute se encarga solo.',
          en: 'ğŸš€ In Free Edition, just run your code. Compute takes care of itself.',
          pt: 'ğŸš€ No Free Edition, apenas execute seu cÃ³digo. O compute cuida de si mesmo.'
        },
        {
          es: 'ğŸ’¡ La primera celda puede tardar unos segundos mientras se inicializa el serverless. Las siguientes son instantÃ¡neas.',
          en: 'ğŸ’¡ The first cell may take a few seconds while serverless initializes. Following ones are instant.',
          pt: 'ğŸ’¡ A primeira cÃ©lula pode levar alguns segundos enquanto o serverless inicializa. As seguintes sÃ£o instantÃ¢neas.'
        },
        {
          es: 'ğŸ“š Entender clusters tradicionales sigue siendo importante para trabajos enterprise. Lo cubrimos en la Fase 2.',
          en: 'ğŸ“š Understanding traditional clusters is still important for enterprise jobs. We cover it in Phase 2.',
          pt: 'ğŸ“š Entender clusters tradicionais ainda Ã© importante para trabalhos enterprise. Cobrimos isso na Fase 2.'
        }
      ],
      externalLinks: [
        {
          title: 'Serverless Compute Documentation',
          url: 'https://docs.databricks.com/en/compute/serverless.html',
          type: 'docs'
        },
        {
          title: 'Free Edition Compute Limitations',
          url: 'https://docs.databricks.com/en/getting-started/free-edition-limitations.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿EntendÃ©s por quÃ© no necesitÃ¡s crear un cluster en Free Edition?',
        en: 'âœ… Do you understand why you don\'t need to create a cluster in Free Edition?',
        pt: 'âœ… VocÃª entende por que nÃ£o precisa criar um cluster no Free Edition?'
      },
      xpReward: 25,
      estimatedMinutes: 15
    },
    // PASO 1.5
    {
      id: 'db-1-5',
      title: {
        es: 'Tu primer Notebook en Databricks',
        en: 'Your first Notebook in Databricks',
        pt: 'Seu primeiro Notebook no Databricks'
      },
      description: {
        es: 'Los notebooks son donde escribÃ­s y ejecutÃ¡s cÃ³digo. CreÃ¡ tu primero y ejecutÃ¡ cÃ³digo Python y SQL.',
        en: 'Notebooks are where you write and run code. Create your first one and run Python and SQL code.',
        pt: 'Os notebooks sÃ£o onde vocÃª escreve e executa cÃ³digo. Crie seu primeiro e execute cÃ³digo Python e SQL.'
      },
      theory: {
        es: `## Notebooks en Databricks

Un notebook es un documento interactivo con **celdas** que pueden contener:
- CÃ³digo (Python, SQL)
- Markdown (documentaciÃ³n)
- Visualizaciones

> âš ï¸ **Free Edition**: Solo soporta Python y SQL. R y Scala requieren plan pago.

### Crear un Notebook:

1. Ir a **Workspace** > **Users** > tu usuario
2. Click derecho > **Create** > **Notebook**
3. **Name**: "01-Mi-Primer-Notebook"
4. **Default Language**: Python
5. Click **Create**

> ğŸ’¡ No necesitÃ¡s seleccionar un cluster - el serverless compute se activa automÃ¡ticamente cuando ejecutÃ¡s.

### AnatomÃ­a del Notebook:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ““ 01-Mi-Primer-Notebook              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [+ Code] [+ Text] [+ SQL]             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ # Celda 1 (Python)               â”‚  â”‚
â”‚  â”‚ print("Hola Databricks!")        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ %sql                             â”‚  â”‚
â”‚  â”‚ SELECT "Hola desde SQL"          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Magic Commands:

PodÃ©s cambiar el lenguaje de una celda individual:
- \`%python\` - Ejecutar Python
- \`%sql\` - Ejecutar SQL
- \`%md\` - Markdown (documentaciÃ³n)
- \`%sh\` - Shell commands
- \`%fs\` - Comandos de DBFS

> âš ï¸ \`%scala\` y \`%r\` NO estÃ¡n disponibles en Free Edition.

### Databricks Assistant (IA):

PodÃ©s usar el asistente de IA integrado:
- Click en el Ã­cono de IA o escribÃ­ \`/\` en una celda vacÃ­a
- Pedile que genere cÃ³digo, explique errores, o optimice queries

### Atajos de teclado:
- \`Shift + Enter\`: Ejecutar celda y avanzar
- \`Ctrl + Enter\`: Ejecutar celda sin avanzar
- \`Esc + A\`: Insertar celda arriba
- \`Esc + B\`: Insertar celda abajo`,
        en: `## Notebooks in Databricks

A notebook is an interactive document with **cells** that can contain:
- Code (Python, SQL)
- Markdown (documentation)
- Visualizations

> âš ï¸ **Free Edition**: Only supports Python and SQL. R and Scala require paid plan.

### Create a Notebook:

1. Go to **Workspace** > **Users** > your user
2. Right click > **Create** > **Notebook**
3. **Name**: "01-My-First-Notebook"
4. **Default Language**: Python
5. Click **Create**

> ğŸ’¡ You don't need to select a cluster - serverless compute activates automatically when you run.

### Notebook Anatomy:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ““ 01-My-First-Notebook               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [+ Code] [+ Text] [+ SQL]             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ # Cell 1 (Python)                â”‚  â”‚
â”‚  â”‚ print("Hello Databricks!")       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ %sql                             â”‚  â”‚
â”‚  â”‚ SELECT "Hello from SQL"          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Magic Commands:

You can change the language of an individual cell:
- \`%python\` - Run Python
- \`%sql\` - Run SQL
- \`%md\` - Markdown (documentation)
- \`%sh\` - Shell commands
- \`%fs\` - DBFS commands

> âš ï¸ \`%scala\` and \`%r\` are NOT available in Free Edition.

### Databricks Assistant (AI):

You can use the integrated AI assistant:
- Click the AI icon or type \`/\` in an empty cell
- Ask it to generate code, explain errors, or optimize queries

### Keyboard shortcuts:
- \`Shift + Enter\`: Run cell and advance
- \`Ctrl + Enter\`: Run cell without advancing
- \`Esc + A\`: Insert cell above
- \`Esc + B\`: Insert cell below`,
        pt: `## Notebooks no Databricks

Um notebook Ã© um documento interativo com **cÃ©lulas** que podem conter:
- CÃ³digo (Python, SQL)
- Markdown (documentaÃ§Ã£o)
- VisualizaÃ§Ãµes

> âš ï¸ **Free Edition**: Suporta apenas Python e SQL. R e Scala requerem plano pago.

### Criar um Notebook:

1. Ir para **Workspace** > **Users** > seu usuÃ¡rio
2. Clique direito > **Create** > **Notebook**
3. **Name**: "01-Meu-Primeiro-Notebook"
4. **Default Language**: Python
5. Clicar em **Create**

> ğŸ’¡ VocÃª nÃ£o precisa selecionar um cluster - o serverless compute ativa automaticamente quando vocÃª executa.

### Anatomia do Notebook:

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ““ 01-Meu-Primeiro-Notebook           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [+ Code] [+ Text] [+ SQL]             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ # CÃ©lula 1 (Python)              â”‚  â”‚
â”‚  â”‚ print("OlÃ¡ Databricks!")         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ %sql                             â”‚  â”‚
â”‚  â”‚ SELECT "OlÃ¡ do SQL"              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Magic Commands:

VocÃª pode mudar a linguagem de uma cÃ©lula individual:
- \`%python\` - Executar Python
- \`%sql\` - Executar SQL
- \`%md\` - Markdown (documentaÃ§Ã£o)
- \`%sh\` - Comandos Shell
- \`%fs\` - Comandos DBFS

> âš ï¸ \`%scala\` e \`%r\` NÃƒO estÃ£o disponÃ­veis no Free Edition.

### Databricks Assistant (IA):

VocÃª pode usar o assistente de IA integrado:
- Clique no Ã­cone de IA ou digite \`/\` em uma cÃ©lula vazia
- PeÃ§a para gerar cÃ³digo, explicar erros ou otimizar queries

### Atalhos de teclado:
- \`Shift + Enter\`: Executar cÃ©lula e avanÃ§ar
- \`Ctrl + Enter\`: Executar cÃ©lula sem avanÃ§ar
- \`Esc + A\`: Inserir cÃ©lula acima
- \`Esc + B\`: Inserir cÃ©lula abaixo`
      },
      codeExample: {
        language: 'python',
        code: `# Celda 1: Python bÃ¡sico
print("ğŸ‰ Â¡Hola Databricks!")

# Celda 2: Ver versiÃ³n de Spark
spark.version

# Celda 3: Crear un DataFrame simple
data = [("Ana", 25), ("Bob", 30), ("Carlos", 35)]
df = spark.createDataFrame(data, ["nombre", "edad"])
df.show()

# Celda 4: SQL (usar %sql al inicio de la celda)
# %sql
# SELECT * FROM VALUES 
#   ('Ana', 25), 
#   ('Bob', 30), 
#   ('Carlos', 35) 
# AS tabla(nombre, edad)`,
        explanation: {
          es: 'Este cÃ³digo muestra las operaciones bÃ¡sicas: imprimir, ver la versiÃ³n de Spark, crear un DataFrame, y ejecutar SQL.',
          en: 'This code shows basic operations: printing, checking Spark version, creating a DataFrame, and running SQL.',
          pt: 'Este cÃ³digo mostra operaÃ§Ãµes bÃ¡sicas: imprimir, verificar versÃ£o do Spark, criar um DataFrame e executar SQL.'
        }
      },
      practicalTips: [
        {
          es: 'ğŸ“ Siempre documentÃ¡ tu notebook con celdas Markdown (%md). Tu yo del futuro te lo agradecerÃ¡.',
          en: 'ğŸ“ Always document your notebook with Markdown cells (%md). Your future self will thank you.',
          pt: 'ğŸ“ Sempre documente seu notebook com cÃ©lulas Markdown (%md). Seu eu do futuro vai agradecer.'
        },
        {
          es: 'ğŸ’¡ UsÃ¡ nombres descriptivos para tus notebooks: "01-ExploraciÃ³n-Datos", "02-Limpieza", etc.',
          en: 'ğŸ’¡ Use descriptive names for your notebooks: "01-Data-Exploration", "02-Cleaning", etc.',
          pt: 'ğŸ’¡ Use nomes descritivos para seus notebooks: "01-ExploraÃ§Ã£o-Dados", "02-Limpeza", etc.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Notebooks Guide',
          url: 'https://docs.databricks.com/notebooks/index.html',
          type: 'docs'
        },
        {
          title: 'Notebook Keyboard Shortcuts',
          url: 'https://docs.databricks.com/notebooks/notebooks-manage.html#keyboard-shortcuts',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Pudiste ejecutar cÃ³digo Python y SQL en tu notebook?',
        en: 'âœ… Were you able to run Python and SQL code in your notebook?',
        pt: 'âœ… VocÃª conseguiu executar cÃ³digo Python e SQL no seu notebook?'
      },
      xpReward: 30,
      estimatedMinutes: 25
    },
    // PASO 1.6
    {
      id: 'db-1-6',
      title: {
        es: 'DBFS: El Sistema de Archivos de Databricks',
        en: 'DBFS: The Databricks File System',
        pt: 'DBFS: O Sistema de Arquivos do Databricks'
      },
      description: {
        es: 'DBFS es donde Databricks almacena datos. AprendÃ© a navegarlo y subir archivos.',
        en: 'DBFS is where Databricks stores data. Learn to navigate it and upload files.',
        pt: 'DBFS Ã© onde o Databricks armazena dados. Aprenda a navegar e fazer upload de arquivos.'
      },
      theory: {
        es: `## DBFS - Databricks File System

DBFS es una capa de abstracciÃ³n sobre el almacenamiento en la nube (S3, ADLS, GCS).

### Estructura de DBFS:

\`\`\`
dbfs:/
â”œâ”€â”€ FileStore/          # Archivos subidos por usuarios
â”‚   â”œâ”€â”€ tables/         # Datos de tablas
â”‚   â””â”€â”€ shared_uploads/ # Uploads compartidos
â”œâ”€â”€ databricks-datasets/ # Datasets de ejemplo (gratis!)
â”œâ”€â”€ user/               # Carpetas de usuarios
â””â”€â”€ mnt/                # Puntos de montaje (cloud storage)
\`\`\`

### Comandos para explorar DBFS:

\`\`\`python
# Listar contenido
%fs ls /

# Ver datasets de ejemplo
%fs ls /databricks-datasets/

# Crear carpeta
%fs mkdirs /FileStore/mi-proyecto/

# Copiar archivo
%fs cp /source/file.csv /destination/file.csv

# Ver contenido de archivo
%fs head /databricks-datasets/README.md
\`\`\`

### Desde Python:
\`\`\`python
# Usando dbutils
dbutils.fs.ls("/databricks-datasets/")

# Leer archivo como texto
dbutils.fs.head("/databricks-datasets/README.md")

# Copiar archivo
dbutils.fs.cp("source", "destination")
\`\`\`

### Datasets de Ejemplo Disponibles:

| Dataset | DescripciÃ³n | TamaÃ±o |
|---------|-------------|--------|
| /databricks-datasets/samples/population-vs-price/ | Datos de ciudades | ~10KB |
| /databricks-datasets/nyctaxi/ | Taxis de NYC | ~100MB |
| /databricks-datasets/wine-quality/ | Calidad de vinos | ~200KB |
| /databricks-datasets/COVID/ | Datos COVID-19 | ~50MB |
| /databricks-datasets/amazon/ | Reviews Amazon | ~1GB |`,
        en: `## DBFS - Databricks File System

DBFS is an abstraction layer over cloud storage (S3, ADLS, GCS).

### DBFS Structure:

\`\`\`
dbfs:/
â”œâ”€â”€ FileStore/          # User-uploaded files
â”‚   â”œâ”€â”€ tables/         # Table data
â”‚   â””â”€â”€ shared_uploads/ # Shared uploads
â”œâ”€â”€ databricks-datasets/ # Sample datasets (free!)
â”œâ”€â”€ user/               # User folders
â””â”€â”€ mnt/                # Mount points (cloud storage)
\`\`\`

### Commands to explore DBFS:

\`\`\`python
# List contents
%fs ls /

# View sample datasets
%fs ls /databricks-datasets/

# Create folder
%fs mkdirs /FileStore/my-project/

# Copy file
%fs cp /source/file.csv /destination/file.csv

# View file contents
%fs head /databricks-datasets/README.md
\`\`\`

### From Python:
\`\`\`python
# Using dbutils
dbutils.fs.ls("/databricks-datasets/")

# Read file as text
dbutils.fs.head("/databricks-datasets/README.md")

# Copy file
dbutils.fs.cp("source", "destination")
\`\`\`

### Available Sample Datasets:

| Dataset | Description | Size |
|---------|-------------|------|
| /databricks-datasets/samples/population-vs-price/ | City data | ~10KB |
| /databricks-datasets/nyctaxi/ | NYC Taxis | ~100MB |
| /databricks-datasets/wine-quality/ | Wine quality | ~200KB |
| /databricks-datasets/COVID/ | COVID-19 data | ~50MB |
| /databricks-datasets/amazon/ | Amazon reviews | ~1GB |`,
        pt: `## DBFS - Databricks File System

DBFS Ã© uma camada de abstraÃ§Ã£o sobre o armazenamento em nuvem (S3, ADLS, GCS).

### Estrutura do DBFS:

\`\`\`
dbfs:/
â”œâ”€â”€ FileStore/          # Arquivos enviados por usuÃ¡rios
â”‚   â”œâ”€â”€ tables/         # Dados de tabelas
â”‚   â””â”€â”€ shared_uploads/ # Uploads compartilhados
â”œâ”€â”€ databricks-datasets/ # Datasets de exemplo (grÃ¡tis!)
â”œâ”€â”€ user/               # Pastas de usuÃ¡rios
â””â”€â”€ mnt/                # Pontos de montagem (cloud storage)
\`\`\`

### Comandos para explorar DBFS:

\`\`\`python
# Listar conteÃºdo
%fs ls /

# Ver datasets de exemplo
%fs ls /databricks-datasets/

# Criar pasta
%fs mkdirs /FileStore/meu-projeto/

# Copiar arquivo
%fs cp /source/file.csv /destination/file.csv

# Ver conteÃºdo do arquivo
%fs head /databricks-datasets/README.md
\`\`\`

### Do Python:
\`\`\`python
# Usando dbutils
dbutils.fs.ls("/databricks-datasets/")

# Ler arquivo como texto
dbutils.fs.head("/databricks-datasets/README.md")

# Copiar arquivo
dbutils.fs.cp("source", "destination")
\`\`\`

### Datasets de Exemplo DisponÃ­veis:

| Dataset | DescriÃ§Ã£o | Tamanho |
|---------|-----------|---------|
| /databricks-datasets/samples/population-vs-price/ | Dados de cidades | ~10KB |
| /databricks-datasets/nyctaxi/ | TÃ¡xis NYC | ~100MB |
| /databricks-datasets/wine-quality/ | Qualidade de vinhos | ~200KB |
| /databricks-datasets/COVID/ | Dados COVID-19 | ~50MB |
| /databricks-datasets/amazon/ | Reviews Amazon | ~1GB |`
      },
      codeExample: {
        language: 'python',
        code: `# Explorar DBFS desde el notebook

# 1. Listar datasets de ejemplo
display(dbutils.fs.ls("/databricks-datasets/"))

# 2. Ver contenido de un archivo
print(dbutils.fs.head("/databricks-datasets/README.md", 500))

# 3. Crear tu carpeta de trabajo
dbutils.fs.mkdirs("/FileStore/mi-proyecto/")

# 4. Verificar que se creÃ³
display(dbutils.fs.ls("/FileStore/"))

# 5. Cargar un dataset de ejemplo en DataFrame
df = spark.read.csv(
    "/databricks-datasets/samples/population-vs-price/data_geo.csv",
    header=True,
    inferSchema=True
)
display(df)`,
        explanation: {
          es: 'Este cÃ³digo muestra cÃ³mo navegar DBFS, crear carpetas y cargar datos de los datasets de ejemplo.',
          en: 'This code shows how to navigate DBFS, create folders, and load data from sample datasets.',
          pt: 'Este cÃ³digo mostra como navegar no DBFS, criar pastas e carregar dados dos datasets de exemplo.'
        }
      },
      practicalTips: [
        {
          es: 'ğŸ“‚ Los datasets en /databricks-datasets/ son perfectos para practicar sin tener que subir datos propios.',
          en: 'ğŸ“‚ The datasets in /databricks-datasets/ are perfect for practicing without uploading your own data.',
          pt: 'ğŸ“‚ Os datasets em /databricks-datasets/ sÃ£o perfeitos para praticar sem precisar fazer upload de dados prÃ³prios.'
        },
        {
          es: 'âš ï¸ En Free Edition, los datos pueden eliminarse si no usÃ¡s tu cuenta por un perÃ­odo prolongado. HacÃ© backups de datos importantes.',
          en: 'âš ï¸ In Free Edition, data may be deleted if you don\'t use your account for an extended period. Backup important data.',
          pt: 'âš ï¸ No Free Edition, os dados podem ser deletados se vocÃª nÃ£o usar sua conta por um perÃ­odo prolongado. FaÃ§a backup de dados importantes.'
        }
      ],
      externalLinks: [
        {
          title: 'DBFS Documentation',
          url: 'https://docs.databricks.com/dbfs/index.html',
          type: 'docs'
        },
        {
          title: 'Sample Datasets',
          url: 'https://docs.databricks.com/dbfs/databricks-datasets.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Pudiste listar los datasets de ejemplo y cargar uno en un DataFrame?',
        en: 'âœ… Were you able to list the sample datasets and load one into a DataFrame?',
        pt: 'âœ… VocÃª conseguiu listar os datasets de exemplo e carregar um em um DataFrame?'
      },
      xpReward: 25,
      estimatedMinutes: 20
    },
    // PASO 1.7
    {
      id: 'db-1-7',
      title: {
        es: 'Subir tus propios datos a Databricks',
        en: 'Upload your own data to Databricks',
        pt: 'Fazer upload dos seus prÃ³prios dados para o Databricks'
      },
      description: {
        es: 'AprendÃ© a subir archivos CSV, JSON y Parquet desde tu computadora.',
        en: 'Learn to upload CSV, JSON and Parquet files from your computer.',
        pt: 'Aprenda a fazer upload de arquivos CSV, JSON e Parquet do seu computador.'
      },
      theory: {
        es: `## Subir Datos a Databricks

Hay varias formas de subir datos:

### MÃ©todo 1: UI de Databricks (mÃ¡s fÃ¡cil)

1. Ir a **Data** en el menÃº lateral
2. Click en **Create Table**
3. **Drop files to upload** o click para seleccionar
4. Elegir opciones:
   - Crear tabla o solo subir archivo
   - Nombre de la tabla
   - Tipo de archivo (CSV, JSON, Parquet, etc.)

### MÃ©todo 2: Arrastrar al Notebook

1. Abrir un notebook
2. Arrastrar archivo desde tu computadora al notebook
3. Databricks genera cÃ³digo automÃ¡ticamente

### MÃ©todo 3: dbutils (programÃ¡tico)

\`\`\`python
# DespuÃ©s de subir por UI, el archivo queda en:
# /FileStore/tables/tu_archivo.csv

# Leerlo
df = spark.read.csv(
    "/FileStore/tables/tu_archivo.csv",
    header=True,
    inferSchema=True
)
\`\`\`

### MÃ©todo 4: Desde URL externa

\`\`\`python
# Descargar desde internet
import urllib.request

url = "https://example.com/data.csv"
local_path = "/tmp/data.csv"
dbfs_path = "/FileStore/data.csv"

# Descargar
urllib.request.urlretrieve(url, local_path)

# Copiar a DBFS
dbutils.fs.cp(f"file:{local_path}", f"dbfs:{dbfs_path}")
\`\`\`

### Formatos soportados:
| Formato | ExtensiÃ³n | Uso recomendado |
|---------|-----------|-----------------|
| CSV | .csv | Datos pequeÃ±os, legibles |
| JSON | .json | APIs, documentos |
| Parquet | .parquet | ProducciÃ³n, performance |
| Delta | .delta | Databricks native |
| Avro | .avro | Streaming |
| ORC | .orc | Hive compatibility |`,
        en: `## Upload Data to Databricks

There are several ways to upload data:

### Method 1: Databricks UI (easiest)

1. Go to **Data** in the sidebar
2. Click **Create Table**
3. **Drop files to upload** or click to select
4. Choose options:
   - Create table or just upload file
   - Table name
   - File type (CSV, JSON, Parquet, etc.)

### Method 2: Drag to Notebook

1. Open a notebook
2. Drag file from your computer to the notebook
3. Databricks generates code automatically

### Method 3: dbutils (programmatic)

\`\`\`python
# After uploading via UI, the file is at:
# /FileStore/tables/your_file.csv

# Read it
df = spark.read.csv(
    "/FileStore/tables/your_file.csv",
    header=True,
    inferSchema=True
)
\`\`\`

### Method 4: From external URL

\`\`\`python
# Download from internet
import urllib.request

url = "https://example.com/data.csv"
local_path = "/tmp/data.csv"
dbfs_path = "/FileStore/data.csv"

# Download
urllib.request.urlretrieve(url, local_path)

# Copy to DBFS
dbutils.fs.cp(f"file:{local_path}", f"dbfs:{dbfs_path}")
\`\`\`

### Supported formats:
| Format | Extension | Recommended use |
|--------|-----------|-----------------|
| CSV | .csv | Small data, readable |
| JSON | .json | APIs, documents |
| Parquet | .parquet | Production, performance |
| Delta | .delta | Databricks native |
| Avro | .avro | Streaming |
| ORC | .orc | Hive compatibility |`,
        pt: `## Fazer Upload de Dados para o Databricks

Existem vÃ¡rias formas de fazer upload de dados:

### MÃ©todo 1: UI do Databricks (mais fÃ¡cil)

1. Ir para **Data** no menu lateral
2. Clicar em **Create Table**
3. **Drop files to upload** ou clicar para selecionar
4. Escolher opÃ§Ãµes:
   - Criar tabela ou apenas fazer upload
   - Nome da tabela
   - Tipo de arquivo (CSV, JSON, Parquet, etc.)

### MÃ©todo 2: Arrastar para o Notebook

1. Abrir um notebook
2. Arrastar arquivo do seu computador para o notebook
3. Databricks gera cÃ³digo automaticamente

### MÃ©todo 3: dbutils (programÃ¡tico)

\`\`\`python
# ApÃ³s fazer upload pela UI, o arquivo fica em:
# /FileStore/tables/seu_arquivo.csv

# Ler
df = spark.read.csv(
    "/FileStore/tables/seu_arquivo.csv",
    header=True,
    inferSchema=True
)
\`\`\`

### MÃ©todo 4: De URL externa

\`\`\`python
# Baixar da internet
import urllib.request

url = "https://example.com/data.csv"
local_path = "/tmp/data.csv"
dbfs_path = "/FileStore/data.csv"

# Baixar
urllib.request.urlretrieve(url, local_path)

# Copiar para DBFS
dbutils.fs.cp(f"file:{local_path}", f"dbfs:{dbfs_path}")
\`\`\`

### Formatos suportados:
| Formato | ExtensÃ£o | Uso recomendado |
|---------|----------|-----------------|
| CSV | .csv | Dados pequenos, legÃ­veis |
| JSON | .json | APIs, documentos |
| Parquet | .parquet | ProduÃ§Ã£o, performance |
| Delta | .delta | Databricks nativo |
| Avro | .avro | Streaming |
| ORC | .orc | Compatibilidade Hive |`
      },
      practicalTips: [
        {
          es: 'ğŸ“Š Siempre que puedas, convertÃ­ tus datos a Parquet o Delta. Son mucho mÃ¡s eficientes.',
          en: 'ğŸ“Š Whenever possible, convert your data to Parquet or Delta. They are much more efficient.',
          pt: 'ğŸ“Š Sempre que puder, converta seus dados para Parquet ou Delta. SÃ£o muito mais eficientes.'
        },
        {
          es: 'ğŸ’¡ Free Edition tiene lÃ­mites de almacenamiento. Para datasets grandes, considerÃ¡ un plan pago.',
          en: 'ğŸ’¡ Free Edition has storage limits. For large datasets, consider a paid plan.',
          pt: 'ğŸ’¡ Free Edition tem limites de armazenamento. Para datasets grandes, considere um plano pago.'
        },
        {
          es: 'âš ï¸ El acceso a internet externo estÃ¡ limitado a dominios confiables en Free Edition.',
          en: 'âš ï¸ External internet access is limited to trusted domains in Free Edition.',
          pt: 'âš ï¸ O acesso Ã  internet externa Ã© limitado a domÃ­nios confiÃ¡veis no Free Edition.'
        }
      ],
      externalLinks: [
        {
          title: 'Importing Data Documentation',
          url: 'https://docs.databricks.com/data/data.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Pudiste subir un archivo CSV propio y leerlo como DataFrame?',
        en: 'âœ… Were you able to upload your own CSV file and read it as a DataFrame?',
        pt: 'âœ… VocÃª conseguiu fazer upload de um arquivo CSV prÃ³prio e lÃª-lo como DataFrame?'
      },
      xpReward: 20,
      estimatedMinutes: 20
    },
    // PASO 1.8
    {
      id: 'db-1-8',
      title: {
        es: 'Visualizaciones Nativas en Databricks',
        en: 'Native Visualizations in Databricks',
        pt: 'VisualizaÃ§Ãµes Nativas no Databricks'
      },
      description: {
        es: 'Databricks tiene visualizaciones built-in increÃ­bles. AprendÃ© a usarlas.',
        en: 'Databricks has amazing built-in visualizations. Learn to use them.',
        pt: 'O Databricks tem visualizaÃ§Ãµes built-in incrÃ­veis. Aprenda a usÃ¡-las.'
      },
      theory: {
        es: `## Visualizaciones en Databricks

Databricks tiene un sistema de visualizaciÃ³n muy poderoso integrado.

### Usar display() en vez de show()

\`\`\`python
# âŒ Esto muestra texto plano
df.show()

# âœ… Esto muestra una tabla interactiva con opciones de visualizaciÃ³n
display(df)
\`\`\`

### Tipos de VisualizaciÃ³n Disponibles:

DespuÃ©s de ejecutar \`display(df)\`, podÃ©s hacer click en el Ã­cono de grÃ¡fico para:

| Tipo | Uso |
|------|-----|
| ğŸ“Š Bar Chart | Comparar categorÃ­as |
| ğŸ“ˆ Line Chart | Tendencias temporales |
| ğŸ¥§ Pie Chart | Proporciones |
| ğŸ“‰ Area Chart | VolÃºmenes acumulados |
| ğŸ—ºï¸ Map | Datos geogrÃ¡ficos |
| ğŸ“‹ Pivot Table | AnÃ¡lisis multidimensional |
| ğŸ¯ Scatter Plot | Correlaciones |

### Configurar VisualizaciÃ³n:

1. Ejecutar \`display(df)\`
2. Click en **+** al lado de "Table"
3. Elegir tipo de grÃ¡fico
4. Configurar:
   - Keys (eje X)
   - Values (eje Y)
   - Series groupings
   - Aggregations

### Guardar VisualizaciÃ³n:

Las visualizaciones se guardan con el notebook. PodÃ©s:
- Renombrarlas
- Tener mÃºltiples por celda
- Exportarlas como imagen`,
        en: `## Visualizations in Databricks

Databricks has a very powerful integrated visualization system.

### Use display() instead of show()

\`\`\`python
# âŒ This shows plain text
df.show()

# âœ… This shows an interactive table with visualization options
display(df)
\`\`\`

### Available Visualization Types:

After running \`display(df)\`, you can click the chart icon for:

| Type | Use |
|------|-----|
| ğŸ“Š Bar Chart | Compare categories |
| ğŸ“ˆ Line Chart | Time trends |
| ğŸ¥§ Pie Chart | Proportions |
| ğŸ“‰ Area Chart | Cumulative volumes |
| ğŸ—ºï¸ Map | Geographic data |
| ğŸ“‹ Pivot Table | Multidimensional analysis |
| ğŸ¯ Scatter Plot | Correlations |

### Configure Visualization:

1. Run \`display(df)\`
2. Click **+** next to "Table"
3. Choose chart type
4. Configure:
   - Keys (X axis)
   - Values (Y axis)
   - Series groupings
   - Aggregations

### Save Visualization:

Visualizations are saved with the notebook. You can:
- Rename them
- Have multiple per cell
- Export as image`,
        pt: `## VisualizaÃ§Ãµes no Databricks

O Databricks tem um sistema de visualizaÃ§Ã£o muito poderoso integrado.

### Usar display() em vez de show()

\`\`\`python
# âŒ Isso mostra texto simples
df.show()

# âœ… Isso mostra uma tabela interativa com opÃ§Ãµes de visualizaÃ§Ã£o
display(df)
\`\`\`

### Tipos de VisualizaÃ§Ã£o DisponÃ­veis:

ApÃ³s executar \`display(df)\`, vocÃª pode clicar no Ã­cone de grÃ¡fico para:

| Tipo | Uso |
|------|-----|
| ğŸ“Š Bar Chart | Comparar categorias |
| ğŸ“ˆ Line Chart | TendÃªncias temporais |
| ğŸ¥§ Pie Chart | ProporÃ§Ãµes |
| ğŸ“‰ Area Chart | Volumes acumulados |
| ğŸ—ºï¸ Map | Dados geogrÃ¡ficos |
| ğŸ“‹ Pivot Table | AnÃ¡lise multidimensional |
| ğŸ¯ Scatter Plot | CorrelaÃ§Ãµes |

### Configurar VisualizaÃ§Ã£o:

1. Executar \`display(df)\`
2. Clicar em **+** ao lado de "Table"
3. Escolher tipo de grÃ¡fico
4. Configurar:
   - Keys (eixo X)
   - Values (eixo Y)
   - Series groupings
   - Aggregations

### Salvar VisualizaÃ§Ã£o:

As visualizaÃ§Ãµes sÃ£o salvas com o notebook. VocÃª pode:
- RenomeÃ¡-las
- Ter mÃºltiplas por cÃ©lula
- ExportÃ¡-las como imagem`
      },
      codeExample: {
        language: 'python',
        code: `# Cargar datos de ejemplo para visualizar
df = spark.read.csv(
    "/databricks-datasets/samples/population-vs-price/data_geo.csv",
    header=True,
    inferSchema=True
)

# Ver con display (interactivo)
display(df)

# Tip: DespuÃ©s de ejecutar, click en el Ã­cono de grÃ¡fico
# para crear visualizaciones sin cÃ³digo

# TambiÃ©n podÃ©s agregar tÃ­tulos con displayHTML
displayHTML("<h2>ğŸ™ï¸ AnÃ¡lisis de Ciudades</h2>")
display(df)`,
        explanation: {
          es: 'UsÃ¡ display() para ver tablas interactivas. Luego podÃ©s crear grÃ¡ficos con clicks.',
          en: 'Use display() to see interactive tables. Then you can create charts with clicks.',
          pt: 'Use display() para ver tabelas interativas. Depois vocÃª pode criar grÃ¡ficos com cliques.'
        }
      },
      practicalTips: [
        {
          es: 'ğŸ¨ Databricks guarda las visualizaciones automÃ¡ticamente. No perdÃ©s tu trabajo.',
          en: 'ğŸ¨ Databricks saves visualizations automatically. You won\'t lose your work.',
          pt: 'ğŸ¨ O Databricks salva as visualizaÃ§Ãµes automaticamente. VocÃª nÃ£o perde seu trabalho.'
        },
        {
          es: 'ğŸ’¡ UsÃ¡ displayHTML() para agregar tÃ­tulos y formato entre visualizaciones.',
          en: 'ğŸ’¡ Use displayHTML() to add titles and formatting between visualizations.',
          pt: 'ğŸ’¡ Use displayHTML() para adicionar tÃ­tulos e formataÃ§Ã£o entre visualizaÃ§Ãµes.'
        }
      ],
      externalLinks: [
        {
          title: 'Databricks Visualizations',
          url: 'https://docs.databricks.com/visualizations/index.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Creaste al menos un grÃ¡fico de barras y uno de lÃ­neas usando display()?',
        en: 'âœ… Did you create at least one bar chart and one line chart using display()?',
        pt: 'âœ… VocÃª criou pelo menos um grÃ¡fico de barras e um de linhas usando display()?'
      },
      xpReward: 20,
      estimatedMinutes: 20
    },
    // PASO 1.9
    {
      id: 'db-1-9',
      title: {
        es: 'dbutils: La Navaja Suiza de Databricks',
        en: 'dbutils: The Swiss Army Knife of Databricks',
        pt: 'dbutils: O Canivete SuÃ­Ã§o do Databricks'
      },
      description: {
        es: 'dbutils es una utilidad poderosa para trabajar con archivos, secrets, widgets y mÃ¡s.',
        en: 'dbutils is a powerful utility for working with files, secrets, widgets and more.',
        pt: 'dbutils Ã© um utilitÃ¡rio poderoso para trabalhar com arquivos, secrets, widgets e mais.'
      },
      theory: {
        es: `## dbutils - Databricks Utilities

\`dbutils\` es un objeto que viene pre-cargado en Databricks con utilidades muy Ãºtiles.

### MÃ³dulos principales:

### 1. dbutils.fs - Sistema de archivos
\`\`\`python
# Listar archivos
dbutils.fs.ls("/path/")

# Copiar
dbutils.fs.cp("source", "dest")

# Mover
dbutils.fs.mv("source", "dest")

# Eliminar
dbutils.fs.rm("path", recurse=True)

# Crear carpeta
dbutils.fs.mkdirs("path")

# Ver contenido
dbutils.fs.head("path", maxBytes=1000)
\`\`\`

### 2. dbutils.widgets - ParÃ¡metros interactivos
\`\`\`python
# Crear widget de texto
dbutils.widgets.text("nombre", "default", "Etiqueta")

# Crear dropdown
dbutils.widgets.dropdown("pais", "AR", ["AR", "MX", "CO"])

# Obtener valor
valor = dbutils.widgets.get("nombre")

# Eliminar
dbutils.widgets.remove("nombre")
dbutils.widgets.removeAll()
\`\`\`

### 3. dbutils.secrets - Credenciales seguras
\`\`\`python
# Obtener secret (configurado en Scope)
password = dbutils.secrets.get(scope="mi-scope", key="db-password")
\`\`\`

### 4. dbutils.notebook - Ejecutar otros notebooks
\`\`\`python
# Ejecutar notebook y obtener resultado
result = dbutils.notebook.run("path/notebook", timeout_seconds=60)

# Salir del notebook con valor
dbutils.notebook.exit("valor_de_retorno")
\`\`\`

### 5. dbutils.library - Instalar bibliotecas
\`\`\`python
# Instalar desde PyPI
dbutils.library.installPyPI("pandas")

# Reiniciar Python
dbutils.library.restartPython()
\`\`\`

### Ver ayuda:
\`\`\`python
dbutils.help()
dbutils.fs.help()
\`\`\``,
        en: `## dbutils - Databricks Utilities

\`dbutils\` is a pre-loaded object in Databricks with very useful utilities.

### Main modules:

### 1. dbutils.fs - File system
\`\`\`python
# List files
dbutils.fs.ls("/path/")

# Copy
dbutils.fs.cp("source", "dest")

# Move
dbutils.fs.mv("source", "dest")

# Delete
dbutils.fs.rm("path", recurse=True)

# Create folder
dbutils.fs.mkdirs("path")

# View contents
dbutils.fs.head("path", maxBytes=1000)
\`\`\`

### 2. dbutils.widgets - Interactive parameters
\`\`\`python
# Create text widget
dbutils.widgets.text("name", "default", "Label")

# Create dropdown
dbutils.widgets.dropdown("country", "US", ["US", "MX", "CO"])

# Get value
value = dbutils.widgets.get("name")

# Remove
dbutils.widgets.remove("name")
dbutils.widgets.removeAll()
\`\`\`

### 3. dbutils.secrets - Secure credentials
\`\`\`python
# Get secret (configured in Scope)
password = dbutils.secrets.get(scope="my-scope", key="db-password")
\`\`\`

### 4. dbutils.notebook - Run other notebooks
\`\`\`python
# Run notebook and get result
result = dbutils.notebook.run("path/notebook", timeout_seconds=60)

# Exit notebook with value
dbutils.notebook.exit("return_value")
\`\`\`

### 5. dbutils.library - Install libraries
\`\`\`python
# Install from PyPI
dbutils.library.installPyPI("pandas")

# Restart Python
dbutils.library.restartPython()
\`\`\`

### View help:
\`\`\`python
dbutils.help()
dbutils.fs.help()
\`\`\``,
        pt: `## dbutils - Databricks Utilities

\`dbutils\` Ã© um objeto prÃ©-carregado no Databricks com utilidades muito Ãºteis.

### MÃ³dulos principais:

### 1. dbutils.fs - Sistema de arquivos
\`\`\`python
# Listar arquivos
dbutils.fs.ls("/path/")

# Copiar
dbutils.fs.cp("source", "dest")

# Mover
dbutils.fs.mv("source", "dest")

# Excluir
dbutils.fs.rm("path", recurse=True)

# Criar pasta
dbutils.fs.mkdirs("path")

# Ver conteÃºdo
dbutils.fs.head("path", maxBytes=1000)
\`\`\`

### 2. dbutils.widgets - ParÃ¢metros interativos
\`\`\`python
# Criar widget de texto
dbutils.widgets.text("nome", "default", "RÃ³tulo")

# Criar dropdown
dbutils.widgets.dropdown("pais", "BR", ["BR", "MX", "CO"])

# Obter valor
valor = dbutils.widgets.get("nome")

# Remover
dbutils.widgets.remove("nome")
dbutils.widgets.removeAll()
\`\`\`

### 3. dbutils.secrets - Credenciais seguras
\`\`\`python
# Obter secret (configurado no Scope)
password = dbutils.secrets.get(scope="meu-scope", key="db-password")
\`\`\`

### 4. dbutils.notebook - Executar outros notebooks
\`\`\`python
# Executar notebook e obter resultado
result = dbutils.notebook.run("path/notebook", timeout_seconds=60)

# Sair do notebook com valor
dbutils.notebook.exit("valor_de_retorno")
\`\`\`

### 5. dbutils.library - Instalar bibliotecas
\`\`\`python
# Instalar do PyPI
dbutils.library.installPyPI("pandas")

# Reiniciar Python
dbutils.library.restartPython()
\`\`\`

### Ver ajuda:
\`\`\`python
dbutils.help()
dbutils.fs.help()
\`\`\``
      },
      codeExample: {
        language: 'python',
        code: `# Ejemplos prÃ¡cticos de dbutils

# 1. Ver ayuda general
dbutils.help()

# 2. Listar datasets de ejemplo
display(dbutils.fs.ls("/databricks-datasets/"))

# 3. Crear un widget para filtrar datos
dbutils.widgets.dropdown(
    "dataset",
    "wine-quality",
    ["wine-quality", "nyctaxi", "COVID"],
    "Seleccionar Dataset"
)

# 4. Usar el valor del widget
selected = dbutils.widgets.get("dataset")
print(f"Dataset seleccionado: {selected}")

# 5. Mostrar archivos del dataset seleccionado
display(dbutils.fs.ls(f"/databricks-datasets/{selected}/"))`,
        explanation: {
          es: 'Los widgets permiten crear notebooks interactivos donde el usuario puede seleccionar parÃ¡metros sin tocar cÃ³digo.',
          en: 'Widgets allow you to create interactive notebooks where the user can select parameters without touching code.',
          pt: 'Os widgets permitem criar notebooks interativos onde o usuÃ¡rio pode selecionar parÃ¢metros sem tocar no cÃ³digo.'
        }
      },
      practicalTips: [
        {
          es: 'â­ Los widgets son ideales para crear notebooks que usen otras personas no tÃ©cnicas.',
          en: 'â­ Widgets are ideal for creating notebooks that non-technical people can use.',
          pt: 'â­ Os widgets sÃ£o ideais para criar notebooks que pessoas nÃ£o tÃ©cnicas possam usar.'
        },
        {
          es: 'ğŸ”’ Nunca hardcodees passwords en el cÃ³digo. UsÃ¡ dbutils.secrets.',
          en: 'ğŸ”’ Never hardcode passwords in code. Use dbutils.secrets.',
          pt: 'ğŸ”’ Nunca coloque senhas fixas no cÃ³digo. Use dbutils.secrets.'
        }
      ],
      externalLinks: [
        {
          title: 'dbutils Reference',
          url: 'https://docs.databricks.com/dev-tools/databricks-utils.html',
          type: 'docs'
        },
        {
          title: 'Widgets Documentation',
          url: 'https://docs.databricks.com/notebooks/widgets.html',
          type: 'docs'
        }
      ],
      checkpoint: {
        es: 'âœ… Â¿Creaste un widget dropdown y lo usaste para filtrar datos?',
        en: 'âœ… Did you create a dropdown widget and use it to filter data?',
        pt: 'âœ… VocÃª criou um widget dropdown e o usou para filtrar dados?'
      },
      xpReward: 25,
      estimatedMinutes: 25
    },
    // PASO 1.10
    {
      id: 'db-1-10',
      title: {
        es: 'Proyecto Mini: Tu Primer Pipeline Completo',
        en: 'Mini Project: Your First Complete Pipeline',
        pt: 'Mini Projeto: Seu Primeiro Pipeline Completo'
      },
      description: {
        es: 'JuntÃ¡ todo lo aprendido en un mini proyecto: cargar datos, transformar, visualizar.',
        en: 'Put everything learned together in a mini project: load data, transform, visualize.',
        pt: 'Junte tudo o que aprendeu em um mini projeto: carregar dados, transformar, visualizar.'
      },
      theory: {
        es: `## Mini Proyecto: AnÃ¡lisis de Calidad de Vinos ğŸ·

Vamos a crear un notebook completo que:
1. Cargue datos del dataset wine-quality
2. Explore y limpie los datos
3. Cree visualizaciones
4. Guarde resultados

### Estructura del Notebook:

\`\`\`
ğŸ““ 01-Analisis-Vinos
â”œâ”€â”€ ğŸ“ Markdown: TÃ­tulo y descripciÃ³n
â”œâ”€â”€ ğŸ”§ Setup: Imports y configuraciÃ³n
â”œâ”€â”€ ğŸ“¥ Ingesta: Cargar datos
â”œâ”€â”€ ğŸ” ExploraciÃ³n: AnÃ¡lisis inicial
â”œâ”€â”€ ğŸ§¹ Limpieza: Transformaciones
â”œâ”€â”€ ğŸ“Š VisualizaciÃ³n: GrÃ¡ficos
â””â”€â”€ ğŸ’¾ Guardado: Persistir resultados
\`\`\`

### Checklist del Proyecto:

- [ ] Crear notebook con nombre descriptivo
- [ ] Documentar con Markdown
- [ ] Cargar dataset wine-quality
- [ ] Explorar schema y estadÃ­sticas
- [ ] Crear al menos 3 visualizaciones
- [ ] Guardar DataFrame limpio como tabla
- [ ] Agregar conclusiones`,
        en: `## Mini Project: Wine Quality Analysis ğŸ·

We're going to create a complete notebook that:
1. Loads data from wine-quality dataset
2. Explores and cleans the data
3. Creates visualizations
4. Saves results

### Notebook Structure:

\`\`\`
ğŸ““ 01-Wine-Analysis
â”œâ”€â”€ ğŸ“ Markdown: Title and description
â”œâ”€â”€ ğŸ”§ Setup: Imports and configuration
â”œâ”€â”€ ğŸ“¥ Ingestion: Load data
â”œâ”€â”€ ğŸ” Exploration: Initial analysis
â”œâ”€â”€ ğŸ§¹ Cleaning: Transformations
â”œâ”€â”€ ğŸ“Š Visualization: Charts
â””â”€â”€ ğŸ’¾ Saving: Persist results
\`\`\`

### Project Checklist:

- [ ] Create notebook with descriptive name
- [ ] Document with Markdown
- [ ] Load wine-quality dataset
- [ ] Explore schema and statistics
- [ ] Create at least 3 visualizations
- [ ] Save clean DataFrame as table
- [ ] Add conclusions`,
        pt: `## Mini Projeto: AnÃ¡lise de Qualidade de Vinhos ğŸ·

Vamos criar um notebook completo que:
1. Carregue dados do dataset wine-quality
2. Explore e limpe os dados
3. Crie visualizaÃ§Ãµes
4. Salve resultados

### Estrutura do Notebook:

\`\`\`
ğŸ““ 01-Analise-Vinhos
â”œâ”€â”€ ğŸ“ Markdown: TÃ­tulo e descriÃ§Ã£o
â”œâ”€â”€ ğŸ”§ Setup: Imports e configuraÃ§Ã£o
â”œâ”€â”€ ğŸ“¥ IngestÃ£o: Carregar dados
â”œâ”€â”€ ğŸ” ExploraÃ§Ã£o: AnÃ¡lise inicial
â”œâ”€â”€ ğŸ§¹ Limpeza: TransformaÃ§Ãµes
â”œâ”€â”€ ğŸ“Š VisualizaÃ§Ã£o: GrÃ¡ficos
â””â”€â”€ ğŸ’¾ Salvamento: Persistir resultados
\`\`\`

### Checklist do Projeto:

- [ ] Criar notebook com nome descritivo
- [ ] Documentar com Markdown
- [ ] Carregar dataset wine-quality
- [ ] Explorar schema e estatÃ­sticas
- [ ] Criar pelo menos 3 visualizaÃ§Ãµes
- [ ] Salvar DataFrame limpo como tabela
- [ ] Adicionar conclusÃµes`
      },
      codeExample: {
        language: 'python',
        code: `# ===========================================
# ğŸ· ANÃLISIS DE CALIDAD DE VINOS
# Proyecto de la Fase 1 - Databricks
# ===========================================

# %md
# # ğŸ· AnÃ¡lisis de Calidad de Vinos
# 
# **Objetivo:** Explorar el dataset de calidad de vinos y crear visualizaciones.
# 
# **Dataset:** /databricks-datasets/wine-quality/

# Celda 1: Setup
from pyspark.sql.functions import *

# Celda 2: Cargar datos
df_red = spark.read.csv(
    "/databricks-datasets/wine-quality/winequality-red.csv",
    header=True,
    inferSchema=True,
    sep=";"
)
df_red = df_red.withColumn("wine_type", lit("red"))

df_white = spark.read.csv(
    "/databricks-datasets/wine-quality/winequality-white.csv",
    header=True,
    inferSchema=True,
    sep=";"
)
df_white = df_white.withColumn("wine_type", lit("white"))

# Unir ambos datasets
df = df_red.union(df_white)
print(f"Total de vinos: {df.count()}")

# Celda 3: ExploraciÃ³n
df.printSchema()
display(df.describe())

# Celda 4: AnÃ¡lisis de calidad
# (Crear grÃ¡fico de barras despuÃ©s de ejecutar)
quality_dist = df.groupBy("quality", "wine_type").count()
display(quality_dist)

# Celda 5: CorrelaciÃ³n alcohol vs calidad
# (Crear scatter plot)
display(df.select("alcohol", "quality", "wine_type"))

# Celda 6: Guardar como tabla
df.write.mode("overwrite").saveAsTable("wines_analysis")
print("âœ… Tabla guardada exitosamente!")`,
        explanation: {
          es: 'Este proyecto integra todo lo aprendido: cargar datos, explorar, transformar, visualizar y guardar.',
          en: 'This project integrates everything learned: loading data, exploring, transforming, visualizing and saving.',
          pt: 'Este projeto integra tudo o que foi aprendido: carregar dados, explorar, transformar, visualizar e salvar.'
        }
      },
      practicalTips: [
        {
          es: 'ğŸ““ Este notebook puede ser parte de tu portfolio. Hacelo prolijo!',
          en: 'ğŸ““ This notebook can be part of your portfolio. Make it neat!',
          pt: 'ğŸ““ Este notebook pode fazer parte do seu portfÃ³lio. FaÃ§a-o organizado!'
        },
        {
          es: 'ğŸ’¡ AgregÃ¡ tus propias conclusiones al final del notebook. Muestra pensamiento analÃ­tico.',
          en: 'ğŸ’¡ Add your own conclusions at the end of the notebook. Show analytical thinking.',
          pt: 'ğŸ’¡ Adicione suas prÃ³prias conclusÃµes no final do notebook. Mostre pensamento analÃ­tico.'
        }
      ],
      externalLinks: [
        {
          title: 'Wine Quality Dataset Info',
          url: 'https://archive.ics.uci.edu/ml/datasets/wine+quality',
          type: 'article'
        }
      ],
      checkpoint: {
        es: 'ğŸ† Â¿Completaste el notebook con al menos 3 visualizaciones y guardaste la tabla?',
        en: 'ğŸ† Did you complete the notebook with at least 3 visualizations and save the table?',
        pt: 'ğŸ† VocÃª completou o notebook com pelo menos 3 visualizaÃ§Ãµes e salvou a tabela?'
      },
      xpReward: 50,
      estimatedMinutes: 45
    }
  ]
};


