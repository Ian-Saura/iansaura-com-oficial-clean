import { DatabricksLab } from './types';

/**
 * Labs de Delta Lake
 * El corazÃ³n del Lakehouse
 */
export const DATABRICKS_LABS_DELTA_LAKE: DatabricksLab[] = [
  // =====================================================
  // LAB 4: IntroducciÃ³n a Delta Lake
  // =====================================================
  {
    id: 'db-lab-004',
    title: {
      es: 'ðŸ”· IntroducciÃ³n a Delta Lake',
      en: 'ðŸ”· Introduction to Delta Lake',
      pt: 'ðŸ”· IntroduÃ§Ã£o ao Delta Lake'
    },
    subtitle: {
      es: 'ACID, Time Travel y Schema Enforcement',
      en: 'ACID, Time Travel and Schema Enforcement',
      pt: 'ACID, Time Travel e Schema Enforcement'
    },
    description: {
      es: 'Descubre Delta Lake, el formato de almacenamiento que hace posible el Lakehouse. AprenderÃ¡s sobre transacciones ACID, time travel, y schema enforcement.',
      en: 'Discover Delta Lake, the storage format that makes Lakehouse possible. You will learn about ACID transactions, time travel, and schema enforcement.',
      pt: 'Descubra o Delta Lake, o formato de armazenamento que torna o Lakehouse possÃ­vel. VocÃª aprenderÃ¡ sobre transaÃ§Ãµes ACID, time travel e schema enforcement.'
    },
    difficulty: 'intermediate',
    estimatedMinutes: 50,
    relatedPhases: ['db-phase-5'],
    prerequisites: [
      { es: 'Labs 1-3 completados', en: 'Labs 1-3 completed', pt: 'Labs 1-3 completados' },
      { es: 'Entender conceptos bÃ¡sicos de bases de datos', en: 'Understand basic database concepts', pt: 'Entender conceitos bÃ¡sicos de bancos de dados' }
    ],
    objectives: [
      { es: 'Crear y gestionar tablas Delta', en: 'Create and manage Delta tables', pt: 'Criar e gerenciar tabelas Delta' },
      { es: 'Usar Time Travel para ver versiones anteriores', en: 'Use Time Travel to view previous versions', pt: 'Usar Time Travel para ver versÃµes anteriores' },
      { es: 'Entender schema enforcement y evolution', en: 'Understand schema enforcement and evolution', pt: 'Entender schema enforcement e evolution' },
      { es: 'Ver el historial de una tabla', en: 'View table history', pt: 'Ver o histÃ³rico de uma tabela' }
    ],
    steps: [
      {
        id: 'lab004-step1',
        title: { es: 'Â¿QuÃ© es Delta Lake?', en: 'What is Delta Lake?', pt: 'O que Ã© Delta Lake?' },
        description: {
          es: 'Delta Lake es un formato de almacenamiento open-source que agrega ACID transactions, schema enforcement, y time travel a data lakes.',
          en: 'Delta Lake is an open-source storage format that adds ACID transactions, schema enforcement, and time travel to data lakes.',
          pt: 'Delta Lake Ã© um formato de armazenamento open-source que adiciona transaÃ§Ãµes ACID, schema enforcement e time travel a data lakes.'
        },
        code: `# Delta Lake = Parquet + Transaction Log
# 
# Ventajas sobre Parquet puro:
# âœ… ACID transactions (lecturas/escrituras consistentes)
# âœ… Time Travel (volver a versiones anteriores)
# âœ… Schema enforcement (rechaza datos incorrectos)
# âœ… Schema evolution (agregar columnas)
# âœ… MERGE, UPDATE, DELETE (impossible en Parquet)
# âœ… OptimizaciÃ³n automÃ¡tica (OPTIMIZE, Z-ORDER)

# Ver que Delta Lake ya estÃ¡ disponible
print(f"VersiÃ³n de Delta Lake: {spark.conf.get('spark.databricks.delta.version')}")`,
        codeLanguage: 'python'
      },
      {
        id: 'lab004-step2',
        title: { es: 'Crear tabla Delta', en: 'Create Delta table', pt: 'Criar tabela Delta' },
        description: {
          es: 'Crea tu primera tabla Delta desde un DataFrame.',
          en: 'Create your first Delta table from a DataFrame.',
          pt: 'Crie sua primeira tabela Delta de um DataFrame.'
        },
        code: `# Crear datos de productos
productos_data = [
    (1, "Laptop Pro", "Electronics", 1299.99, 50),
    (2, "Wireless Mouse", "Electronics", 29.99, 200),
    (3, "USB-C Hub", "Electronics", 49.99, 150),
    (4, "Desk Chair", "Furniture", 299.99, 30),
    (5, "Standing Desk", "Furniture", 599.99, 20)
]

df_productos = spark.createDataFrame(
    productos_data,
    ["product_id", "name", "category", "price", "stock"]
)

# Guardar como tabla Delta
df_productos.write.format("delta").mode("overwrite").save("/tmp/delta/productos")

# TambiÃ©n podemos crear como tabla registrada
df_productos.write.format("delta").mode("overwrite").saveAsTable("productos_delta")

print("âœ… Tabla Delta creada!")

# Verificar
spark.read.format("delta").load("/tmp/delta/productos").show()`,
        codeLanguage: 'python',
        tip: { es: 'ðŸ’¡ format("delta") es la clave para usar Delta Lake', en: 'ðŸ’¡ format("delta") is the key to using Delta Lake', pt: 'ðŸ’¡ format("delta") Ã© a chave para usar o Delta Lake' }
      },
      {
        id: 'lab004-step3',
        title: { es: 'UPDATE y DELETE', en: 'UPDATE and DELETE', pt: 'UPDATE e DELETE' },
        description: {
          es: 'A diferencia de Parquet, Delta Lake soporta UPDATE y DELETE.',
          en: 'Unlike Parquet, Delta Lake supports UPDATE and DELETE.',
          pt: 'Diferente do Parquet, o Delta Lake suporta UPDATE e DELETE.'
        },
        code: `%sql
-- Actualizar precio de un producto
UPDATE productos_delta
SET price = 1199.99
WHERE product_id = 1;

-- Verificar el cambio
SELECT * FROM productos_delta WHERE product_id = 1;

-- Eliminar producto con stock bajo
DELETE FROM productos_delta
WHERE stock < 25;

-- Verificar
SELECT * FROM productos_delta;`,
        codeLanguage: 'sql',
        checkpoint: { es: 'Â¿CuÃ¡ntos productos quedan despuÃ©s del DELETE?', en: 'How many products remain after DELETE?', pt: 'Quantos produtos restam apÃ³s o DELETE?' }
      },
      {
        id: 'lab004-step4',
        title: { es: 'Ver historial de la tabla', en: 'View table history', pt: 'Ver histÃ³rico da tabela' },
        description: {
          es: 'Delta Lake mantiene un registro de todas las operaciones realizadas.',
          en: 'Delta Lake keeps a record of all operations performed.',
          pt: 'O Delta Lake mantÃ©m um registro de todas as operaÃ§Ãµes realizadas.'
        },
        code: `%sql
-- Ver historial completo
DESCRIBE HISTORY productos_delta;

-- Historial limitado
DESCRIBE HISTORY productos_delta LIMIT 5;`,
        codeLanguage: 'sql',
        tip: { es: 'ðŸ’¡ El historial muestra quiÃ©n, cuÃ¡ndo y quÃ© operaciÃ³n se hizo', en: 'ðŸ’¡ History shows who, when and what operation was done', pt: 'ðŸ’¡ O histÃ³rico mostra quem, quando e que operaÃ§Ã£o foi feita' }
      },
      {
        id: 'lab004-step5',
        title: { es: 'Time Travel', en: 'Time Travel', pt: 'Time Travel' },
        description: {
          es: 'Viaja en el tiempo para ver versiones anteriores de tus datos.',
          en: 'Time travel to see previous versions of your data.',
          pt: 'Viaje no tempo para ver versÃµes anteriores dos seus dados.'
        },
        code: `%sql
-- Ver datos de la versiÃ³n 0 (inicial)
SELECT * FROM productos_delta VERSION AS OF 0;

-- TambiÃ©n puedes usar timestamp
-- SELECT * FROM productos_delta TIMESTAMP AS OF '2024-01-15 10:00:00';

-- Comparar versiÃ³n actual vs anterior
SELECT 'actual' as version, * FROM productos_delta
UNION ALL
SELECT 'version_0' as version, * FROM productos_delta VERSION AS OF 0;`,
        codeLanguage: 'sql'
      },
      {
        id: 'lab004-step6',
        title: { es: 'Time Travel con Python', en: 'Time Travel with Python', pt: 'Time Travel com Python' },
        description: {
          es: 'TambiÃ©n puedes hacer time travel desde Python.',
          en: 'You can also do time travel from Python.',
          pt: 'VocÃª tambÃ©m pode fazer time travel do Python.'
        },
        code: `from delta.tables import DeltaTable

# Leer versiÃ³n especÃ­fica
df_v0 = spark.read.format("delta").option("versionAsOf", 0).load("/tmp/delta/productos")
print("VersiÃ³n 0:")
df_v0.show()

# Leer versiÃ³n actual
df_actual = spark.read.format("delta").load("/tmp/delta/productos")
print("VersiÃ³n actual:")
df_actual.show()

# Contar diferencia
print(f"Diferencia de registros: {df_v0.count() - df_actual.count()}")`,
        codeLanguage: 'python'
      },
      {
        id: 'lab004-step7',
        title: { es: 'Schema Enforcement', en: 'Schema Enforcement', pt: 'Schema Enforcement' },
        description: {
          es: 'Delta Lake rechaza datos que no coinciden con el schema.',
          en: 'Delta Lake rejects data that doesn\'t match the schema.',
          pt: 'O Delta Lake rejeita dados que nÃ£o correspondem ao schema.'
        },
        code: `# Intentar insertar datos con schema diferente
bad_data = [
    (6, "New Product", "Electronics", "INVALID_PRICE", 100)  # price deberÃ­a ser float
]

df_bad = spark.createDataFrame(bad_data, ["product_id", "name", "category", "price", "stock"])

try:
    df_bad.write.format("delta").mode("append").save("/tmp/delta/productos")
except Exception as e:
    print(f"âŒ Error esperado: {type(e).__name__}")
    print("Delta Lake rechazÃ³ los datos porque el schema no coincide")
    
# Con datos correctos funciona
good_data = [(6, "New Product", "Electronics", 99.99, 100)]
df_good = spark.createDataFrame(good_data, ["product_id", "name", "category", "price", "stock"])
df_good.write.format("delta").mode("append").save("/tmp/delta/productos")
print("âœ… Datos correctos insertados")`,
        codeLanguage: 'python',
        warning: { es: 'âš ï¸ Schema enforcement es estricto por default', en: 'âš ï¸ Schema enforcement is strict by default', pt: 'âš ï¸ Schema enforcement Ã© estrito por padrÃ£o' }
      },
      {
        id: 'lab004-step8',
        title: { es: 'Schema Evolution', en: 'Schema Evolution', pt: 'Schema Evolution' },
        description: {
          es: 'Agrega nuevas columnas a una tabla existente.',
          en: 'Add new columns to an existing table.',
          pt: 'Adicione novas colunas a uma tabela existente.'
        },
        code: `# Datos con columna nueva
new_data_with_column = [(7, "Premium Item", "Electronics", 199.99, 50, "2024-01-20")]
df_new = spark.createDataFrame(
    new_data_with_column, 
    ["product_id", "name", "category", "price", "stock", "added_date"]
)

# Sin mergeSchema, esto falla
# Con mergeSchema, agrega la nueva columna
df_new.write.format("delta") \\
    .mode("append") \\
    .option("mergeSchema", "true") \\
    .save("/tmp/delta/productos")

# Verificar nuevo schema
spark.read.format("delta").load("/tmp/delta/productos").printSchema()
spark.read.format("delta").load("/tmp/delta/productos").show()`,
        codeLanguage: 'python',
        tip: { es: 'ðŸ’¡ mergeSchema=true permite schema evolution', en: 'ðŸ’¡ mergeSchema=true enables schema evolution', pt: 'ðŸ’¡ mergeSchema=true habilita schema evolution' },
        checkpoint: { es: 'Â¿QuÃ© valor tiene added_date en los registros viejos?', en: 'What value does added_date have in old records?', pt: 'Que valor tem added_date nos registros antigos?' }
      }
    ],
    xpReward: 100,
    badge: {
      id: 'badge-db-delta-intro',
      name: { es: 'Delta Lake Explorer', en: 'Delta Lake Explorer', pt: 'Delta Lake Explorer' },
      icon: 'ðŸ”·'
    },
    resources: [
      { title: 'Delta Lake Documentation', url: 'https://docs.delta.io/latest/index.html', type: 'docs' },
      { title: 'Delta Lake Quick Start', url: 'https://docs.databricks.com/delta/quick-start.html', type: 'docs' }
    ],
    tags: ['intermediate', 'delta-lake', 'acid', 'time-travel'],
    services: ['Delta Lake', 'Spark']
  },

  // =====================================================
  // LAB 5: MERGE y Upserts
  // =====================================================
  {
    id: 'db-lab-005',
    title: {
      es: 'ðŸ”€ MERGE: Upserts en Delta Lake',
      en: 'ðŸ”€ MERGE: Upserts in Delta Lake',
      pt: 'ðŸ”€ MERGE: Upserts no Delta Lake'
    },
    subtitle: {
      es: 'Update, Insert o Delete en una sola operaciÃ³n',
      en: 'Update, Insert or Delete in a single operation',
      pt: 'Update, Insert ou Delete em uma Ãºnica operaÃ§Ã£o'
    },
    description: {
      es: 'Domina MERGE INTO, la operaciÃ³n mÃ¡s poderosa de Delta Lake. AprenderÃ¡s a hacer upserts, CDC (Change Data Capture), y SCD Type 2.',
      en: 'Master MERGE INTO, Delta Lake\'s most powerful operation. You will learn to do upserts, CDC (Change Data Capture), and SCD Type 2.',
      pt: 'Domine MERGE INTO, a operaÃ§Ã£o mais poderosa do Delta Lake. VocÃª aprenderÃ¡ a fazer upserts, CDC (Change Data Capture) e SCD Type 2.'
    },
    difficulty: 'intermediate',
    estimatedMinutes: 45,
    relatedPhases: ['db-phase-5'],
    prerequisites: [
      { es: 'Lab 4 completado', en: 'Lab 4 completed', pt: 'Lab 4 completado' }
    ],
    objectives: [
      { es: 'Usar MERGE INTO para upserts', en: 'Use MERGE INTO for upserts', pt: 'Usar MERGE INTO para upserts' },
      { es: 'Implementar diferentes patrones de MERGE', en: 'Implement different MERGE patterns', pt: 'Implementar diferentes padrÃµes de MERGE' },
      { es: 'Entender CDC con Delta Lake', en: 'Understand CDC with Delta Lake', pt: 'Entender CDC com Delta Lake' },
      { es: 'Usar la API Python de DeltaTable', en: 'Use DeltaTable Python API', pt: 'Usar a API Python do DeltaTable' }
    ],
    steps: [
      {
        id: 'lab005-step1',
        title: { es: 'Preparar tabla de clientes', en: 'Prepare customers table', pt: 'Preparar tabela de clientes' },
        description: {
          es: 'Crea una tabla Delta de clientes para practicar MERGE.',
          en: 'Create a Delta customers table to practice MERGE.',
          pt: 'Crie uma tabela Delta de clientes para praticar MERGE.'
        },
        code: `# Crear tabla de clientes
clientes_data = [
    (1, "Alice Johnson", "alice@email.com", "Gold", "2023-01-15"),
    (2, "Bob Smith", "bob@email.com", "Silver", "2023-03-20"),
    (3, "Charlie Brown", "charlie@email.com", "Bronze", "2023-06-10"),
    (4, "Diana Prince", "diana@email.com", "Gold", "2023-02-28")
]

df_clientes = spark.createDataFrame(
    clientes_data,
    ["customer_id", "name", "email", "tier", "signup_date"]
)

df_clientes.write.format("delta").mode("overwrite").saveAsTable("clientes")

print("Tabla inicial:")
spark.table("clientes").show()`,
        codeLanguage: 'python'
      },
      {
        id: 'lab005-step2',
        title: { es: 'MERGE bÃ¡sico - Upsert', en: 'Basic MERGE - Upsert', pt: 'MERGE bÃ¡sico - Upsert' },
        description: {
          es: 'Actualiza registros existentes e inserta nuevos en una sola operaciÃ³n.',
          en: 'Update existing records and insert new ones in a single operation.',
          pt: 'Atualize registros existentes e insira novos em uma Ãºnica operaÃ§Ã£o.'
        },
        code: `%sql
-- Datos nuevos: Alice cambiÃ³ de tier, Eve es nueva
CREATE OR REPLACE TEMP VIEW updates AS
SELECT * FROM VALUES
    (1, 'Alice Johnson', 'alice@email.com', 'Platinum', '2023-01-15'),
    (5, 'Eve Wilson', 'eve@email.com', 'Silver', '2024-01-10')
AS t(customer_id, name, email, tier, signup_date);

-- MERGE: actualizar si existe, insertar si no
MERGE INTO clientes AS target
USING updates AS source
ON target.customer_id = source.customer_id
WHEN MATCHED THEN
    UPDATE SET *
WHEN NOT MATCHED THEN
    INSERT *;

-- Verificar resultados
SELECT * FROM clientes ORDER BY customer_id;`,
        codeLanguage: 'sql',
        checkpoint: { es: 'Â¿Alice ahora es Platinum? Â¿Se agregÃ³ Eve?', en: 'Is Alice now Platinum? Was Eve added?', pt: 'Alice agora Ã© Platinum? Eve foi adicionada?' }
      },
      {
        id: 'lab005-step3',
        title: { es: 'MERGE con condiciones', en: 'MERGE with conditions', pt: 'MERGE com condiÃ§Ãµes' },
        description: {
          es: 'Agrega condiciones adicionales para controlar cuÃ¡ndo actualizar.',
          en: 'Add additional conditions to control when to update.',
          pt: 'Adicione condiÃ§Ãµes adicionais para controlar quando atualizar.'
        },
        code: `%sql
-- Solo actualizar si el tier es "mejor" (mÃ¡s alto)
CREATE OR REPLACE TEMP VIEW tier_updates AS
SELECT * FROM VALUES
    (2, 'Bob Smith', 'bob.new@email.com', 'Gold', '2023-03-20'),
    (3, 'Charlie Brown', 'charlie@email.com', 'Bronze', '2023-06-10') -- mismo tier
AS t(customer_id, name, email, tier, signup_date);

-- MERGE con condiciÃ³n adicional
MERGE INTO clientes AS target
USING tier_updates AS source
ON target.customer_id = source.customer_id
WHEN MATCHED AND source.tier != target.tier THEN
    UPDATE SET 
        tier = source.tier,
        email = source.email
WHEN MATCHED THEN
    UPDATE SET email = source.email;  -- solo actualizar email si tier es igual

SELECT * FROM clientes ORDER BY customer_id;`,
        codeLanguage: 'sql'
      },
      {
        id: 'lab005-step4',
        title: { es: 'MERGE con DELETE', en: 'MERGE with DELETE', pt: 'MERGE com DELETE' },
        description: {
          es: 'MERGE tambiÃ©n puede eliminar registros.',
          en: 'MERGE can also delete records.',
          pt: 'MERGE tambÃ©m pode deletar registros.'
        },
        code: `%sql
-- Marcar clientes para eliminar con flag is_deleted
CREATE OR REPLACE TEMP VIEW delete_updates AS
SELECT * FROM VALUES
    (4, 'Diana Prince', 'diana@email.com', 'Gold', '2023-02-28', true)
AS t(customer_id, name, email, tier, signup_date, is_deleted);

-- MERGE con DELETE
MERGE INTO clientes AS target
USING delete_updates AS source
ON target.customer_id = source.customer_id
WHEN MATCHED AND source.is_deleted = true THEN
    DELETE
WHEN MATCHED THEN
    UPDATE SET *;

SELECT * FROM clientes ORDER BY customer_id;`,
        codeLanguage: 'sql',
        tip: { es: 'ðŸ’¡ DELETE en MERGE es Ãºtil para CDC', en: 'ðŸ’¡ DELETE in MERGE is useful for CDC', pt: 'ðŸ’¡ DELETE no MERGE Ã© Ãºtil para CDC' }
      },
      {
        id: 'lab005-step5',
        title: { es: 'MERGE con Python API', en: 'MERGE with Python API', pt: 'MERGE com Python API' },
        description: {
          es: 'Usa la API Python de DeltaTable para operaciones mÃ¡s complejas.',
          en: 'Use DeltaTable Python API for more complex operations.',
          pt: 'Use a API Python do DeltaTable para operaÃ§Ãµes mais complexas.'
        },
        code: `from delta.tables import DeltaTable
from pyspark.sql.functions import col, lit, current_timestamp

# Obtener referencia a la tabla Delta
delta_table = DeltaTable.forName(spark, "clientes")

# Nuevos datos
updates_data = [
    (1, "Alice Johnson", "alice.updated@email.com", "Platinum", "2023-01-15"),
    (6, "Frank Miller", "frank@email.com", "Bronze", "2024-01-15")
]
df_updates = spark.createDataFrame(
    updates_data,
    ["customer_id", "name", "email", "tier", "signup_date"]
)

# MERGE usando Python API
delta_table.alias("target").merge(
    df_updates.alias("source"),
    "target.customer_id = source.customer_id"
).whenMatchedUpdate(
    set={
        "email": "source.email",
        "tier": "source.tier"
    }
).whenNotMatchedInsert(
    values={
        "customer_id": "source.customer_id",
        "name": "source.name",
        "email": "source.email",
        "tier": "source.tier",
        "signup_date": "source.signup_date"
    }
).execute()

print("DespuÃ©s del MERGE con Python:")
spark.table("clientes").show()`,
        codeLanguage: 'python'
      },
      {
        id: 'lab005-step6',
        title: { es: 'SCD Type 2 con MERGE', en: 'SCD Type 2 with MERGE', pt: 'SCD Type 2 com MERGE' },
        description: {
          es: 'Implementa Slowly Changing Dimensions Type 2 para mantener historial.',
          en: 'Implement Slowly Changing Dimensions Type 2 to maintain history.',
          pt: 'Implemente Slowly Changing Dimensions Type 2 para manter histÃ³rico.'
        },
        code: `%sql
-- Crear tabla con campos SCD2
CREATE OR REPLACE TABLE clientes_scd2 (
    customer_id INT,
    name STRING,
    email STRING,
    tier STRING,
    effective_date DATE,
    end_date DATE,
    is_current BOOLEAN
) USING DELTA;

-- Insertar datos iniciales
INSERT INTO clientes_scd2
SELECT 
    customer_id, name, email, tier,
    CAST(signup_date AS DATE) as effective_date,
    CAST('9999-12-31' AS DATE) as end_date,
    true as is_current
FROM clientes;

SELECT * FROM clientes_scd2;`,
        codeLanguage: 'sql'
      },
      {
        id: 'lab005-step7',
        title: { es: 'Aplicar cambio SCD2', en: 'Apply SCD2 change', pt: 'Aplicar mudanÃ§a SCD2' },
        description: {
          es: 'Cuando un cliente cambia de tier, mantÃ©n el historial.',
          en: 'When a customer changes tier, maintain the history.',
          pt: 'Quando um cliente muda de tier, mantenha o histÃ³rico.'
        },
        code: `%sql
-- Simular cambio: Bob pasa de Silver a Gold
CREATE OR REPLACE TEMP VIEW scd2_updates AS
SELECT * FROM VALUES
    (2, 'Bob Smith', 'bob@email.com', 'Gold', CAST('2024-01-20' AS DATE))
AS t(customer_id, name, email, new_tier, change_date);

-- MERGE SCD2: cerrar registro viejo, insertar nuevo
MERGE INTO clientes_scd2 AS target
USING scd2_updates AS source
ON target.customer_id = source.customer_id AND target.is_current = true

-- Cerrar el registro actual
WHEN MATCHED AND target.tier != source.new_tier THEN
    UPDATE SET 
        end_date = source.change_date,
        is_current = false;

-- Insertar nuevo registro para Bob
INSERT INTO clientes_scd2 VALUES
    (2, 'Bob Smith', 'bob@email.com', 'Gold', '2024-01-20', '9999-12-31', true);

-- Ver historial completo de Bob
SELECT * FROM clientes_scd2 
WHERE customer_id = 2 
ORDER BY effective_date;`,
        codeLanguage: 'sql',
        checkpoint: { es: 'Â¿Puedes ver ambas versiones de Bob?', en: 'Can you see both versions of Bob?', pt: 'VocÃª consegue ver ambas versÃµes de Bob?' }
      }
    ],
    xpReward: 100,
    badge: {
      id: 'badge-db-merge-master',
      name: { es: 'MERGE Master', en: 'MERGE Master', pt: 'MERGE Master' },
      icon: 'ðŸ”€'
    },
    resources: [
      { title: 'MERGE INTO Documentation', url: 'https://docs.databricks.com/delta/merge.html', type: 'docs' },
      { title: 'DeltaTable Python API', url: 'https://docs.delta.io/latest/api/python/index.html', type: 'docs' }
    ],
    tags: ['intermediate', 'delta-lake', 'merge', 'upsert', 'scd2'],
    services: ['Delta Lake', 'Spark SQL']
  },

  // =====================================================
  // LAB 6: OptimizaciÃ³n de Delta Lake
  // =====================================================
  {
    id: 'db-lab-006',
    title: {
      es: 'âš¡ OptimizaciÃ³n de Delta Lake',
      en: 'âš¡ Delta Lake Optimization',
      pt: 'âš¡ OtimizaÃ§Ã£o do Delta Lake'
    },
    subtitle: {
      es: 'OPTIMIZE, Z-ORDER, VACUUM y mÃ¡s',
      en: 'OPTIMIZE, Z-ORDER, VACUUM and more',
      pt: 'OPTIMIZE, Z-ORDER, VACUUM e mais'
    },
    description: {
      es: 'Aprende a optimizar tus tablas Delta para mÃ¡ximo rendimiento. OPTIMIZE compacta archivos pequeÃ±os, Z-ORDER mejora queries con filtros, y VACUUM limpia archivos obsoletos.',
      en: 'Learn to optimize your Delta tables for maximum performance. OPTIMIZE compacts small files, Z-ORDER improves queries with filters, and VACUUM cleans obsolete files.',
      pt: 'Aprenda a otimizar suas tabelas Delta para mÃ¡ximo desempenho. OPTIMIZE compacta arquivos pequenos, Z-ORDER melhora queries com filtros e VACUUM limpa arquivos obsoletos.'
    },
    difficulty: 'intermediate',
    estimatedMinutes: 40,
    relatedPhases: ['db-phase-5'],
    prerequisites: [
      { es: 'Labs 4-5 completados', en: 'Labs 4-5 completed', pt: 'Labs 4-5 completados' }
    ],
    objectives: [
      { es: 'Usar OPTIMIZE para compactar archivos', en: 'Use OPTIMIZE to compact files', pt: 'Usar OPTIMIZE para compactar arquivos' },
      { es: 'Aplicar Z-ORDER para queries mÃ¡s rÃ¡pidos', en: 'Apply Z-ORDER for faster queries', pt: 'Aplicar Z-ORDER para queries mais rÃ¡pidos' },
      { es: 'Usar VACUUM para limpiar storage', en: 'Use VACUUM to clean storage', pt: 'Usar VACUUM para limpar storage' },
      { es: 'Entender particionamiento', en: 'Understand partitioning', pt: 'Entender particionamento' }
    ],
    steps: [
      {
        id: 'lab006-step1',
        title: { es: 'El problema de archivos pequeÃ±os', en: 'The small files problem', pt: 'O problema de arquivos pequenos' },
        description: {
          es: 'Muchos appends generan archivos pequeÃ±os que hacen los queries lentos.',
          en: 'Many appends generate small files that make queries slow.',
          pt: 'Muitos appends geram arquivos pequenos que tornam queries lentos.'
        },
        code: `# Simular muchos appends pequeÃ±os (problema comÃºn)
from pyspark.sql.functions import rand, expr
import time

# Crear tabla para demo
spark.sql("DROP TABLE IF EXISTS ventas_demo")
spark.sql("""
    CREATE TABLE ventas_demo (
        sale_id BIGINT,
        product STRING,
        category STRING,
        amount DOUBLE,
        sale_date DATE
    ) USING DELTA
""")

# Hacer 10 pequeÃ±os inserts (simula streaming o micro-batches)
for i in range(10):
    spark.sql(f"""
        INSERT INTO ventas_demo
        SELECT 
            {i*100} + monotonically_increasing_id() as sale_id,
            concat('Product_', cast(rand()*10 as int)) as product,
            CASE WHEN rand() < 0.33 THEN 'Electronics' 
                 WHEN rand() < 0.66 THEN 'Clothing' 
                 ELSE 'Home' END as category,
            rand() * 500 as amount,
            date_add('2024-01-01', cast(rand()*30 as int)) as sale_date
        FROM range(100)
    """)

print("Datos insertados en 10 batches pequeÃ±os")`,
        codeLanguage: 'python'
      },
      {
        id: 'lab006-step2',
        title: { es: 'Ver detalles de archivos', en: 'View file details', pt: 'Ver detalhes de arquivos' },
        description: {
          es: 'Usa DESCRIBE DETAIL para ver cuÃ¡ntos archivos tiene tu tabla.',
          en: 'Use DESCRIBE DETAIL to see how many files your table has.',
          pt: 'Use DESCRIBE DETAIL para ver quantos arquivos sua tabela tem.'
        },
        code: `%sql
-- Ver detalles de la tabla
DESCRIBE DETAIL ventas_demo;

-- Ver nÃºmero de archivos (antes de OPTIMIZE)
SELECT numFiles, sizeInBytes/1024 as sizeKB 
FROM (DESCRIBE DETAIL ventas_demo);`,
        codeLanguage: 'sql',
        tip: { es: 'ðŸ’¡ Muchos archivos pequeÃ±os = queries lentos', en: 'ðŸ’¡ Many small files = slow queries', pt: 'ðŸ’¡ Muitos arquivos pequenos = queries lentos' }
      },
      {
        id: 'lab006-step3',
        title: { es: 'OPTIMIZE: Compactar archivos', en: 'OPTIMIZE: Compact files', pt: 'OPTIMIZE: Compactar arquivos' },
        description: {
          es: 'OPTIMIZE combina archivos pequeÃ±os en archivos mÃ¡s grandes y eficientes.',
          en: 'OPTIMIZE combines small files into larger, more efficient files.',
          pt: 'OPTIMIZE combina arquivos pequenos em arquivos maiores e mais eficientes.'
        },
        code: `%sql
-- Compactar archivos
OPTIMIZE ventas_demo;

-- Ver resultado
SELECT numFiles, sizeInBytes/1024 as sizeKB 
FROM (DESCRIBE DETAIL ventas_demo);

-- TambiÃ©n podemos ver las mÃ©tricas de la operaciÃ³n
DESCRIBE HISTORY ventas_demo LIMIT 1;`,
        codeLanguage: 'sql',
        checkpoint: { es: 'Â¿CuÃ¡ntos archivos hay ahora vs antes?', en: 'How many files are there now vs before?', pt: 'Quantos arquivos hÃ¡ agora vs antes?' }
      },
      {
        id: 'lab006-step4',
        title: { es: 'Z-ORDER: Clustering de datos', en: 'Z-ORDER: Data clustering', pt: 'Z-ORDER: Clustering de dados' },
        description: {
          es: 'Z-ORDER agrupa datos por columnas de filtro frecuente, mejorando data skipping.',
          en: 'Z-ORDER groups data by frequently filtered columns, improving data skipping.',
          pt: 'Z-ORDER agrupa dados por colunas de filtro frequente, melhorando data skipping.'
        },
        code: `%sql
-- OPTIMIZE con Z-ORDER en columna de filtro frecuente
OPTIMIZE ventas_demo ZORDER BY (category);

-- Ahora los queries que filtran por category son mÃ¡s rÃ¡pidos
-- porque Delta Lake puede "saltar" archivos que no tienen la categorÃ­a buscada

-- Query de ejemplo (observa el tiempo de ejecuciÃ³n)
SELECT category, SUM(amount) as total
FROM ventas_demo
WHERE category = 'Electronics'
GROUP BY category;`,
        codeLanguage: 'sql',
        tip: { es: 'ðŸ’¡ Z-ORDER en columnas que usas frecuentemente en WHERE', en: 'ðŸ’¡ Z-ORDER on columns you frequently use in WHERE', pt: 'ðŸ’¡ Z-ORDER em colunas que vocÃª usa frequentemente em WHERE' }
      },
      {
        id: 'lab006-step5',
        title: { es: 'VACUUM: Limpiar archivos obsoletos', en: 'VACUUM: Clean obsolete files', pt: 'VACUUM: Limpar arquivos obsoletos' },
        description: {
          es: 'VACUUM elimina archivos que ya no son necesarios (de versiones antiguas).',
          en: 'VACUUM removes files that are no longer needed (from old versions).',
          pt: 'VACUUM remove arquivos que nÃ£o sÃ£o mais necessÃ¡rios (de versÃµes antigas).'
        },
        code: `%sql
-- Ver historial primero
DESCRIBE HISTORY ventas_demo;

-- VACUUM por defecto mantiene 7 dÃ­as de historial
-- Para demo, usamos 0 horas (NO HACER EN PRODUCCIÃ“N)
SET spark.databricks.delta.retentionDurationCheck.enabled = false;

VACUUM ventas_demo RETAIN 0 HOURS;

-- En producciÃ³n usarÃ­as:
-- VACUUM ventas_demo RETAIN 168 HOURS; -- 7 dÃ­as

SET spark.databricks.delta.retentionDurationCheck.enabled = true;`,
        codeLanguage: 'sql',
        warning: { es: 'âš ï¸ VACUUM con 0 HOURS elimina la posibilidad de time travel', en: 'âš ï¸ VACUUM with 0 HOURS removes time travel capability', pt: 'âš ï¸ VACUUM com 0 HOURS remove a capacidade de time travel' }
      },
      {
        id: 'lab006-step6',
        title: { es: 'Particionamiento', en: 'Partitioning', pt: 'Particionamento' },
        description: {
          es: 'Las particiones dividen datos fÃ­sicamente para queries mÃ¡s eficientes.',
          en: 'Partitions physically divide data for more efficient queries.',
          pt: 'PartiÃ§Ãµes dividem dados fisicamente para queries mais eficientes.'
        },
        code: `# Crear tabla particionada por fecha
spark.sql("""
    CREATE TABLE ventas_particionada (
        sale_id BIGINT,
        product STRING,
        category STRING,
        amount DOUBLE,
        sale_date DATE
    ) USING DELTA
    PARTITIONED BY (sale_date)
""")

# Insertar datos
spark.sql("""
    INSERT INTO ventas_particionada
    SELECT * FROM ventas_demo
""")

# Ver particiones
spark.sql("SHOW PARTITIONS ventas_particionada").show()

# Query que usa partition pruning
spark.sql("""
    SELECT * FROM ventas_particionada
    WHERE sale_date = '2024-01-15'
""").explain()  # Ver que solo lee 1 particiÃ³n`,
        codeLanguage: 'python',
        tip: { es: 'ðŸ’¡ Particiona por columnas con alta cardinalidad que usas en filtros', en: 'ðŸ’¡ Partition by high cardinality columns you use in filters', pt: 'ðŸ’¡ Particione por colunas de alta cardinalidade que vocÃª usa em filtros' }
      },
      {
        id: 'lab006-step7',
        title: { es: 'Resumen de buenas prÃ¡cticas', en: 'Best practices summary', pt: 'Resumo de boas prÃ¡ticas' },
        description: {
          es: 'Checklist de optimizaciÃ³n para tablas Delta en producciÃ³n.',
          en: 'Optimization checklist for Delta tables in production.',
          pt: 'Checklist de otimizaÃ§Ã£o para tabelas Delta em produÃ§Ã£o.'
        },
        code: `# CHECKLIST DE OPTIMIZACIÃ“N DELTA LAKE
# 
# 1. OPTIMIZE regularmente (diario o semanal)
#    - Compacta archivos pequeÃ±os
#    - Mejora performance de lectura
#
# 2. Z-ORDER en columnas de filtro frecuente
#    - MÃ¡ximo 3-4 columnas
#    - Las mÃ¡s selectivas primero
#
# 3. VACUUM para liberar storage
#    - MÃ­nimo 7 dÃ­as de retenciÃ³n en producciÃ³n
#    - Programar semanal/mensual
#
# 4. Particionamiento inteligente
#    - Por fecha/timestamp es comÃºn
#    - Evitar over-partitioning (muchas particiones pequeÃ±as)
#    - Ideal: particiones de 1GB+
#
# 5. Auto Optimize (en Databricks)
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")

print("âœ… Auto Optimize habilitado para nuevas escrituras")`,
        codeLanguage: 'python'
      }
    ],
    xpReward: 100,
    badge: {
      id: 'badge-db-optimizer',
      name: { es: 'Delta Optimizer', en: 'Delta Optimizer', pt: 'Delta Optimizer' },
      icon: 'âš¡'
    },
    resources: [
      { title: 'OPTIMIZE Documentation', url: 'https://docs.databricks.com/delta/optimize.html', type: 'docs' },
      { title: 'Z-ORDER Documentation', url: 'https://docs.databricks.com/delta/data-skipping.html', type: 'docs' },
      { title: 'VACUUM Documentation', url: 'https://docs.databricks.com/delta/vacuum.html', type: 'docs' }
    ],
    tags: ['intermediate', 'delta-lake', 'optimize', 'zorder', 'vacuum', 'performance'],
    services: ['Delta Lake', 'Spark']
  }
];

