/**
 * FASE 6: AMAZON REDSHIFT - DATA WAREHOUSE
 * 9 pasos para dominar Redshift
 */
import { AWSStep } from '../types';

export const phase6Steps: AWSStep[] = [
  { id: 'aws-6-1', stepNumber: 46, title: { es: 'Introducci√≥n a Amazon Redshift', en: 'Introduction to Amazon Redshift', pt: 'Introdu√ß√£o ao Amazon Redshift' }, description: { es: 'Entender arquitectura MPP y casos de uso.', en: 'Understand MPP architecture and use cases.', pt: 'Entender arquitetura MPP e casos de uso.' }, theory: { es: `## Amazon Redshift\n\n### Arquitectura MPP (Massively Parallel Processing)\n- **Leader Node**: Recibe queries, planifica, coordina\n- **Compute Nodes**: Ejecutan queries en paralelo\n- **Slices**: Subdivisiones de cada nodo\n\n### Tipos de nodos\n| Tipo | vCPU | RAM | Storage | Uso |\n|------|------|-----|---------|-----|\n| dc2.large | 2 | 15GB | 160GB SSD | Dev/Test |\n| dc2.8xlarge | 32 | 244GB | 2.56TB SSD | Production |\n| ra3.xlplus | 4 | 32GB | Managed | Scalable |\n| ra3.4xlarge | 12 | 96GB | Managed | Production |\n\n### Redshift Serverless\nPaga por capacidad usada (RPUs), sin gesti√≥n de clusters.`, en: `## Amazon Redshift\n\n### MPP Architecture (Massively Parallel Processing)\n- **Leader Node**: Receives queries, plans, coordinates\n- **Compute Nodes**: Execute queries in parallel\n- **Slices**: Subdivisions of each node\n\n### Node types\n| Type | vCPU | RAM | Storage | Use |\n|------|------|-----|---------|-----|\n| dc2.large | 2 | 15GB | 160GB SSD | Dev/Test |\n| dc2.8xlarge | 32 | 244GB | 2.56TB SSD | Production |\n| ra3.xlplus | 4 | 32GB | Managed | Scalable |\n| ra3.4xlarge | 12 | 96GB | Managed | Production |\n\n### Redshift Serverless\nPay for used capacity (RPUs), no cluster management.`, pt: `## Amazon Redshift\n\n### Arquitetura MPP (Massively Parallel Processing)\n- **Leader Node**: Recebe queries, planeja, coordena\n- **Compute Nodes**: Executam queries em paralelo\n- **Slices**: Subdivis√µes de cada n√≥\n\n### Tipos de n√≥s\n| Tipo | vCPU | RAM | Storage | Uso |\n|------|------|-----|---------|-----|\n| dc2.large | 2 | 15GB | 160GB SSD | Dev/Test |\n| dc2.8xlarge | 32 | 244GB | 2.56TB SSD | Production |\n| ra3.xlplus | 4 | 32GB | Managed | Scalable |\n| ra3.4xlarge | 12 | 96GB | Managed | Production |\n\n### Redshift Serverless\nPaga por capacidade usada (RPUs), sem gest√£o de clusters.` }, practicalTips: [{ es: 'üöÄ Usa Redshift Serverless para desarrollo y peque√±os workloads', en: 'üöÄ Use Redshift Serverless for development and small workloads', pt: 'üöÄ Use Redshift Serverless para desenvolvimento e pequenos workloads' }], externalLinks: [{ title: 'Redshift Documentation', url: 'https://docs.aws.amazon.com/redshift/latest/dg/welcome.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øEntiendes la diferencia entre Leader y Compute nodes?', en: '‚úÖ Do you understand the difference between Leader and Compute nodes?', pt: '‚úÖ Voc√™ entende a diferen√ßa entre Leader e Compute nodes?' }, xpReward: 55, estimatedMinutes: 30, services: ['Redshift'] },
  
  { id: 'aws-6-2', stepNumber: 47, title: { es: 'Crear cluster Redshift', en: 'Create Redshift cluster', pt: 'Criar cluster Redshift' }, description: { es: 'Configurar un cluster o Redshift Serverless.', en: 'Configure a cluster or Redshift Serverless.', pt: 'Configurar um cluster ou Redshift Serverless.' }, theory: { es: `## Crear Cluster Redshift\n\n### Opciones de creaci√≥n\n1. **Provisioned**: Cluster con nodos fijos\n2. **Serverless**: Capacidad autom√°tica\n\n### Configuraci√≥n b√°sica\n\`\`\`yaml\nCluster: my-dwh-cluster\n  Node type: dc2.large\n  Number of nodes: 2\n  Database name: analytics\n  Admin user: admin\n  VPC: vpc-prod\n  Security Group: sg-redshift\n  Encrypted: Yes (KMS)\n  Enhanced VPC Routing: Yes\n\`\`\`\n\n### Conexi√≥n\n\`\`\`bash\n# Con psql\npsql -h cluster.xyz.us-east-1.redshift.amazonaws.com -p 5439 -U admin -d analytics\n\n# JDBC URL\njdbc:redshift://cluster.xyz.us-east-1.redshift.amazonaws.com:5439/analytics\n\`\`\``, en: `## Create Redshift Cluster\n\n### Creation options\n1. **Provisioned**: Cluster with fixed nodes\n2. **Serverless**: Automatic capacity\n\n### Basic configuration\n\`\`\`yaml\nCluster: my-dwh-cluster\n  Node type: dc2.large\n  Number of nodes: 2\n  Database name: analytics\n  Admin user: admin\n  VPC: vpc-prod\n  Security Group: sg-redshift\n  Encrypted: Yes (KMS)\n  Enhanced VPC Routing: Yes\n\`\`\`\n\n### Connection\n\`\`\`bash\n# With psql\npsql -h cluster.xyz.us-east-1.redshift.amazonaws.com -p 5439 -U admin -d analytics\n\n# JDBC URL\njdbc:redshift://cluster.xyz.us-east-1.redshift.amazonaws.com:5439/analytics\n\`\`\``, pt: `## Criar Cluster Redshift\n\n### Op√ß√µes de cria√ß√£o\n1. **Provisioned**: Cluster com n√≥s fixos\n2. **Serverless**: Capacidade autom√°tica\n\n### Configura√ß√£o b√°sica\n\`\`\`yaml\nCluster: my-dwh-cluster\n  Node type: dc2.large\n  Number of nodes: 2\n  Database name: analytics\n  Admin user: admin\n  VPC: vpc-prod\n  Security Group: sg-redshift\n  Encrypted: Yes (KMS)\n  Enhanced VPC Routing: Yes\n\`\`\`\n\n### Conex√£o\n\`\`\`bash\n# Com psql\npsql -h cluster.xyz.us-east-1.redshift.amazonaws.com -p 5439 -U admin -d analytics\n\n# JDBC URL\njdbc:redshift://cluster.xyz.us-east-1.redshift.amazonaws.com:5439/analytics\n\`\`\`` }, practicalTips: [{ es: 'üí∞ dc2.large es suficiente para aprender - NO crees clusters grandes', en: 'üí∞ dc2.large is enough for learning - DON\'T create large clusters', pt: 'üí∞ dc2.large √© suficiente para aprender - N√ÉO crie clusters grandes' }], externalLinks: [{ title: 'Getting Started with Redshift', url: 'https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øCreaste un cluster Serverless o dc2.large y te conectaste?', en: '‚úÖ Did you create a Serverless or dc2.large cluster and connect?', pt: '‚úÖ Voc√™ criou um cluster Serverless ou dc2.large e se conectou?' }, xpReward: 60, estimatedMinutes: 35, services: ['Redshift'] },
  
  { id: 'aws-6-3', stepNumber: 48, title: { es: 'COPY: Carga masiva de datos', en: 'COPY: Bulk data loading', pt: 'COPY: Carga massiva de dados' }, description: { es: 'Cargar datos desde S3 con COPY command.', en: 'Load data from S3 with COPY command.', pt: 'Carregar dados do S3 com COPY command.' }, theory: { es: `## COPY Command - Carga de Datos\n\n### Sintaxis b√°sica\n\`\`\`sql\nCOPY sales\nFROM 's3://bucket/data/sales/'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftS3Role'\nFORMAT AS PARQUET;\n\`\`\`\n\n### Opciones comunes\n\`\`\`sql\nCOPY users\nFROM 's3://bucket/users.csv'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nCSV\nIGNOREHEADER 1\nDELIMITER ','\nREGION 'us-east-1'\nGZIP\nMAXERROR 100;\n\`\`\`\n\n### Best practices\n1. **Divide archivos**: M√∫ltiples archivos = carga paralela\n2. **Usa MANIFEST**: Lista exacta de archivos\n3. **Comprime datos**: Gzip reduce tiempo de transferencia\n4. **Parquet > CSV**: M√°s eficiente`, en: `## COPY Command - Data Loading\n\n### Basic syntax\n\`\`\`sql\nCOPY sales\nFROM 's3://bucket/data/sales/'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftS3Role'\nFORMAT AS PARQUET;\n\`\`\`\n\n### Common options\n\`\`\`sql\nCOPY users\nFROM 's3://bucket/users.csv'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nCSV\nIGNOREHEADER 1\nDELIMITER ','\nREGION 'us-east-1'\nGZIP\nMAXERROR 100;\n\`\`\`\n\n### Best practices\n1. **Split files**: Multiple files = parallel load\n2. **Use MANIFEST**: Exact file list\n3. **Compress data**: Gzip reduces transfer time\n4. **Parquet > CSV**: More efficient`, pt: `## COPY Command - Carga de Dados\n\n### Sintaxe b√°sica\n\`\`\`sql\nCOPY sales\nFROM 's3://bucket/data/sales/'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftS3Role'\nFORMAT AS PARQUET;\n\`\`\`\n\n### Op√ß√µes comuns\n\`\`\`sql\nCOPY users\nFROM 's3://bucket/users.csv'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nCSV\nIGNOREHEADER 1\nDELIMITER ','\nREGION 'us-east-1'\nGZIP\nMAXERROR 100;\n\`\`\`\n\n### Best practices\n1. **Divida arquivos**: M√∫ltiplos arquivos = carga paralela\n2. **Use MANIFEST**: Lista exata de arquivos\n3. **Comprima dados**: Gzip reduz tempo de transfer√™ncia\n4. **Parquet > CSV**: Mais eficiente` }, practicalTips: [{ es: '‚ö° Divide archivos grandes en partes del mismo tama√±o para carga paralela √≥ptima', en: '‚ö° Split large files into same-size parts for optimal parallel loading', pt: '‚ö° Divida arquivos grandes em partes do mesmo tamanho para carga paralela √≥tima' }], externalLinks: [{ title: 'COPY Command Reference', url: 'https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øCargaste datos de S3 a Redshift con COPY?', en: '‚úÖ Did you load data from S3 to Redshift with COPY?', pt: '‚úÖ Voc√™ carregou dados do S3 para Redshift com COPY?' }, xpReward: 65, estimatedMinutes: 40, services: ['Redshift', 'S3'] },
  
  { id: 'aws-6-4', stepNumber: 49, title: { es: 'Distribution y Sort Keys', en: 'Distribution and Sort Keys', pt: 'Distribution e Sort Keys' }, description: { es: 'Optimizar tablas con distribution y sort keys.', en: 'Optimize tables with distribution and sort keys.', pt: 'Otimizar tabelas com distribution e sort keys.' }, theory: { es: `## Distribution y Sort Keys\n\n### Distribution Styles\n1. **AUTO**: Redshift elige (recomendado para empezar)\n2. **EVEN**: Distribuye filas uniformemente\n3. **KEY**: Por valor de columna (bueno para JOINs)\n4. **ALL**: Copia tabla a todos los nodos (tablas peque√±as)\n\n### Sort Keys\n1. **COMPOUND**: M√∫ltiples columnas, orden importa\n2. **INTERLEAVED**: M√∫ltiples columnas, orden no importa (deprecated)\n\n### Ejemplo optimizado\n\`\`\`sql\nCREATE TABLE sales (\n  sale_id BIGINT,\n  customer_id INT,\n  sale_date DATE,\n  amount DECIMAL(10,2)\n)\nDISTKEY(customer_id)  -- JOIN frecuente con customers\nSORTKEY(sale_date);   -- Filtro frecuente por fecha\n\`\`\`\n\n### Cu√°ndo usar qu√©\n- **DISTKEY**: Columna de JOIN m√°s frecuente\n- **SORTKEY**: Columna de filtro m√°s frecuente`, en: `## Distribution and Sort Keys\n\n### Distribution Styles\n1. **AUTO**: Redshift chooses (recommended to start)\n2. **EVEN**: Distributes rows evenly\n3. **KEY**: By column value (good for JOINs)\n4. **ALL**: Copies table to all nodes (small tables)\n\n### Sort Keys\n1. **COMPOUND**: Multiple columns, order matters\n2. **INTERLEAVED**: Multiple columns, order doesn't matter (deprecated)\n\n### Optimized example\n\`\`\`sql\nCREATE TABLE sales (\n  sale_id BIGINT,\n  customer_id INT,\n  sale_date DATE,\n  amount DECIMAL(10,2)\n)\nDISTKEY(customer_id)  -- Frequent JOIN with customers\nSORTKEY(sale_date);   -- Frequent filter by date\n\`\`\`\n\n### When to use what\n- **DISTKEY**: Most frequent JOIN column\n- **SORTKEY**: Most frequent filter column`, pt: `## Distribution e Sort Keys\n\n### Distribution Styles\n1. **AUTO**: Redshift escolhe (recomendado para come√ßar)\n2. **EVEN**: Distribui linhas uniformemente\n3. **KEY**: Por valor de coluna (bom para JOINs)\n4. **ALL**: Copia tabela para todos os n√≥s (tabelas pequenas)\n\n### Sort Keys\n1. **COMPOUND**: M√∫ltiplas colunas, ordem importa\n2. **INTERLEAVED**: M√∫ltiplas colunas, ordem n√£o importa (deprecated)\n\n### Exemplo otimizado\n\`\`\`sql\nCREATE TABLE sales (\n  sale_id BIGINT,\n  customer_id INT,\n  sale_date DATE,\n  amount DECIMAL(10,2)\n)\nDISTKEY(customer_id)  -- JOIN frequente com customers\nSORTKEY(sale_date);   -- Filtro frequente por data\n\`\`\`\n\n### Quando usar o qu√™\n- **DISTKEY**: Coluna de JOIN mais frequente\n- **SORTKEY**: Coluna de filtro mais frequente` }, practicalTips: [{ es: 'üìä Analiza tus queries m√°s frecuentes antes de elegir keys', en: 'üìä Analyze your most frequent queries before choosing keys', pt: 'üìä Analise suas queries mais frequentes antes de escolher keys' }], externalLinks: [{ title: 'Table Design Best Practices', url: 'https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øCreaste tablas con DISTKEY y SORTKEY apropiados?', en: '‚úÖ Did you create tables with appropriate DISTKEY and SORTKEY?', pt: '‚úÖ Voc√™ criou tabelas com DISTKEY e SORTKEY apropriados?' }, xpReward: 70, estimatedMinutes: 45, services: ['Redshift'] },
  
  { id: 'aws-6-5', stepNumber: 50, title: { es: 'Redshift Spectrum', en: 'Redshift Spectrum', pt: 'Redshift Spectrum' }, description: { es: 'Consultar datos en S3 directamente desde Redshift.', en: 'Query data in S3 directly from Redshift.', pt: 'Consultar dados no S3 diretamente do Redshift.' }, theory: { es: `## Redshift Spectrum - Data Lakehouse\n\n### ¬øQu√© es?\nExtensi√≥n que permite ejecutar queries sobre S3 desde Redshift, combinando datos del warehouse con el data lake.\n\n### Crear external schema\n\`\`\`sql\nCREATE EXTERNAL SCHEMA spectrum_schema\nFROM DATA CATALOG\nDATABASE 'glue_database'\nIAM_ROLE 'arn:aws:iam::123:role/SpectrumRole'\nREGION 'us-east-1';\n\`\`\`\n\n### Query combinada\n\`\`\`sql\n-- Tabla interna + S3 via Spectrum\nSELECT \n  r.customer_name,\n  SUM(s.amount) as total\nFROM redshift_schema.customers r\nJOIN spectrum_schema.sales_history s ON r.id = s.customer_id\nGROUP BY r.customer_name;\n\`\`\`\n\n### Pricing\n- $5 por TB escaneado en S3 (igual que Athena)`, en: `## Redshift Spectrum - Data Lakehouse\n\n### What is it?\nExtension that allows running queries on S3 from Redshift, combining warehouse data with data lake.\n\n### Create external schema\n\`\`\`sql\nCREATE EXTERNAL SCHEMA spectrum_schema\nFROM DATA CATALOG\nDATABASE 'glue_database'\nIAM_ROLE 'arn:aws:iam::123:role/SpectrumRole'\nREGION 'us-east-1';\n\`\`\`\n\n### Combined query\n\`\`\`sql\n-- Internal table + S3 via Spectrum\nSELECT \n  r.customer_name,\n  SUM(s.amount) as total\nFROM redshift_schema.customers r\nJOIN spectrum_schema.sales_history s ON r.id = s.customer_id\nGROUP BY r.customer_name;\n\`\`\`\n\n### Pricing\n- $5 per TB scanned in S3 (same as Athena)`, pt: `## Redshift Spectrum - Data Lakehouse\n\n### O que √©?\nExtens√£o que permite executar queries sobre S3 do Redshift, combinando dados do warehouse com o data lake.\n\n### Criar external schema\n\`\`\`sql\nCREATE EXTERNAL SCHEMA spectrum_schema\nFROM DATA CATALOG\nDATABASE 'glue_database'\nIAM_ROLE 'arn:aws:iam::123:role/SpectrumRole'\nREGION 'us-east-1';\n\`\`\`\n\n### Query combinada\n\`\`\`sql\n-- Tabela interna + S3 via Spectrum\nSELECT \n  r.customer_name,\n  SUM(s.amount) as total\nFROM redshift_schema.customers r\nJOIN spectrum_schema.sales_history s ON r.id = s.customer_id\nGROUP BY r.customer_name;\n\`\`\`\n\n### Pricing\n- $5 por TB escaneado no S3 (igual ao Athena)` }, practicalTips: [{ es: 'üè† Spectrum es ideal para Data Lakehouse - datos calientes en Redshift, hist√≥ricos en S3', en: 'üè† Spectrum is ideal for Data Lakehouse - hot data in Redshift, historical in S3', pt: 'üè† Spectrum √© ideal para Data Lakehouse - dados quentes no Redshift, hist√≥ricos no S3' }], externalLinks: [{ title: 'Redshift Spectrum', url: 'https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øConfiguraste Spectrum y consultaste datos de S3?', en: '‚úÖ Did you configure Spectrum and query S3 data?', pt: '‚úÖ Voc√™ configurou Spectrum e consultou dados do S3?' }, xpReward: 65, estimatedMinutes: 40, services: ['Redshift', 'Glue Data Catalog'] },
  
  { id: 'aws-6-6', stepNumber: 51, title: { es: 'UNLOAD: Exportar datos a S3', en: 'UNLOAD: Export data to S3', pt: 'UNLOAD: Exportar dados para S3' }, description: { es: 'Exportar resultados de queries a S3.', en: 'Export query results to S3.', pt: 'Exportar resultados de queries para S3.' }, theory: { es: `## UNLOAD - Exportar a S3\n\n### Sintaxis b√°sica\n\`\`\`sql\nUNLOAD ('SELECT * FROM sales WHERE year = 2024')\nTO 's3://bucket/exports/sales_2024_'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nPARQUET\nPARTITION BY (region)\nALLOWOVERWRITE;\n\`\`\`\n\n### Opciones √∫tiles\n- **PARQUET/CSV**: Formato de output\n- **PARTITION BY**: Particionar output\n- **PARALLEL ON/OFF**: Control de paralelismo\n- **HEADER**: Incluir headers (CSV)\n- **GZIP**: Comprimir output`, en: `## UNLOAD - Export to S3\n\n### Basic syntax\n\`\`\`sql\nUNLOAD ('SELECT * FROM sales WHERE year = 2024')\nTO 's3://bucket/exports/sales_2024_'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nPARQUET\nPARTITION BY (region)\nALLOWOVERWRITE;\n\`\`\`\n\n### Useful options\n- **PARQUET/CSV**: Output format\n- **PARTITION BY**: Partition output\n- **PARALLEL ON/OFF**: Parallelism control\n- **HEADER**: Include headers (CSV)\n- **GZIP**: Compress output`, pt: `## UNLOAD - Exportar para S3\n\n### Sintaxe b√°sica\n\`\`\`sql\nUNLOAD ('SELECT * FROM sales WHERE year = 2024')\nTO 's3://bucket/exports/sales_2024_'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nPARQUET\nPARTITION BY (region)\nALLOWOVERWRITE;\n\`\`\`\n\n### Op√ß√µes √∫teis\n- **PARQUET/CSV**: Formato de output\n- **PARTITION BY**: Particionar output\n- **PARALLEL ON/OFF**: Controle de paralelismo\n- **HEADER**: Incluir headers (CSV)\n- **GZIP**: Comprimir output` }, practicalTips: [{ es: 'üì§ UNLOAD es m√°s eficiente que SELECT INTO S3 para grandes vol√∫menes', en: 'üì§ UNLOAD is more efficient than SELECT INTO S3 for large volumes', pt: 'üì§ UNLOAD √© mais eficiente que SELECT INTO S3 para grandes volumes' }], externalLinks: [{ title: 'UNLOAD Reference', url: 'https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øExportaste datos de Redshift a S3 con UNLOAD?', en: '‚úÖ Did you export data from Redshift to S3 with UNLOAD?', pt: '‚úÖ Voc√™ exportou dados do Redshift para S3 com UNLOAD?' }, xpReward: 50, estimatedMinutes: 25, services: ['Redshift', 'S3'] },
  
  { id: 'aws-6-7', stepNumber: 52, title: { es: 'VACUUM y ANALYZE', en: 'VACUUM and ANALYZE', pt: 'VACUUM e ANALYZE' }, description: { es: 'Mantenimiento de tablas para performance √≥ptima.', en: 'Table maintenance for optimal performance.', pt: 'Manuten√ß√£o de tabelas para performance √≥tima.' }, theory: { es: `## Mantenimiento de Tablas\n\n### VACUUM\nRecupera espacio de filas eliminadas y reordena datos:\n\`\`\`sql\n-- VACUUM completo\nVACUUM FULL sales;\n\n-- Solo ordenar\nVACUUM SORT ONLY sales;\n\n-- Solo recuperar espacio\nVACUUM DELETE ONLY sales;\n\`\`\`\n\n### ANALYZE\nActualiza estad√≠sticas para el query planner:\n\`\`\`sql\nANALYZE sales;\nANALYZE PREDICATE COLUMNS sales;  -- Solo columnas usadas en predicados\n\`\`\`\n\n### Auto-mantenimiento\nRedshift hace VACUUM y ANALYZE autom√°ticamente, pero puedes forzarlos despu√©s de cargas grandes.`, en: `## Table Maintenance\n\n### VACUUM\nRecovers space from deleted rows and reorders data:\n\`\`\`sql\n-- Full VACUUM\nVACUUM FULL sales;\n\n-- Sort only\nVACUUM SORT ONLY sales;\n\n-- Delete only\nVACUUM DELETE ONLY sales;\n\`\`\`\n\n### ANALYZE\nUpdates statistics for query planner:\n\`\`\`sql\nANALYZE sales;\nANALYZE PREDICATE COLUMNS sales;  -- Only columns used in predicates\n\`\`\`\n\n### Auto-maintenance\nRedshift does VACUUM and ANALYZE automatically, but you can force them after large loads.`, pt: `## Manuten√ß√£o de Tabelas\n\n### VACUUM\nRecupera espa√ßo de linhas deletadas e reordena dados:\n\`\`\`sql\n-- VACUUM completo\nVACUUM FULL sales;\n\n-- S√≥ ordenar\nVACUUM SORT ONLY sales;\n\n-- S√≥ recuperar espa√ßo\nVACUUM DELETE ONLY sales;\n\`\`\`\n\n### ANALYZE\nAtualiza estat√≠sticas para o query planner:\n\`\`\`sql\nANALYZE sales;\nANALYZE PREDICATE COLUMNS sales;  -- S√≥ colunas usadas em predicados\n\`\`\`\n\n### Auto-manuten√ß√£o\nRedshift faz VACUUM e ANALYZE automaticamente, mas voc√™ pode for√ß√°-los ap√≥s cargas grandes.` }, practicalTips: [{ es: 'üîß Ejecuta VACUUM despu√©s de cargas grandes de datos', en: 'üîß Run VACUUM after large data loads', pt: 'üîß Execute VACUUM ap√≥s cargas grandes de dados' }], externalLinks: [{ title: 'VACUUM Command', url: 'https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øEjecutaste VACUUM y ANALYZE en una tabla?', en: '‚úÖ Did you run VACUUM and ANALYZE on a table?', pt: '‚úÖ Voc√™ executou VACUUM e ANALYZE em uma tabela?' }, xpReward: 45, estimatedMinutes: 25, services: ['Redshift'] },
  
  { id: 'aws-6-8', stepNumber: 53, title: { es: 'Query Performance y Workload Management', en: 'Query Performance and Workload Management', pt: 'Query Performance e Workload Management' }, description: { es: 'Diagnosticar y optimizar performance de queries.', en: 'Diagnose and optimize query performance.', pt: 'Diagnosticar e otimizar performance de queries.' }, theory: { es: `## Query Performance\n\n### Herramientas de diagn√≥stico\n\`\`\`sql\n-- Ver plan de ejecuci√≥n\nEXPLAIN SELECT * FROM sales WHERE date > '2024-01-01';\n\n-- Ver queries lentas\nSELECT * FROM STL_QUERY\nWHERE elapsed > 60000000 -- m√°s de 60 segundos\nORDER BY elapsed DESC;\n\n-- Ver locks\nSELECT * FROM SVV_TRANSACTIONS WHERE lockable_object_type = 'relation';\n\`\`\`\n\n### WLM (Workload Management)\nConfigura colas con diferentes prioridades:\n\`\`\`yaml\nQueues:\n  - Name: ETL\n    Memory: 40%\n    Concurrency: 5\n    User Groups: [etl_users]\n  - Name: BI\n    Memory: 50%\n    Concurrency: 15\n    User Groups: [analysts]\n  - Name: Default\n    Memory: 10%\n    Concurrency: 5\n\`\`\``, en: `## Query Performance\n\n### Diagnostic tools\n\`\`\`sql\n-- View execution plan\nEXPLAIN SELECT * FROM sales WHERE date > '2024-01-01';\n\n-- View slow queries\nSELECT * FROM STL_QUERY\nWHERE elapsed > 60000000 -- more than 60 seconds\nORDER BY elapsed DESC;\n\n-- View locks\nSELECT * FROM SVV_TRANSACTIONS WHERE lockable_object_type = 'relation';\n\`\`\`\n\n### WLM (Workload Management)\nConfigure queues with different priorities:\n\`\`\`yaml\nQueues:\n  - Name: ETL\n    Memory: 40%\n    Concurrency: 5\n    User Groups: [etl_users]\n  - Name: BI\n    Memory: 50%\n    Concurrency: 15\n    User Groups: [analysts]\n  - Name: Default\n    Memory: 10%\n    Concurrency: 5\n\`\`\``, pt: `## Query Performance\n\n### Ferramentas de diagn√≥stico\n\`\`\`sql\n-- Ver plano de execu√ß√£o\nEXPLAIN SELECT * FROM sales WHERE date > '2024-01-01';\n\n-- Ver queries lentas\nSELECT * FROM STL_QUERY\nWHERE elapsed > 60000000 -- mais de 60 segundos\nORDER BY elapsed DESC;\n\n-- Ver locks\nSELECT * FROM SVV_TRANSACTIONS WHERE lockable_object_type = 'relation';\n\`\`\`\n\n### WLM (Workload Management)\nConfigure filas com diferentes prioridades:\n\`\`\`yaml\nQueues:\n  - Name: ETL\n    Memory: 40%\n    Concurrency: 5\n    User Groups: [etl_users]\n  - Name: BI\n    Memory: 50%\n    Concurrency: 15\n    User Groups: [analysts]\n  - Name: Default\n    Memory: 10%\n    Concurrency: 5\n\`\`\`` }, practicalTips: [{ es: 'üìà Usa STL_QUERY y STL_QUERYTEXT para analizar queries problem√°ticas', en: 'üìà Use STL_QUERY and STL_QUERYTEXT to analyze problematic queries', pt: 'üìà Use STL_QUERY e STL_QUERYTEXT para analisar queries problem√°ticas' }], externalLinks: [{ title: 'Query Performance Tuning', url: 'https://docs.aws.amazon.com/redshift/latest/dg/c-optimizing-query-performance.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øUsaste EXPLAIN para analizar un query?', en: '‚úÖ Did you use EXPLAIN to analyze a query?', pt: '‚úÖ Voc√™ usou EXPLAIN para analisar uma query?' }, xpReward: 60, estimatedMinutes: 40, services: ['Redshift'] },
  
  { id: 'aws-6-9', stepNumber: 54, title: { es: 'Redshift Data Sharing y ML', en: 'Redshift Data Sharing and ML', pt: 'Redshift Data Sharing e ML' }, description: { es: 'Compartir datos entre clusters y usar ML integrado.', en: 'Share data between clusters and use integrated ML.', pt: 'Compartilhar dados entre clusters e usar ML integrado.' }, theory: { es: `## Funcionalidades Avanzadas\n\n### Data Sharing\nComparte datos entre clusters sin copiarlos:\n\`\`\`sql\n-- Productor: crear datashare\nCREATE DATASHARE sales_share;\nALTER DATASHARE sales_share ADD SCHEMA public;\nALTER DATASHARE sales_share ADD TABLE public.sales;\nGRANT USAGE ON DATASHARE sales_share TO NAMESPACE 'consumer-namespace-id';\n\n-- Consumidor: usar datashare\nCREATE DATABASE sales_db FROM DATASHARE sales_share OF NAMESPACE 'producer-namespace-id';\n\`\`\`\n\n### Redshift ML\n\`\`\`sql\nCREATE MODEL churn_model\nFROM (\n  SELECT features, churned FROM training_data\n)\nTARGET churned\nFUNCTION predict_churn\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftMLRole'\nAUTO ON;\n\n-- Usar modelo\nSELECT customer_id, predict_churn(features) as will_churn\nFROM customers;\n\`\`\``, en: `## Advanced Features\n\n### Data Sharing\nShare data between clusters without copying:\n\`\`\`sql\n-- Producer: create datashare\nCREATE DATASHARE sales_share;\nALTER DATASHARE sales_share ADD SCHEMA public;\nALTER DATASHARE sales_share ADD TABLE public.sales;\nGRANT USAGE ON DATASHARE sales_share TO NAMESPACE 'consumer-namespace-id';\n\n-- Consumer: use datashare\nCREATE DATABASE sales_db FROM DATASHARE sales_share OF NAMESPACE 'producer-namespace-id';\n\`\`\`\n\n### Redshift ML\n\`\`\`sql\nCREATE MODEL churn_model\nFROM (\n  SELECT features, churned FROM training_data\n)\nTARGET churned\nFUNCTION predict_churn\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftMLRole'\nAUTO ON;\n\n-- Use model\nSELECT customer_id, predict_churn(features) as will_churn\nFROM customers;\n\`\`\``, pt: `## Funcionalidades Avan√ßadas\n\n### Data Sharing\nCompartilha dados entre clusters sem copiar:\n\`\`\`sql\n-- Produtor: criar datashare\nCREATE DATASHARE sales_share;\nALTER DATASHARE sales_share ADD SCHEMA public;\nALTER DATASHARE sales_share ADD TABLE public.sales;\nGRANT USAGE ON DATASHARE sales_share TO NAMESPACE 'consumer-namespace-id';\n\n-- Consumidor: usar datashare\nCREATE DATABASE sales_db FROM DATASHARE sales_share OF NAMESPACE 'producer-namespace-id';\n\`\`\`\n\n### Redshift ML\n\`\`\`sql\nCREATE MODEL churn_model\nFROM (\n  SELECT features, churned FROM training_data\n)\nTARGET churned\nFUNCTION predict_churn\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftMLRole'\nAUTO ON;\n\n-- Usar modelo\nSELECT customer_id, predict_churn(features) as will_churn\nFROM customers;\n\`\`\`` }, practicalTips: [{ es: 'ü§ù Data Sharing es clave para arquitecturas multi-cluster y data mesh', en: 'ü§ù Data Sharing is key for multi-cluster and data mesh architectures', pt: 'ü§ù Data Sharing √© chave para arquiteturas multi-cluster e data mesh' }], externalLinks: [{ title: 'Redshift Data Sharing', url: 'https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øEntiendes c√≥mo funciona Data Sharing entre clusters?', en: '‚úÖ Do you understand how Data Sharing works between clusters?', pt: '‚úÖ Voc√™ entende como funciona Data Sharing entre clusters?' }, xpReward: 55, estimatedMinutes: 35, services: ['Redshift'] }
];








