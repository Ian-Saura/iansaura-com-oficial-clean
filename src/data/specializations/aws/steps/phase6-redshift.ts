/**
 * FASE 6: AMAZON REDSHIFT - DATA WAREHOUSE
 * 9 pasos para dominar Redshift
 */
import { AWSStep } from '../types';

export const phase6Steps: AWSStep[] = [
  { id: 'aws-6-1', stepNumber: 46, title: { es: 'Introducci√≥n a Amazon Redshift', en: 'Introduction to Amazon Redshift', pt: 'Introdu√ß√£o ao Amazon Redshift' }, description: { es: 'Entender arquitectura MPP y casos de uso.', en: 'Understand MPP architecture and use cases.', pt: 'Entender arquitetura MPP e casos de uso.' }, theory: { es: `## Amazon Redshift\n\n### Arquitectura MPP (Massively Parallel Processing)\n- **Leader Node**: Recibe queries, planifica, coordina\n- **Compute Nodes**: Ejecutan queries en paralelo\n- **Slices**: Subdivisiones de cada nodo\n\n### Tipos de nodos\n| Tipo | vCPU | RAM | Storage | Uso |\n|------|------|-----|---------|-----|\n| dc2.large | 2 | 15GB | 160GB SSD | Dev/Test |\n| dc2.8xlarge | 32 | 244GB | 2.56TB SSD | Production |\n| ra3.xlplus | 4 | 32GB | Managed | Scalable |\n| ra3.4xlarge | 12 | 96GB | Managed | Production |\n\n### Redshift Serverless\nPaga por capacidad usada (RPUs), sin gesti√≥n de clusters.`, en: `## Amazon Redshift\n\n### MPP Architecture (Massively Parallel Processing)\n- **Leader Node**: Receives queries, plans, coordinates\n- **Compute Nodes**: Execute queries in parallel\n- **Slices**: Subdivisions of each node\n\n### Node types\n| Type | vCPU | RAM | Storage | Use |\n|------|------|-----|---------|-----|\n| dc2.large | 2 | 15GB | 160GB SSD | Dev/Test |\n| dc2.8xlarge | 32 | 244GB | 2.56TB SSD | Production |\n| ra3.xlplus | 4 | 32GB | Managed | Scalable |\n| ra3.4xlarge | 12 | 96GB | Managed | Production |\n\n### Redshift Serverless\nPay for used capacity (RPUs), no cluster management.`, pt: `## Amazon Redshift\n\n### Arquitetura MPP (Massively Parallel Processing)\n- **Leader Node**: Recebe queries, planeja, coordena\n- **Compute Nodes**: Executam queries em paralelo\n- **Slices**: Subdivis√µes de cada n√≥\n\n### Tipos de n√≥s\n| Tipo | vCPU | RAM | Storage | Uso |\n|------|------|-----|---------|-----|\n| dc2.large | 2 | 15GB | 160GB SSD | Dev/Test |\n| dc2.8xlarge | 32 | 244GB | 2.56TB SSD | Production |\n| ra3.xlplus | 4 | 32GB | Managed | Scalable |\n| ra3.4xlarge | 12 | 96GB | Managed | Production |\n\n### Redshift Serverless\nPaga por capacidade usada (RPUs), sem gest√£o de clusters.` }, practicalTips: [{ es: 'üöÄ Usa Redshift Serverless para desarrollo y peque√±os workloads', en: 'üöÄ Use Redshift Serverless for development and small workloads', pt: 'üöÄ Use Redshift Serverless para desenvolvimento e pequenos workloads' }], externalLinks: [{ title: 'Redshift Documentation', url: 'https://docs.aws.amazon.com/redshift/latest/dg/welcome.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øEntiendes la diferencia entre Leader y Compute nodes?', en: '‚úÖ Do you understand the difference between Leader and Compute nodes?', pt: '‚úÖ Voc√™ entende a diferen√ßa entre Leader e Compute nodes?' }, xpReward: 55, estimatedMinutes: 30, services: ['Redshift'] },
  
  { id: 'aws-6-2', stepNumber: 47, title: { es: 'Crear cluster Redshift', en: 'Create Redshift cluster', pt: 'Criar cluster Redshift' }, description: { es: 'Configurar un cluster o Redshift Serverless.', en: 'Configure a cluster or Redshift Serverless.', pt: 'Configurar um cluster ou Redshift Serverless.' }, theory: { es: `## Crear Cluster Redshift\n\n### Opciones de creaci√≥n\n1. **Provisioned**: Cluster con nodos fijos\n2. **Serverless**: Capacidad autom√°tica\n\n### Configuraci√≥n b√°sica\n\`\`\`yaml\nCluster: my-dwh-cluster\n  Node type: dc2.large\n  Number of nodes: 2\n  Database name: analytics\n  Admin user: admin\n  VPC: vpc-prod\n  Security Group: sg-redshift\n  Encrypted: Yes (KMS)\n  Enhanced VPC Routing: Yes\n\`\`\`\n\n### Conexi√≥n\n\`\`\`bash\n# Con psql\npsql -h cluster.xyz.us-east-1.redshift.amazonaws.com -p 5439 -U admin -d analytics\n\n# JDBC URL\njdbc:redshift://cluster.xyz.us-east-1.redshift.amazonaws.com:5439/analytics\n\`\`\``, en: `## Create Redshift Cluster\n\n### Creation options\n1. **Provisioned**: Cluster with fixed nodes\n2. **Serverless**: Automatic capacity\n\n### Basic configuration\n\`\`\`yaml\nCluster: my-dwh-cluster\n  Node type: dc2.large\n  Number of nodes: 2\n  Database name: analytics\n  Admin user: admin\n  VPC: vpc-prod\n  Security Group: sg-redshift\n  Encrypted: Yes (KMS)\n  Enhanced VPC Routing: Yes\n\`\`\`\n\n### Connection\n\`\`\`bash\n# With psql\npsql -h cluster.xyz.us-east-1.redshift.amazonaws.com -p 5439 -U admin -d analytics\n\n# JDBC URL\njdbc:redshift://cluster.xyz.us-east-1.redshift.amazonaws.com:5439/analytics\n\`\`\``, pt: `## Criar Cluster Redshift\n\n### Op√ß√µes de cria√ß√£o\n1. **Provisioned**: Cluster com n√≥s fixos\n2. **Serverless**: Capacidade autom√°tica\n\n### Configura√ß√£o b√°sica\n\`\`\`yaml\nCluster: my-dwh-cluster\n  Node type: dc2.large\n  Number of nodes: 2\n  Database name: analytics\n  Admin user: admin\n  VPC: vpc-prod\n  Security Group: sg-redshift\n  Encrypted: Yes (KMS)\n  Enhanced VPC Routing: Yes\n\`\`\`\n\n### Conex√£o\n\`\`\`bash\n# Com psql\npsql -h cluster.xyz.us-east-1.redshift.amazonaws.com -p 5439 -U admin -d analytics\n\n# JDBC URL\njdbc:redshift://cluster.xyz.us-east-1.redshift.amazonaws.com:5439/analytics\n\`\`\`` }, practicalTips: [{ es: 'üí∞ dc2.large es suficiente para aprender - NO crees clusters grandes', en: 'üí∞ dc2.large is enough for learning - DON\'T create large clusters', pt: 'üí∞ dc2.large √© suficiente para aprender - N√ÉO crie clusters grandes' }], externalLinks: [{ title: 'Getting Started with Redshift', url: 'https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øCreaste un cluster Serverless o dc2.large y te conectaste?', en: '‚úÖ Did you create a Serverless or dc2.large cluster and connect?', pt: '‚úÖ Voc√™ criou um cluster Serverless ou dc2.large e se conectou?' }, xpReward: 60, estimatedMinutes: 35, services: ['Redshift'] },
  
  { id: 'aws-6-3', stepNumber: 48, title: { es: 'COPY: El Comando M√°s Importante de Redshift', en: 'COPY: The Most Important Redshift Command', pt: 'COPY: O Comando Mais Importante do Redshift' }, description: { es: 'COPY es 10-100x m√°s r√°pido que INSERT. Es la √öNICA forma correcta de cargar datos masivos. Aprovecha MPP para cargar en paralelo desde S3.', en: 'COPY is 10-100x faster than INSERT. It is the ONLY correct way to bulk load data. It leverages MPP to load in parallel from S3.', pt: 'COPY √© 10-100x mais r√°pido que INSERT. √â a √öNICA forma correta de carregar dados massivos. Aproveita MPP para carregar em paralelo do S3.' }, theory: { es: `## COPY: El Comando M√°s Importante de Redshift

### ¬øPor Qu√© COPY es Clave?
COPY es **10-100x m√°s r√°pido** que INSERT porque aprovecha la arquitectura MPP (Massively Parallel Processing):

\`\`\`
INSERT (MALO - fila por fila):
  App ‚Üí Leader Node ‚Üí 1 Compute Node ‚Üí disco
  1 mill√≥n de filas = 1 mill√≥n de operaciones secuenciales
  Tiempo: ~30 minutos

COPY (BUENO - paralelo masivo):
  S3 ‚Üí Leader Node ‚Üí TODOS los Compute Nodes en paralelo
  1 mill√≥n de filas = repartidas entre N nodos simult√°neamente
  Tiempo: ~30 segundos (60x m√°s r√°pido)
\`\`\`

**REGLA DE ORO: NUNCA uses INSERT para cargas de m√°s de 100 filas. Siempre COPY.**

### Sintaxis B√°sica
\`\`\`sql
-- Desde Parquet (RECOMENDADO - m√°s r√°pido, tipos nativos)
COPY sales
FROM 's3://mi-datalake/silver/sales/'
IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3Role'
FORMAT AS PARQUET;

-- Desde CSV comprimido
COPY users
FROM 's3://mi-datalake/bronze/users/'
IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3Role'
CSV
IGNOREHEADER 1
DELIMITER ','
GZIP
REGION 'us-east-1'
MAXERROR 100
COMPUPDATE ON
STATUPDATE ON;
\`\`\`

### Optimizaci√≥n: Dividir Archivos para Carga Paralela
La regla m√°s importante: **n√∫mero de archivos = m√∫ltiplo del n√∫mero de slices**.

\`\`\`
Cluster con 2 nodos dc2.large (2 slices cada uno) = 4 slices total

‚ùå MALO: 1 archivo de 10GB
   ‚Üí Solo 1 slice trabaja, los otros 3 esperan

‚úÖ BUENO: 4 archivos de 2.5GB
   ‚Üí Cada slice carga 1 archivo en paralelo (4x m√°s r√°pido)

‚úÖ MEJOR: 8 archivos de 1.25GB
   ‚Üí 2 rondas, cada slice siempre ocupado

Regla: archivos de 100MB-1GB, cantidad = m√∫ltiplo de slices
\`\`\`

\`\`\`bash
# Ver cu√°ntos slices tiene tu cluster
SELECT node, COUNT(*) as slices FROM stv_slices GROUP BY node;
\`\`\`

### MANIFEST: Control Exacto de Archivos
\`\`\`json
// manifest.json en S3
{
  "entries": [
    {"url": "s3://bucket/data/part-00000.parquet", "mandatory": true},
    {"url": "s3://bucket/data/part-00001.parquet", "mandatory": true},
    {"url": "s3://bucket/data/part-00002.parquet", "mandatory": true},
    {"url": "s3://bucket/data/part-00003.parquet", "mandatory": true}
  ]
}
\`\`\`

\`\`\`sql
-- COPY con MANIFEST (carga SOLO los archivos listados)
COPY sales
FROM 's3://bucket/data/manifest.json'
IAM_ROLE 'arn:aws:iam::123:role/RedshiftS3Role'
FORMAT AS PARQUET
MANIFEST;
\`\`\`

### Debugging: Cuando COPY Falla
\`\`\`sql
-- Ver errores de la √∫ltima carga
SELECT * FROM stl_load_errors ORDER BY starttime DESC LIMIT 20;

-- Ver columnas problem√°ticas
SELECT colname, type, col_length, err_reason
FROM stl_load_errors 
WHERE filename LIKE '%sales%'
ORDER BY starttime DESC LIMIT 10;

-- Errores comunes:
-- "Delimiter not found" ‚Üí archivo no tiene el delimiter correcto
-- "String length exceeds DDL length" ‚Üí VARCHAR muy corto
-- "Invalid digit" ‚Üí columna INT tiene texto
\`\`\`

### Opciones Avanzadas
\`\`\`sql
COPY sales
FROM 's3://bucket/data/'
IAM_ROLE 'arn:aws:iam::123:role/RedshiftS3Role'
FORMAT AS PARQUET
COMPUPDATE ON        -- Aplica encoding √≥ptimo autom√°ticamente
STATUPDATE ON        -- Actualiza estad√≠sticas para el optimizer
MAXERROR 1000        -- Permite hasta 1000 errores antes de fallar
TRUNCATECOLUMNS      -- Trunca strings que excedan VARCHAR length
TIMEFORMAT 'auto'    -- Detecta formato de timestamps autom√°ticamente
ACCEPTINVCHARS ' '   -- Reemplaza caracteres inv√°lidos con espacio
BLANKSASNULL;        -- Trata blanks como NULL
\`\`\`

### COPY vs INSERT vs UNLOAD - Cu√°ndo Usar Cada Uno
| Operaci√≥n | Comando | Velocidad | Cu√°ndo |
|-----------|---------|-----------|--------|
| S3 ‚Üí Redshift | **COPY** | ‚ö°‚ö°‚ö° | SIEMPRE para cargas bulk |
| Redshift ‚Üí S3 | **UNLOAD** | ‚ö°‚ö°‚ö° | Exportar resultados/Gold |
| Fila individual | INSERT | ‚ö° | Solo para < 100 filas |
| Tabla a tabla | INSERT INTO...SELECT | ‚ö°‚ö° | Transformaciones internas |

### Costo: COPY es GRATIS
COPY no tiene costo adicional. Solo pagas por el compute de Redshift que ya est√° corriendo. La transferencia desde S3 en la misma regi√≥n es gratis. Es la operaci√≥n con mejor relaci√≥n costo/rendimiento en todo AWS.

### Post-COPY: VACUUM y ANALYZE
\`\`\`sql
-- Despu√©s de COPY o DELETE, SIEMPRE ejecutar:
VACUUM sales;    -- Recupera espacio de filas eliminadas, re-sort
ANALYZE sales;   -- Actualiza estad√≠sticas para query optimizer

-- VACUUM FULL vs SORT ONLY
VACUUM FULL sales;       -- Recupera espacio + re-sort (m√°s lento)
VACUUM SORT ONLY sales;  -- Solo re-sort (m√°s r√°pido)
VACUUM DELETE ONLY sales; -- Solo recuperar espacio
\`\`\``, en: `## COPY: The Most Important Redshift Command

### Why COPY is Key
COPY is **10-100x faster** than INSERT because it leverages MPP (Massively Parallel Processing):

\`\`\`
INSERT (BAD - row by row):
  App ‚Üí Leader Node ‚Üí 1 Compute Node ‚Üí disk
  1 million rows = 1 million sequential operations
  Time: ~30 minutes

COPY (GOOD - massive parallel):
  S3 ‚Üí Leader Node ‚Üí ALL Compute Nodes in parallel
  1 million rows = distributed across N nodes simultaneously
  Time: ~30 seconds (60x faster)
\`\`\`

**GOLDEN RULE: NEVER use INSERT for loads of more than 100 rows. Always COPY.**

### Basic Syntax
\`\`\`sql
-- From Parquet (RECOMMENDED)
COPY sales
FROM 's3://my-datalake/silver/sales/'
IAM_ROLE 'arn:aws:iam::123:role/RedshiftS3Role'
FORMAT AS PARQUET;

-- From compressed CSV
COPY users
FROM 's3://my-datalake/bronze/users/'
IAM_ROLE 'arn:aws:iam::123:role/RedshiftS3Role'
CSV IGNOREHEADER 1 GZIP MAXERROR 100
COMPUPDATE ON STATUPDATE ON;
\`\`\`

### Optimization: Split Files for Parallel Loading
Most important rule: **number of files = multiple of slice count**.

### Debugging: When COPY Fails
\`\`\`sql
SELECT * FROM stl_load_errors ORDER BY starttime DESC LIMIT 20;
\`\`\`

### Post-COPY: VACUUM and ANALYZE
\`\`\`sql
VACUUM sales;   -- Reclaim space from deleted rows, re-sort
ANALYZE sales;  -- Update statistics for query optimizer
\`\`\`

### COPY is FREE
No additional cost. You only pay for Redshift compute already running. S3 transfer in same region is free.`, pt: `## COPY: O Comando Mais Importante do Redshift

### Por Que COPY √© Chave
COPY √© **10-100x mais r√°pido** que INSERT porque aproveita a arquitetura MPP:

\`\`\`
INSERT (RUIM - linha por linha):
  App ‚Üí Leader Node ‚Üí 1 Compute Node ‚Üí disco
  1 milh√£o de linhas = 1 milh√£o de opera√ß√µes sequenciais

COPY (BOM - paralelo massivo):
  S3 ‚Üí Leader Node ‚Üí TODOS os Compute Nodes em paralelo
  1 milh√£o de linhas = distribu√≠das entre N n√≥s simultaneamente
\`\`\`

**REGRA DE OURO: NUNCA use INSERT para cargas de mais de 100 linhas. Sempre COPY.**

### Sintaxe B√°sica
\`\`\`sql
COPY sales
FROM 's3://meu-datalake/silver/sales/'
IAM_ROLE 'arn:aws:iam::123:role/RedshiftS3Role'
FORMAT AS PARQUET;
\`\`\`

### Otimiza√ß√£o: Dividir Arquivos
Regra: n√∫mero de arquivos = m√∫ltiplo do n√∫mero de slices.

### Debugging
\`\`\`sql
SELECT * FROM stl_load_errors ORDER BY starttime DESC LIMIT 20;
\`\`\`

### COPY √© GR√ÅTIS
Sem custo adicional. Transfer√™ncia do S3 na mesma regi√£o √© gr√°tis.` }, practicalTips: [{ es: '‚ö° NUNCA uses INSERT para m√°s de 100 filas - COPY es 10-100x m√°s r√°pido porque carga en paralelo usando todos los nodos del cluster', en: '‚ö° NEVER use INSERT for more than 100 rows - COPY is 10-100x faster because it loads in parallel using all cluster nodes', pt: '‚ö° NUNCA use INSERT para mais de 100 linhas - COPY √© 10-100x mais r√°pido porque carrega em paralelo usando todos os n√≥s do cluster' }, { es: 'üì¶ Divide archivos en m√∫ltiplos del n√∫mero de slices (ej: cluster de 4 slices ‚Üí 4, 8, 12 archivos). Archivos de 100MB-1GB cada uno.', en: 'üì¶ Split files into multiples of slice count (e.g.: 4 slice cluster ‚Üí 4, 8, 12 files). Files of 100MB-1GB each.', pt: 'üì¶ Divida arquivos em m√∫ltiplos do n√∫mero de slices (ex: cluster de 4 slices ‚Üí 4, 8, 12 arquivos). Arquivos de 100MB-1GB cada.' }, { es: 'üÜì COPY no tiene costo extra - solo pagas el compute de Redshift que ya est√° corriendo. La transferencia desde S3 en la misma regi√≥n es gratis.', en: 'üÜì COPY has no extra cost - you only pay for Redshift compute already running. S3 transfer in same region is free.', pt: 'üÜì COPY n√£o tem custo extra - voc√™ s√≥ paga pelo compute do Redshift que j√° est√° rodando. Transfer√™ncia do S3 na mesma regi√£o √© gr√°tis.' }, { es: 'üîç Si COPY falla, revisa stl_load_errors para ver exactamente qu√© fila y columna caus√≥ el error', en: 'üîç If COPY fails, check stl_load_errors to see exactly which row and column caused the error', pt: 'üîç Se COPY falhar, verifique stl_load_errors para ver exatamente qual linha e coluna causou o erro' }], interviewTips: [{ es: 'üéØ "¬øC√≥mo cargar√≠as 100 millones de filas en Redshift?" ‚Üí NUNCA con INSERT. Siempre COPY desde S3. Dividir los datos en archivos de 100MB-1GB, cantidad = m√∫ltiplo de slices. Usar Parquet con Snappy para m√°xima eficiencia. COPY es 10-100x m√°s r√°pido que INSERT porque aprovecha MPP para cargar en paralelo en todos los nodos.', en: 'üéØ "How would you load 100 million rows into Redshift?" ‚Üí NEVER with INSERT. Always COPY from S3. Split data into 100MB-1GB files, count = multiple of slices. Use Parquet with Snappy. COPY is 10-100x faster than INSERT because it leverages MPP.', pt: 'üéØ "Como voc√™ carregaria 100 milh√µes de linhas no Redshift?" ‚Üí NUNCA com INSERT. Sempre COPY do S3. Dividir dados em arquivos de 100MB-1GB, quantidade = m√∫ltiplo de slices. Usar Parquet com Snappy. COPY √© 10-100x mais r√°pido que INSERT.' }, { es: 'üéØ "¬øQu√© haces despu√©s de un COPY o DELETE en Redshift?" ‚Üí VACUUM para recuperar espacio de filas eliminadas y re-sortear. ANALYZE para actualizar estad√≠sticas del query optimizer. Sin VACUUM, las queries se degradan progresivamente. Sin ANALYZE, el optimizer toma decisiones sub√≥ptimas.', en: 'üéØ "What do you do after a COPY or DELETE in Redshift?" ‚Üí VACUUM to reclaim space and re-sort. ANALYZE to update optimizer statistics. Without VACUUM, queries degrade progressively. Without ANALYZE, optimizer makes suboptimal decisions.', pt: 'üéØ "O que voc√™ faz depois de um COPY ou DELETE no Redshift?" ‚Üí VACUUM para recuperar espa√ßo e re-ordenar. ANALYZE para atualizar estat√≠sticas do optimizer. Sem VACUUM, queries degradam progressivamente.' }, { es: 'üéØ "¬øC√≥mo debuggear√≠as un COPY que falla?" ‚Üí Consultar stl_load_errors para ver fila exacta, columna y motivo del error. Errores comunes: tipos de datos incompatibles, VARCHAR muy corto, caracteres inv√°lidos. Usar MAXERROR para tolerar N errores. Usar MANIFEST para control exacto de qu√© archivos cargar.', en: 'üéØ "How would you debug a failing COPY?" ‚Üí Query stl_load_errors for exact row, column and error reason. Common errors: incompatible data types, VARCHAR too short, invalid characters. Use MAXERROR to tolerate N errors.', pt: 'üéØ "Como voc√™ debuggaria um COPY que falha?" ‚Üí Consultar stl_load_errors para ver linha exata, coluna e motivo do erro.' }], commonMistakes: [{ es: '‚ùå Usar INSERT INTO para cargas masivas - es 10-100x m√°s lento que COPY', en: '‚ùå Using INSERT INTO for bulk loads - it is 10-100x slower than COPY', pt: '‚ùå Usar INSERT INTO para cargas massivas - √© 10-100x mais lento que COPY' }, { es: '‚ùå Cargar un solo archivo enorme - dividir en m√∫ltiplos del n√∫mero de slices', en: '‚ùå Loading a single huge file - split into multiples of slice count', pt: '‚ùå Carregar um √∫nico arquivo enorme - dividir em m√∫ltiplos do n√∫mero de slices' }, { es: '‚ùå No ejecutar VACUUM/ANALYZE despu√©s de COPY con DELETE previo', en: '‚ùå Not running VACUUM/ANALYZE after COPY with prior DELETE', pt: '‚ùå N√£o executar VACUUM/ANALYZE depois de COPY com DELETE anterior' }], externalLinks: [{ title: 'COPY Command Reference', url: 'https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html', type: 'aws_docs' }, { title: 'COPY Best Practices', url: 'https://docs.aws.amazon.com/redshift/latest/dg/c_loading-data-best-practices.html', type: 'aws_docs' }, { title: 'STL_LOAD_ERRORS', url: 'https://docs.aws.amazon.com/redshift/latest/dg/r_STL_LOAD_ERRORS.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øCargaste 1M+ filas con COPY y verificaste que fue paralelo revisando slices?', en: '‚úÖ Did you load 1M+ rows with COPY and verify it was parallel by checking slices?', pt: '‚úÖ Voc√™ carregou 1M+ linhas com COPY e verificou que foi paralelo checando slices?' }, xpReward: 75, estimatedMinutes: 50, services: ['Redshift', 'S3'] },
  
  { id: 'aws-6-4', stepNumber: 49, title: { es: 'Distribution y Sort Keys', en: 'Distribution and Sort Keys', pt: 'Distribution e Sort Keys' }, description: { es: 'Optimizar tablas con distribution y sort keys.', en: 'Optimize tables with distribution and sort keys.', pt: 'Otimizar tabelas com distribution e sort keys.' }, theory: { es: `## Distribution y Sort Keys\n\n### Distribution Styles\n1. **AUTO**: Redshift elige (recomendado para empezar)\n2. **EVEN**: Distribuye filas uniformemente\n3. **KEY**: Por valor de columna (bueno para JOINs)\n4. **ALL**: Copia tabla a todos los nodos (tablas peque√±as)\n\n### Sort Keys\n1. **COMPOUND**: M√∫ltiples columnas, orden importa\n2. **INTERLEAVED**: M√∫ltiples columnas, orden no importa (deprecated)\n\n### Ejemplo optimizado\n\`\`\`sql\nCREATE TABLE sales (\n  sale_id BIGINT,\n  customer_id INT,\n  sale_date DATE,\n  amount DECIMAL(10,2)\n)\nDISTKEY(customer_id)  -- JOIN frecuente con customers\nSORTKEY(sale_date);   -- Filtro frecuente por fecha\n\`\`\`\n\n### Cu√°ndo usar qu√©\n- **DISTKEY**: Columna de JOIN m√°s frecuente\n- **SORTKEY**: Columna de filtro m√°s frecuente`, en: `## Distribution and Sort Keys\n\n### Distribution Styles\n1. **AUTO**: Redshift chooses (recommended to start)\n2. **EVEN**: Distributes rows evenly\n3. **KEY**: By column value (good for JOINs)\n4. **ALL**: Copies table to all nodes (small tables)\n\n### Sort Keys\n1. **COMPOUND**: Multiple columns, order matters\n2. **INTERLEAVED**: Multiple columns, order doesn't matter (deprecated)\n\n### Optimized example\n\`\`\`sql\nCREATE TABLE sales (\n  sale_id BIGINT,\n  customer_id INT,\n  sale_date DATE,\n  amount DECIMAL(10,2)\n)\nDISTKEY(customer_id)  -- Frequent JOIN with customers\nSORTKEY(sale_date);   -- Frequent filter by date\n\`\`\`\n\n### When to use what\n- **DISTKEY**: Most frequent JOIN column\n- **SORTKEY**: Most frequent filter column`, pt: `## Distribution e Sort Keys\n\n### Distribution Styles\n1. **AUTO**: Redshift escolhe (recomendado para come√ßar)\n2. **EVEN**: Distribui linhas uniformemente\n3. **KEY**: Por valor de coluna (bom para JOINs)\n4. **ALL**: Copia tabela para todos os n√≥s (tabelas pequenas)\n\n### Sort Keys\n1. **COMPOUND**: M√∫ltiplas colunas, ordem importa\n2. **INTERLEAVED**: M√∫ltiplas colunas, ordem n√£o importa (deprecated)\n\n### Exemplo otimizado\n\`\`\`sql\nCREATE TABLE sales (\n  sale_id BIGINT,\n  customer_id INT,\n  sale_date DATE,\n  amount DECIMAL(10,2)\n)\nDISTKEY(customer_id)  -- JOIN frequente com customers\nSORTKEY(sale_date);   -- Filtro frequente por data\n\`\`\`\n\n### Quando usar o qu√™\n- **DISTKEY**: Coluna de JOIN mais frequente\n- **SORTKEY**: Coluna de filtro mais frequente` }, practicalTips: [{ es: 'üìä Analiza tus queries m√°s frecuentes antes de elegir keys', en: 'üìä Analyze your most frequent queries before choosing keys', pt: 'üìä Analise suas queries mais frequentes antes de escolher keys' }], externalLinks: [{ title: 'Table Design Best Practices', url: 'https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øCreaste tablas con DISTKEY y SORTKEY apropiados?', en: '‚úÖ Did you create tables with appropriate DISTKEY and SORTKEY?', pt: '‚úÖ Voc√™ criou tabelas com DISTKEY e SORTKEY apropriados?' }, xpReward: 70, estimatedMinutes: 45, services: ['Redshift'] },
  
  { id: 'aws-6-5', stepNumber: 50, title: { es: 'Redshift Spectrum', en: 'Redshift Spectrum', pt: 'Redshift Spectrum' }, description: { es: 'Consultar datos en S3 directamente desde Redshift.', en: 'Query data in S3 directly from Redshift.', pt: 'Consultar dados no S3 diretamente do Redshift.' }, theory: { es: `## Redshift Spectrum - Data Lakehouse\n\n### ¬øQu√© es?\nExtensi√≥n que permite ejecutar queries sobre S3 desde Redshift, combinando datos del warehouse con el data lake.\n\n### Crear external schema\n\`\`\`sql\nCREATE EXTERNAL SCHEMA spectrum_schema\nFROM DATA CATALOG\nDATABASE 'glue_database'\nIAM_ROLE 'arn:aws:iam::123:role/SpectrumRole'\nREGION 'us-east-1';\n\`\`\`\n\n### Query combinada\n\`\`\`sql\n-- Tabla interna + S3 via Spectrum\nSELECT \n  r.customer_name,\n  SUM(s.amount) as total\nFROM redshift_schema.customers r\nJOIN spectrum_schema.sales_history s ON r.id = s.customer_id\nGROUP BY r.customer_name;\n\`\`\`\n\n### Pricing\n- $5 por TB escaneado en S3 (igual que Athena)`, en: `## Redshift Spectrum - Data Lakehouse\n\n### What is it?\nExtension that allows running queries on S3 from Redshift, combining warehouse data with data lake.\n\n### Create external schema\n\`\`\`sql\nCREATE EXTERNAL SCHEMA spectrum_schema\nFROM DATA CATALOG\nDATABASE 'glue_database'\nIAM_ROLE 'arn:aws:iam::123:role/SpectrumRole'\nREGION 'us-east-1';\n\`\`\`\n\n### Combined query\n\`\`\`sql\n-- Internal table + S3 via Spectrum\nSELECT \n  r.customer_name,\n  SUM(s.amount) as total\nFROM redshift_schema.customers r\nJOIN spectrum_schema.sales_history s ON r.id = s.customer_id\nGROUP BY r.customer_name;\n\`\`\`\n\n### Pricing\n- $5 per TB scanned in S3 (same as Athena)`, pt: `## Redshift Spectrum - Data Lakehouse\n\n### O que √©?\nExtens√£o que permite executar queries sobre S3 do Redshift, combinando dados do warehouse com o data lake.\n\n### Criar external schema\n\`\`\`sql\nCREATE EXTERNAL SCHEMA spectrum_schema\nFROM DATA CATALOG\nDATABASE 'glue_database'\nIAM_ROLE 'arn:aws:iam::123:role/SpectrumRole'\nREGION 'us-east-1';\n\`\`\`\n\n### Query combinada\n\`\`\`sql\n-- Tabela interna + S3 via Spectrum\nSELECT \n  r.customer_name,\n  SUM(s.amount) as total\nFROM redshift_schema.customers r\nJOIN spectrum_schema.sales_history s ON r.id = s.customer_id\nGROUP BY r.customer_name;\n\`\`\`\n\n### Pricing\n- $5 por TB escaneado no S3 (igual ao Athena)` }, practicalTips: [{ es: 'üè† Spectrum es ideal para Data Lakehouse - datos calientes en Redshift, hist√≥ricos en S3', en: 'üè† Spectrum is ideal for Data Lakehouse - hot data in Redshift, historical in S3', pt: 'üè† Spectrum √© ideal para Data Lakehouse - dados quentes no Redshift, hist√≥ricos no S3' }], externalLinks: [{ title: 'Redshift Spectrum', url: 'https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øConfiguraste Spectrum y consultaste datos de S3?', en: '‚úÖ Did you configure Spectrum and query S3 data?', pt: '‚úÖ Voc√™ configurou Spectrum e consultou dados do S3?' }, xpReward: 65, estimatedMinutes: 40, services: ['Redshift', 'Glue Data Catalog'] },
  
  { id: 'aws-6-6', stepNumber: 51, title: { es: 'UNLOAD: Exportar datos a S3', en: 'UNLOAD: Export data to S3', pt: 'UNLOAD: Exportar dados para S3' }, description: { es: 'Exportar resultados de queries a S3.', en: 'Export query results to S3.', pt: 'Exportar resultados de queries para S3.' }, theory: { es: `## UNLOAD - Exportar a S3\n\n### Sintaxis b√°sica\n\`\`\`sql\nUNLOAD ('SELECT * FROM sales WHERE year = 2024')\nTO 's3://bucket/exports/sales_2024_'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nPARQUET\nPARTITION BY (region)\nALLOWOVERWRITE;\n\`\`\`\n\n### Opciones √∫tiles\n- **PARQUET/CSV**: Formato de output\n- **PARTITION BY**: Particionar output\n- **PARALLEL ON/OFF**: Control de paralelismo\n- **HEADER**: Incluir headers (CSV)\n- **GZIP**: Comprimir output`, en: `## UNLOAD - Export to S3\n\n### Basic syntax\n\`\`\`sql\nUNLOAD ('SELECT * FROM sales WHERE year = 2024')\nTO 's3://bucket/exports/sales_2024_'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nPARQUET\nPARTITION BY (region)\nALLOWOVERWRITE;\n\`\`\`\n\n### Useful options\n- **PARQUET/CSV**: Output format\n- **PARTITION BY**: Partition output\n- **PARALLEL ON/OFF**: Parallelism control\n- **HEADER**: Include headers (CSV)\n- **GZIP**: Compress output`, pt: `## UNLOAD - Exportar para S3\n\n### Sintaxe b√°sica\n\`\`\`sql\nUNLOAD ('SELECT * FROM sales WHERE year = 2024')\nTO 's3://bucket/exports/sales_2024_'\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftRole'\nPARQUET\nPARTITION BY (region)\nALLOWOVERWRITE;\n\`\`\`\n\n### Op√ß√µes √∫teis\n- **PARQUET/CSV**: Formato de output\n- **PARTITION BY**: Particionar output\n- **PARALLEL ON/OFF**: Controle de paralelismo\n- **HEADER**: Incluir headers (CSV)\n- **GZIP**: Comprimir output` }, practicalTips: [{ es: 'üì§ UNLOAD es m√°s eficiente que SELECT INTO S3 para grandes vol√∫menes', en: 'üì§ UNLOAD is more efficient than SELECT INTO S3 for large volumes', pt: 'üì§ UNLOAD √© mais eficiente que SELECT INTO S3 para grandes volumes' }], externalLinks: [{ title: 'UNLOAD Reference', url: 'https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øExportaste datos de Redshift a S3 con UNLOAD?', en: '‚úÖ Did you export data from Redshift to S3 with UNLOAD?', pt: '‚úÖ Voc√™ exportou dados do Redshift para S3 com UNLOAD?' }, xpReward: 50, estimatedMinutes: 25, services: ['Redshift', 'S3'] },
  
  { id: 'aws-6-7', stepNumber: 52, title: { es: 'VACUUM y ANALYZE', en: 'VACUUM and ANALYZE', pt: 'VACUUM e ANALYZE' }, description: { es: 'Mantenimiento de tablas para performance √≥ptima.', en: 'Table maintenance for optimal performance.', pt: 'Manuten√ß√£o de tabelas para performance √≥tima.' }, theory: { es: `## Mantenimiento de Tablas\n\n### VACUUM\nRecupera espacio de filas eliminadas y reordena datos:\n\`\`\`sql\n-- VACUUM completo\nVACUUM FULL sales;\n\n-- Solo ordenar\nVACUUM SORT ONLY sales;\n\n-- Solo recuperar espacio\nVACUUM DELETE ONLY sales;\n\`\`\`\n\n### ANALYZE\nActualiza estad√≠sticas para el query planner:\n\`\`\`sql\nANALYZE sales;\nANALYZE PREDICATE COLUMNS sales;  -- Solo columnas usadas en predicados\n\`\`\`\n\n### Auto-mantenimiento\nRedshift hace VACUUM y ANALYZE autom√°ticamente, pero puedes forzarlos despu√©s de cargas grandes.`, en: `## Table Maintenance\n\n### VACUUM\nRecovers space from deleted rows and reorders data:\n\`\`\`sql\n-- Full VACUUM\nVACUUM FULL sales;\n\n-- Sort only\nVACUUM SORT ONLY sales;\n\n-- Delete only\nVACUUM DELETE ONLY sales;\n\`\`\`\n\n### ANALYZE\nUpdates statistics for query planner:\n\`\`\`sql\nANALYZE sales;\nANALYZE PREDICATE COLUMNS sales;  -- Only columns used in predicates\n\`\`\`\n\n### Auto-maintenance\nRedshift does VACUUM and ANALYZE automatically, but you can force them after large loads.`, pt: `## Manuten√ß√£o de Tabelas\n\n### VACUUM\nRecupera espa√ßo de linhas deletadas e reordena dados:\n\`\`\`sql\n-- VACUUM completo\nVACUUM FULL sales;\n\n-- S√≥ ordenar\nVACUUM SORT ONLY sales;\n\n-- S√≥ recuperar espa√ßo\nVACUUM DELETE ONLY sales;\n\`\`\`\n\n### ANALYZE\nAtualiza estat√≠sticas para o query planner:\n\`\`\`sql\nANALYZE sales;\nANALYZE PREDICATE COLUMNS sales;  -- S√≥ colunas usadas em predicados\n\`\`\`\n\n### Auto-manuten√ß√£o\nRedshift faz VACUUM e ANALYZE automaticamente, mas voc√™ pode for√ß√°-los ap√≥s cargas grandes.` }, practicalTips: [{ es: 'üîß Ejecuta VACUUM despu√©s de cargas grandes de datos', en: 'üîß Run VACUUM after large data loads', pt: 'üîß Execute VACUUM ap√≥s cargas grandes de dados' }], externalLinks: [{ title: 'VACUUM Command', url: 'https://docs.aws.amazon.com/redshift/latest/dg/r_VACUUM_command.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øEjecutaste VACUUM y ANALYZE en una tabla?', en: '‚úÖ Did you run VACUUM and ANALYZE on a table?', pt: '‚úÖ Voc√™ executou VACUUM e ANALYZE em uma tabela?' }, xpReward: 45, estimatedMinutes: 25, services: ['Redshift'] },
  
  { id: 'aws-6-8', stepNumber: 53, title: { es: 'Query Performance y Workload Management', en: 'Query Performance and Workload Management', pt: 'Query Performance e Workload Management' }, description: { es: 'Diagnosticar y optimizar performance de queries.', en: 'Diagnose and optimize query performance.', pt: 'Diagnosticar e otimizar performance de queries.' }, theory: { es: `## Query Performance\n\n### Herramientas de diagn√≥stico\n\`\`\`sql\n-- Ver plan de ejecuci√≥n\nEXPLAIN SELECT * FROM sales WHERE date > '2024-01-01';\n\n-- Ver queries lentas\nSELECT * FROM STL_QUERY\nWHERE elapsed > 60000000 -- m√°s de 60 segundos\nORDER BY elapsed DESC;\n\n-- Ver locks\nSELECT * FROM SVV_TRANSACTIONS WHERE lockable_object_type = 'relation';\n\`\`\`\n\n### WLM (Workload Management)\nConfigura colas con diferentes prioridades:\n\`\`\`yaml\nQueues:\n  - Name: ETL\n    Memory: 40%\n    Concurrency: 5\n    User Groups: [etl_users]\n  - Name: BI\n    Memory: 50%\n    Concurrency: 15\n    User Groups: [analysts]\n  - Name: Default\n    Memory: 10%\n    Concurrency: 5\n\`\`\``, en: `## Query Performance\n\n### Diagnostic tools\n\`\`\`sql\n-- View execution plan\nEXPLAIN SELECT * FROM sales WHERE date > '2024-01-01';\n\n-- View slow queries\nSELECT * FROM STL_QUERY\nWHERE elapsed > 60000000 -- more than 60 seconds\nORDER BY elapsed DESC;\n\n-- View locks\nSELECT * FROM SVV_TRANSACTIONS WHERE lockable_object_type = 'relation';\n\`\`\`\n\n### WLM (Workload Management)\nConfigure queues with different priorities:\n\`\`\`yaml\nQueues:\n  - Name: ETL\n    Memory: 40%\n    Concurrency: 5\n    User Groups: [etl_users]\n  - Name: BI\n    Memory: 50%\n    Concurrency: 15\n    User Groups: [analysts]\n  - Name: Default\n    Memory: 10%\n    Concurrency: 5\n\`\`\``, pt: `## Query Performance\n\n### Ferramentas de diagn√≥stico\n\`\`\`sql\n-- Ver plano de execu√ß√£o\nEXPLAIN SELECT * FROM sales WHERE date > '2024-01-01';\n\n-- Ver queries lentas\nSELECT * FROM STL_QUERY\nWHERE elapsed > 60000000 -- mais de 60 segundos\nORDER BY elapsed DESC;\n\n-- Ver locks\nSELECT * FROM SVV_TRANSACTIONS WHERE lockable_object_type = 'relation';\n\`\`\`\n\n### WLM (Workload Management)\nConfigure filas com diferentes prioridades:\n\`\`\`yaml\nQueues:\n  - Name: ETL\n    Memory: 40%\n    Concurrency: 5\n    User Groups: [etl_users]\n  - Name: BI\n    Memory: 50%\n    Concurrency: 15\n    User Groups: [analysts]\n  - Name: Default\n    Memory: 10%\n    Concurrency: 5\n\`\`\`` }, practicalTips: [{ es: 'üìà Usa STL_QUERY y STL_QUERYTEXT para analizar queries problem√°ticas', en: 'üìà Use STL_QUERY and STL_QUERYTEXT to analyze problematic queries', pt: 'üìà Use STL_QUERY e STL_QUERYTEXT para analisar queries problem√°ticas' }], externalLinks: [{ title: 'Query Performance Tuning', url: 'https://docs.aws.amazon.com/redshift/latest/dg/c-optimizing-query-performance.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øUsaste EXPLAIN para analizar un query?', en: '‚úÖ Did you use EXPLAIN to analyze a query?', pt: '‚úÖ Voc√™ usou EXPLAIN para analisar uma query?' }, xpReward: 60, estimatedMinutes: 40, services: ['Redshift'] },
  
  { id: 'aws-6-9', stepNumber: 54, title: { es: 'Redshift Data Sharing y ML', en: 'Redshift Data Sharing and ML', pt: 'Redshift Data Sharing e ML' }, description: { es: 'Compartir datos entre clusters y usar ML integrado.', en: 'Share data between clusters and use integrated ML.', pt: 'Compartilhar dados entre clusters e usar ML integrado.' }, theory: { es: `## Funcionalidades Avanzadas\n\n### Data Sharing\nComparte datos entre clusters sin copiarlos:\n\`\`\`sql\n-- Productor: crear datashare\nCREATE DATASHARE sales_share;\nALTER DATASHARE sales_share ADD SCHEMA public;\nALTER DATASHARE sales_share ADD TABLE public.sales;\nGRANT USAGE ON DATASHARE sales_share TO NAMESPACE 'consumer-namespace-id';\n\n-- Consumidor: usar datashare\nCREATE DATABASE sales_db FROM DATASHARE sales_share OF NAMESPACE 'producer-namespace-id';\n\`\`\`\n\n### Redshift ML\n\`\`\`sql\nCREATE MODEL churn_model\nFROM (\n  SELECT features, churned FROM training_data\n)\nTARGET churned\nFUNCTION predict_churn\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftMLRole'\nAUTO ON;\n\n-- Usar modelo\nSELECT customer_id, predict_churn(features) as will_churn\nFROM customers;\n\`\`\``, en: `## Advanced Features\n\n### Data Sharing\nShare data between clusters without copying:\n\`\`\`sql\n-- Producer: create datashare\nCREATE DATASHARE sales_share;\nALTER DATASHARE sales_share ADD SCHEMA public;\nALTER DATASHARE sales_share ADD TABLE public.sales;\nGRANT USAGE ON DATASHARE sales_share TO NAMESPACE 'consumer-namespace-id';\n\n-- Consumer: use datashare\nCREATE DATABASE sales_db FROM DATASHARE sales_share OF NAMESPACE 'producer-namespace-id';\n\`\`\`\n\n### Redshift ML\n\`\`\`sql\nCREATE MODEL churn_model\nFROM (\n  SELECT features, churned FROM training_data\n)\nTARGET churned\nFUNCTION predict_churn\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftMLRole'\nAUTO ON;\n\n-- Use model\nSELECT customer_id, predict_churn(features) as will_churn\nFROM customers;\n\`\`\``, pt: `## Funcionalidades Avan√ßadas\n\n### Data Sharing\nCompartilha dados entre clusters sem copiar:\n\`\`\`sql\n-- Produtor: criar datashare\nCREATE DATASHARE sales_share;\nALTER DATASHARE sales_share ADD SCHEMA public;\nALTER DATASHARE sales_share ADD TABLE public.sales;\nGRANT USAGE ON DATASHARE sales_share TO NAMESPACE 'consumer-namespace-id';\n\n-- Consumidor: usar datashare\nCREATE DATABASE sales_db FROM DATASHARE sales_share OF NAMESPACE 'producer-namespace-id';\n\`\`\`\n\n### Redshift ML\n\`\`\`sql\nCREATE MODEL churn_model\nFROM (\n  SELECT features, churned FROM training_data\n)\nTARGET churned\nFUNCTION predict_churn\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftMLRole'\nAUTO ON;\n\n-- Usar modelo\nSELECT customer_id, predict_churn(features) as will_churn\nFROM customers;\n\`\`\`` }, practicalTips: [{ es: 'ü§ù Data Sharing es clave para arquitecturas multi-cluster y data mesh', en: 'ü§ù Data Sharing is key for multi-cluster and data mesh architectures', pt: 'ü§ù Data Sharing √© chave para arquiteturas multi-cluster e data mesh' }], externalLinks: [{ title: 'Redshift Data Sharing', url: 'https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html', type: 'aws_docs' }], checkpoint: { es: '‚úÖ ¬øEntiendes c√≥mo funciona Data Sharing entre clusters?', en: '‚úÖ Do you understand how Data Sharing works between clusters?', pt: '‚úÖ Voc√™ entende como funciona Data Sharing entre clusters?' }, xpReward: 55, estimatedMinutes: 35, services: ['Redshift'] }
];








