/**
 * FASE 8: AMAZON KINESIS - REAL-TIME STREAMING
 * 8 pasos para dominar streaming en AWS
 */
import { AWSStep } from '../types';

export const phase8Steps: AWSStep[] = [
  { id: 'aws-8-1', stepNumber: 63, title: { es: 'IntroducciÃ³n a Amazon Kinesis', en: 'Introduction to Amazon Kinesis', pt: 'IntroduÃ§Ã£o ao Amazon Kinesis' }, description: { es: 'Entender la familia de servicios Kinesis.', en: 'Understand the Kinesis service family.', pt: 'Entender a famÃ­lia de serviÃ§os Kinesis.' }, theory: { es: `## Amazon Kinesis - Familia de Streaming\n\n### Servicios\n1. **Kinesis Data Streams**: Ingesta de datos en tiempo real\n2. **Kinesis Data Firehose**: Entrega a destinos (S3, Redshift)\n3. **Kinesis Data Analytics**: SQL/Flink sobre streams\n4. **Kinesis Video Streams**: Video streaming\n\n### Kinesis vs Kafka\n| Kinesis | Kafka (MSK) |\n|---------|-------------|\n| Managed | Semi-managed |\n| Por shard | Por partition |\n| 7 dÃ­as max | Ilimitado |\n| AWS lock-in | Portable |`, en: `## Amazon Kinesis - Streaming Family\n\n### Services\n1. **Kinesis Data Streams**: Real-time data ingestion\n2. **Kinesis Data Firehose**: Delivery to destinations (S3, Redshift)\n3. **Kinesis Data Analytics**: SQL/Flink over streams\n4. **Kinesis Video Streams**: Video streaming\n\n### Kinesis vs Kafka\n| Kinesis | Kafka (MSK) |\n|---------|-------------|\n| Managed | Semi-managed |\n| Per shard | Per partition |\n| 7 days max | Unlimited |\n| AWS lock-in | Portable |`, pt: `## Amazon Kinesis - FamÃ­lia de Streaming\n\n### ServiÃ§os\n1. **Kinesis Data Streams**: IngestÃ£o de dados em tempo real\n2. **Kinesis Data Firehose**: Entrega a destinos (S3, Redshift)\n3. **Kinesis Data Analytics**: SQL/Flink sobre streams\n4. **Kinesis Video Streams**: Video streaming\n\n### Kinesis vs Kafka\n| Kinesis | Kafka (MSK) |\n|---------|-------------|\n| Managed | Semi-managed |\n| Por shard | Por partition |\n| 7 dias max | Ilimitado |\n| AWS lock-in | PortÃ¡vel |` }, practicalTips: [{ es: 'ðŸŒŠ Kinesis Firehose es el mÃ¡s fÃ¡cil para empezar - delivery directo a S3', en: 'ðŸŒŠ Kinesis Firehose is the easiest to start - direct delivery to S3', pt: 'ðŸŒŠ Kinesis Firehose Ã© o mais fÃ¡cil para comeÃ§ar - entrega direta ao S3' }], externalLinks: [{ title: 'Kinesis Overview', url: 'https://aws.amazon.com/kinesis/', type: 'aws_docs' }], checkpoint: { es: 'âœ… Â¿Entiendes la diferencia entre Streams, Firehose y Analytics?', en: 'âœ… Do you understand the difference between Streams, Firehose and Analytics?', pt: 'âœ… VocÃª entende a diferenÃ§a entre Streams, Firehose e Analytics?' }, xpReward: 50, estimatedMinutes: 25, services: ['Kinesis'] },
  { id: 'aws-8-2', stepNumber: 64, title: { es: 'Kinesis Data Streams', en: 'Kinesis Data Streams', pt: 'Kinesis Data Streams' }, description: { es: 'Crear streams y producir/consumir mensajes.', en: 'Create streams and produce/consume messages.', pt: 'Criar streams e produzir/consumir mensagens.' }, theory: { es: `## Kinesis Data Streams\n\n### Conceptos\n- **Stream**: Canal de datos\n- **Shard**: Unidad de throughput (1MB/s write, 2MB/s read)\n- **Record**: Mensaje (hasta 1MB)\n- **Partition Key**: Determina el shard\n\n### Crear stream\n\`\`\`bash\naws kinesis create-stream --stream-name events --shard-count 2\n\`\`\`\n\n### Producir con Python\n\`\`\`python\nimport boto3, json\nkinesis = boto3.client('kinesis')\nkinesis.put_record(\n    StreamName='events',\n    Data=json.dumps({'event': 'click', 'user': 'u1'}),\n    PartitionKey='user_u1'\n)\n\`\`\``, en: `## Kinesis Data Streams\n\n### Concepts\n- **Stream**: Data channel\n- **Shard**: Throughput unit (1MB/s write, 2MB/s read)\n- **Record**: Message (up to 1MB)\n- **Partition Key**: Determines the shard\n\n### Create stream\n\`\`\`bash\naws kinesis create-stream --stream-name events --shard-count 2\n\`\`\`\n\n### Produce with Python\n\`\`\`python\nimport boto3, json\nkinesis = boto3.client('kinesis')\nkinesis.put_record(\n    StreamName='events',\n    Data=json.dumps({'event': 'click', 'user': 'u1'}),\n    PartitionKey='user_u1'\n)\n\`\`\``, pt: `## Kinesis Data Streams\n\n### Conceitos\n- **Stream**: Canal de dados\n- **Shard**: Unidade de throughput (1MB/s write, 2MB/s read)\n- **Record**: Mensagem (atÃ© 1MB)\n- **Partition Key**: Determina o shard\n\n### Criar stream\n\`\`\`bash\naws kinesis create-stream --stream-name events --shard-count 2\n\`\`\`\n\n### Produzir com Python\n\`\`\`python\nimport boto3, json\nkinesis = boto3.client('kinesis')\nkinesis.put_record(\n    StreamName='events',\n    Data=json.dumps({'event': 'click', 'user': 'u1'}),\n    PartitionKey='user_u1'\n)\n\`\`\`` }, practicalTips: [{ es: 'âš™ï¸ Usa On-Demand capacity mode para auto-scaling de shards', en: 'âš™ï¸ Use On-Demand capacity mode for shard auto-scaling', pt: 'âš™ï¸ Use On-Demand capacity mode para auto-scaling de shards' }], externalLinks: [{ title: 'Kinesis Data Streams', url: 'https://docs.aws.amazon.com/streams/latest/dev/introduction.html', type: 'aws_docs' }], checkpoint: { es: 'âœ… Â¿Creaste un stream y enviaste mensajes?', en: 'âœ… Did you create a stream and send messages?', pt: 'âœ… VocÃª criou um stream e enviou mensagens?' }, xpReward: 65, estimatedMinutes: 40, services: ['Kinesis Data Streams'] },
  { id: 'aws-8-3', stepNumber: 65, title: { es: 'Kinesis Data Firehose', en: 'Kinesis Data Firehose', pt: 'Kinesis Data Firehose' }, description: { es: 'Configurar delivery streams a S3 y Redshift.', en: 'Configure delivery streams to S3 and Redshift.', pt: 'Configurar delivery streams para S3 e Redshift.' }, theory: { es: `## Kinesis Data Firehose\n\n### Destinos soportados\n- Amazon S3\n- Amazon Redshift (via S3)\n- Amazon OpenSearch\n- HTTP endpoints\n- Third-party (Datadog, Splunk, etc.)\n\n### ConfiguraciÃ³n para S3\n\`\`\`yaml\nDelivery Stream: events-to-s3\n  Source: Direct PUT / Kinesis Stream\n  Destination: S3\n  Bucket: s3://bucket/raw/events/\n  Buffer:\n    Size: 5 MB\n    Interval: 60 seconds\n  Compression: GZIP\n  Prefix: year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/\n\`\`\`\n\n### TransformaciÃ³n con Lambda\nPuedes transformar datos antes de entregarlos.`, en: `## Kinesis Data Firehose\n\n### Supported destinations\n- Amazon S3\n- Amazon Redshift (via S3)\n- Amazon OpenSearch\n- HTTP endpoints\n- Third-party (Datadog, Splunk, etc.)\n\n### S3 Configuration\n\`\`\`yaml\nDelivery Stream: events-to-s3\n  Source: Direct PUT / Kinesis Stream\n  Destination: S3\n  Bucket: s3://bucket/raw/events/\n  Buffer:\n    Size: 5 MB\n    Interval: 60 seconds\n  Compression: GZIP\n  Prefix: year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/\n\`\`\`\n\n### Lambda transformation\nYou can transform data before delivery.`, pt: `## Kinesis Data Firehose\n\n### Destinos suportados\n- Amazon S3\n- Amazon Redshift (via S3)\n- Amazon OpenSearch\n- HTTP endpoints\n- Third-party (Datadog, Splunk, etc.)\n\n### ConfiguraÃ§Ã£o para S3\n\`\`\`yaml\nDelivery Stream: events-to-s3\n  Source: Direct PUT / Kinesis Stream\n  Destination: S3\n  Bucket: s3://bucket/raw/events/\n  Buffer:\n    Size: 5 MB\n    Interval: 60 seconds\n  Compression: GZIP\n  Prefix: year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/\n\`\`\`\n\n### TransformaÃ§Ã£o com Lambda\nVocÃª pode transformar dados antes da entrega.` }, practicalTips: [{ es: 'ðŸ“¦ Firehose es perfecto para log ingestion - configura y olvida', en: 'ðŸ“¦ Firehose is perfect for log ingestion - set and forget', pt: 'ðŸ“¦ Firehose Ã© perfeito para log ingestion - configure e esqueÃ§a' }], externalLinks: [{ title: 'Kinesis Data Firehose', url: 'https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html', type: 'aws_docs' }], checkpoint: { es: 'âœ… Â¿Configuraste un Firehose que entrega a S3 con particiones por fecha?', en: 'âœ… Did you configure a Firehose delivering to S3 with date partitions?', pt: 'âœ… VocÃª configurou um Firehose que entrega ao S3 com partiÃ§Ãµes por data?' }, xpReward: 60, estimatedMinutes: 35, services: ['Kinesis Data Firehose'] },
  { id: 'aws-8-4', stepNumber: 66, title: { es: 'Kinesis Data Analytics', en: 'Kinesis Data Analytics', pt: 'Kinesis Data Analytics' }, description: { es: 'Procesar streams con SQL o Apache Flink.', en: 'Process streams with SQL or Apache Flink.', pt: 'Processar streams com SQL ou Apache Flink.' }, theory: { es: `## Kinesis Data Analytics\n\n### Dos modos\n1. **SQL**: Queries SQL sobre streams (legacy)\n2. **Apache Flink**: Procesamiento complejo (recomendado)\n\n### SQL ejemplo\n\`\`\`sql\nCREATE OR REPLACE STREAM "OUTPUT_STREAM" (\n  event_type VARCHAR(10),\n  event_count INTEGER\n);\n\nCREATE OR REPLACE PUMP "STREAM_PUMP" AS \nINSERT INTO "OUTPUT_STREAM"\nSELECT STREAM event_type, COUNT(*) as event_count\nFROM "SOURCE_SQL_STREAM_001"\nGROUP BY event_type, STEP("SOURCE_SQL_STREAM_001".ROWTIME BY INTERVAL '1' MINUTE);\n\`\`\`\n\n### Apache Flink\nMÃ¡s potente para:\n- Windowing complejo\n- State management\n- Exactly-once processing`, en: `## Kinesis Data Analytics\n\n### Two modes\n1. **SQL**: SQL queries over streams (legacy)\n2. **Apache Flink**: Complex processing (recommended)\n\n### SQL example\n\`\`\`sql\nCREATE OR REPLACE STREAM "OUTPUT_STREAM" (\n  event_type VARCHAR(10),\n  event_count INTEGER\n);\n\nCREATE OR REPLACE PUMP "STREAM_PUMP" AS \nINSERT INTO "OUTPUT_STREAM"\nSELECT STREAM event_type, COUNT(*) as event_count\nFROM "SOURCE_SQL_STREAM_001"\nGROUP BY event_type, STEP("SOURCE_SQL_STREAM_001".ROWTIME BY INTERVAL '1' MINUTE);\n\`\`\`\n\n### Apache Flink\nMore powerful for:\n- Complex windowing\n- State management\n- Exactly-once processing`, pt: `## Kinesis Data Analytics\n\n### Dois modos\n1. **SQL**: Queries SQL sobre streams (legacy)\n2. **Apache Flink**: Processamento complexo (recomendado)\n\n### SQL exemplo\n\`\`\`sql\nCREATE OR REPLACE STREAM "OUTPUT_STREAM" (\n  event_type VARCHAR(10),\n  event_count INTEGER\n);\n\nCREATE OR REPLACE PUMP "STREAM_PUMP" AS \nINSERT INTO "OUTPUT_STREAM"\nSELECT STREAM event_type, COUNT(*) as event_count\nFROM "SOURCE_SQL_STREAM_001"\nGROUP BY event_type, STEP("SOURCE_SQL_STREAM_001".ROWTIME BY INTERVAL '1' MINUTE);\n\`\`\`\n\n### Apache Flink\nMais potente para:\n- Windowing complexo\n- State management\n- Exactly-once processing` }, practicalTips: [{ es: 'ðŸ”¥ Usa Flink para procesamiento serio - SQL es solo para casos simples', en: 'ðŸ”¥ Use Flink for serious processing - SQL is only for simple cases', pt: 'ðŸ”¥ Use Flink para processamento sÃ©rio - SQL Ã© sÃ³ para casos simples' }], externalLinks: [{ title: 'Kinesis Data Analytics', url: 'https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html', type: 'aws_docs' }], checkpoint: { es: 'âœ… Â¿Creaste una aplicaciÃ³n SQL que agrega eventos por minuto?', en: 'âœ… Did you create a SQL application that aggregates events per minute?', pt: 'âœ… VocÃª criou uma aplicaÃ§Ã£o SQL que agrega eventos por minuto?' }, xpReward: 70, estimatedMinutes: 50, services: ['Kinesis Data Analytics'] },
  { id: 'aws-8-5', stepNumber: 67, title: { es: 'Lambda con Kinesis', en: 'Lambda with Kinesis', pt: 'Lambda com Kinesis' }, description: { es: 'Procesar streams con Lambda functions.', en: 'Process streams with Lambda functions.', pt: 'Processar streams com Lambda functions.' }, theory: { es: `## Lambda + Kinesis\n\n### Event Source Mapping\nLambda puede leer automÃ¡ticamente de Kinesis:\n\`\`\`python\ndef handler(event, context):\n    for record in event['Records']:\n        payload = base64.b64decode(record['kinesis']['data'])\n        data = json.loads(payload)\n        # Procesar\n        process_event(data)\n    return {'statusCode': 200}\n\`\`\`\n\n### ConfiguraciÃ³n\n\`\`\`yaml\nEvent Source:\n  Stream: arn:aws:kinesis:...:stream/events\n  Batch size: 100\n  Starting position: LATEST\n  Parallelization factor: 1\n  Retry attempts: 3\n  Bisect on error: true\n\`\`\``, en: `## Lambda + Kinesis\n\n### Event Source Mapping\nLambda can automatically read from Kinesis:\n\`\`\`python\ndef handler(event, context):\n    for record in event['Records']:\n        payload = base64.b64decode(record['kinesis']['data'])\n        data = json.loads(payload)\n        # Process\n        process_event(data)\n    return {'statusCode': 200}\n\`\`\`\n\n### Configuration\n\`\`\`yaml\nEvent Source:\n  Stream: arn:aws:kinesis:...:stream/events\n  Batch size: 100\n  Starting position: LATEST\n  Parallelization factor: 1\n  Retry attempts: 3\n  Bisect on error: true\n\`\`\``, pt: `## Lambda + Kinesis\n\n### Event Source Mapping\nLambda pode ler automaticamente do Kinesis:\n\`\`\`python\ndef handler(event, context):\n    for record in event['Records']:\n        payload = base64.b64decode(record['kinesis']['data'])\n        data = json.loads(payload)\n        # Processar\n        process_event(data)\n    return {'statusCode': 200}\n\`\`\`\n\n### ConfiguraÃ§Ã£o\n\`\`\`yaml\nEvent Source:\n  Stream: arn:aws:kinesis:...:stream/events\n  Batch size: 100\n  Starting position: LATEST\n  Parallelization factor: 1\n  Retry attempts: 3\n  Bisect on error: true\n\`\`\`` }, practicalTips: [{ es: 'âš¡ Lambda es ideal para procesamiento simple de bajo volumen', en: 'âš¡ Lambda is ideal for simple low-volume processing', pt: 'âš¡ Lambda Ã© ideal para processamento simples de baixo volume' }], externalLinks: [{ title: 'Lambda with Kinesis', url: 'https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html', type: 'aws_docs' }], checkpoint: { es: 'âœ… Â¿Configuraste Lambda para procesar mensajes de un stream?', en: 'âœ… Did you configure Lambda to process messages from a stream?', pt: 'âœ… VocÃª configurou Lambda para processar mensagens de um stream?' }, xpReward: 55, estimatedMinutes: 35, services: ['Lambda', 'Kinesis'] },
  { id: 'aws-8-6', stepNumber: 68, title: { es: 'Manejo de errores en streaming', en: 'Error handling in streaming', pt: 'Tratamento de erros em streaming' }, description: { es: 'Gestionar errores, retries y dead letter queues.', en: 'Handle errors, retries and dead letter queues.', pt: 'Gerenciar erros, retries e dead letter queues.' }, theory: { es: `## Manejo de Errores\n\n### Patrones de error handling\n1. **Retry**: Reintentar mensajes fallidos\n2. **DLQ**: Enviar a cola de errores\n3. **Bisect**: Dividir batch y reintentar\n4. **Skip**: Ignorar y continuar\n\n### Dead Letter Queue (DLQ)\n\`\`\`yaml\nLambda Event Source:\n  On failure destination:\n    Type: SQS\n    Queue: arn:aws:sqs:...:dlq\n\`\`\`\n\n### Firehose error handling\n\`\`\`yaml\nDelivery Stream:\n  S3 error output: s3://bucket/errors/\n  # Records fallidos van aquÃ­\n\`\`\``, en: `## Error Handling\n\n### Error handling patterns\n1. **Retry**: Retry failed messages\n2. **DLQ**: Send to error queue\n3. **Bisect**: Split batch and retry\n4. **Skip**: Ignore and continue\n\n### Dead Letter Queue (DLQ)\n\`\`\`yaml\nLambda Event Source:\n  On failure destination:\n    Type: SQS\n    Queue: arn:aws:sqs:...:dlq\n\`\`\`\n\n### Firehose error handling\n\`\`\`yaml\nDelivery Stream:\n  S3 error output: s3://bucket/errors/\n  # Failed records go here\n\`\`\``, pt: `## Tratamento de Erros\n\n### PadrÃµes de tratamento de erros\n1. **Retry**: Retentar mensagens que falharam\n2. **DLQ**: Enviar para fila de erros\n3. **Bisect**: Dividir batch e retentar\n4. **Skip**: Ignorar e continuar\n\n### Dead Letter Queue (DLQ)\n\`\`\`yaml\nLambda Event Source:\n  On failure destination:\n    Type: SQS\n    Queue: arn:aws:sqs:...:dlq\n\`\`\`\n\n### Firehose error handling\n\`\`\`yaml\nDelivery Stream:\n  S3 error output: s3://bucket/errors/\n  # Records que falharam vÃ£o aqui\n\`\`\`` }, practicalTips: [{ es: 'ðŸ”„ SIEMPRE configura DLQ para no perder datos en errores', en: 'ðŸ”„ ALWAYS configure DLQ to not lose data on errors', pt: 'ðŸ”„ SEMPRE configure DLQ para nÃ£o perder dados em erros' }], externalLinks: [{ title: 'Error Handling Best Practices', url: 'https://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html', type: 'aws_docs' }], checkpoint: { es: 'âœ… Â¿Configuraste DLQ para tu Lambda con Kinesis?', en: 'âœ… Did you configure DLQ for your Lambda with Kinesis?', pt: 'âœ… VocÃª configurou DLQ para seu Lambda com Kinesis?' }, xpReward: 50, estimatedMinutes: 30, services: ['SQS', 'Lambda'] },
  { id: 'aws-8-7', stepNumber: 69, title: { es: 'Monitoreo de pipelines streaming', en: 'Streaming pipeline monitoring', pt: 'Monitoramento de pipelines streaming' }, description: { es: 'MÃ©tricas, alertas y dashboards para streaming.', en: 'Metrics, alerts and dashboards for streaming.', pt: 'MÃ©tricas, alertas e dashboards para streaming.' }, theory: { es: `## Monitoreo de Streaming\n\n### MÃ©tricas clave de Kinesis\n- **IncomingRecords**: Records recibidos\n- **GetRecords.IteratorAgeMilliseconds**: Lag del consumidor\n- **WriteProvisionedThroughputExceeded**: Throttling\n\n### MÃ©tricas de Firehose\n- **DeliveryToS3.Records**: Records entregados\n- **DeliveryToS3.DataFreshness**: Latencia\n- **FailedConversion.Records**: Errores\n\n### Alertas recomendadas\n\`\`\`yaml\nAlarms:\n  - IteratorAge > 60000ms  # Consumer estÃ¡ atrasado\n  - FailedRecords > 0      # Hay errores\n  - WriteThrottle > 0      # Necesitas mÃ¡s shards\n\`\`\``, en: `## Streaming Monitoring\n\n### Key Kinesis metrics\n- **IncomingRecords**: Records received\n- **GetRecords.IteratorAgeMilliseconds**: Consumer lag\n- **WriteProvisionedThroughputExceeded**: Throttling\n\n### Firehose metrics\n- **DeliveryToS3.Records**: Records delivered\n- **DeliveryToS3.DataFreshness**: Latency\n- **FailedConversion.Records**: Errors\n\n### Recommended alarms\n\`\`\`yaml\nAlarms:\n  - IteratorAge > 60000ms  # Consumer is behind\n  - FailedRecords > 0      # There are errors\n  - WriteThrottle > 0      # Need more shards\n\`\`\``, pt: `## Monitoramento de Streaming\n\n### MÃ©tricas-chave do Kinesis\n- **IncomingRecords**: Records recebidos\n- **GetRecords.IteratorAgeMilliseconds**: Lag do consumidor\n- **WriteProvisionedThroughputExceeded**: Throttling\n\n### MÃ©tricas de Firehose\n- **DeliveryToS3.Records**: Records entregues\n- **DeliveryToS3.DataFreshness**: LatÃªncia\n- **FailedConversion.Records**: Erros\n\n### Alertas recomendados\n\`\`\`yaml\nAlarms:\n  - IteratorAge > 60000ms  # Consumidor estÃ¡ atrasado\n  - FailedRecords > 0      # HÃ¡ erros\n  - WriteThrottle > 0      # Precisa de mais shards\n\`\`\`` }, practicalTips: [{ es: 'ðŸ“ˆ IteratorAge es la mÃ©trica mÃ¡s importante - indica si estÃ¡s procesando a tiempo', en: 'ðŸ“ˆ IteratorAge is the most important metric - indicates if you\'re processing on time', pt: 'ðŸ“ˆ IteratorAge Ã© a mÃ©trica mais importante - indica se vocÃª estÃ¡ processando no tempo' }], externalLinks: [{ title: 'Kinesis Monitoring', url: 'https://docs.aws.amazon.com/streams/latest/dev/monitoring.html', type: 'aws_docs' }], checkpoint: { es: 'âœ… Â¿Creaste un dashboard con mÃ©tricas de tu stream?', en: 'âœ… Did you create a dashboard with your stream metrics?', pt: 'âœ… VocÃª criou um dashboard com mÃ©tricas do seu stream?' }, xpReward: 50, estimatedMinutes: 30, services: ['CloudWatch'] },
  { id: 'aws-8-8', stepNumber: 70, title: { es: 'Amazon MSK (Kafka managed)', en: 'Amazon MSK (Managed Kafka)', pt: 'Amazon MSK (Kafka gerenciado)' }, description: { es: 'Alternativa a Kinesis con Apache Kafka.', en: 'Alternative to Kinesis with Apache Kafka.', pt: 'Alternativa ao Kinesis com Apache Kafka.' }, theory: { es: `## Amazon MSK - Managed Kafka\n\n### Â¿CuÃ¡ndo MSK vs Kinesis?\n| MSK | Kinesis |\n|-----|---------||\n| Ya usas Kafka | Nuevo en streaming |\n| RetenciÃ³n larga | 7 dÃ­as max |\n| Ecosistema Kafka | AWS native |\n| Schema Registry | BÃ¡sico |\n\n### Componentes MSK\n- **Brokers**: Instancias de Kafka\n- **ZooKeeper**: CoordinaciÃ³n (opcional en nuevas versiones)\n- **Connect**: Integraciones\n\n### MSK Serverless\nKafka sin gestiÃ³n de brokers - paga por throughput.`, en: `## Amazon MSK - Managed Kafka\n\n### When MSK vs Kinesis?\n| MSK | Kinesis |\n|-----|---------||\n| Already use Kafka | New to streaming |\n| Long retention | 7 days max |\n| Kafka ecosystem | AWS native |\n| Schema Registry | Basic |\n\n### MSK Components\n- **Brokers**: Kafka instances\n- **ZooKeeper**: Coordination (optional in new versions)\n- **Connect**: Integrations\n\n### MSK Serverless\nKafka without broker management - pay per throughput.`, pt: `## Amazon MSK - Managed Kafka\n\n### Quando MSK vs Kinesis?\n| MSK | Kinesis |\n|-----|---------||\n| JÃ¡ usa Kafka | Novo em streaming |\n| RetenÃ§Ã£o longa | 7 dias max |\n| Ecossistema Kafka | AWS native |\n| Schema Registry | BÃ¡sico |\n\n### Componentes MSK\n- **Brokers**: InstÃ¢ncias de Kafka\n- **ZooKeeper**: CoordenaÃ§Ã£o (opcional em novas versÃµes)\n- **Connect**: IntegraÃ§Ãµes\n\n### MSK Serverless\nKafka sem gestÃ£o de brokers - paga por throughput.` }, practicalTips: [{ es: 'â˜• Si tu equipo ya conoce Kafka, MSK puede ser mejor opciÃ³n que Kinesis', en: 'â˜• If your team already knows Kafka, MSK might be a better option than Kinesis', pt: 'â˜• Se sua equipe jÃ¡ conhece Kafka, MSK pode ser melhor opÃ§Ã£o que Kinesis' }], externalLinks: [{ title: 'Amazon MSK', url: 'https://docs.aws.amazon.com/msk/latest/developerguide/what-is-msk.html', type: 'aws_docs' }], checkpoint: { es: 'âœ… Â¿Entiendes cuÃ¡ndo elegir MSK vs Kinesis?', en: 'âœ… Do you understand when to choose MSK vs Kinesis?', pt: 'âœ… VocÃª entende quando escolher MSK vs Kinesis?' }, xpReward: 45, estimatedMinutes: 25, services: ['MSK'] }
];








