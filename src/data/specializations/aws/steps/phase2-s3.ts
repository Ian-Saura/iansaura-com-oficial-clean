/**
 * FASE 2: AMAZON S3 - EL CORAZÃ“N DEL DATA LAKE
 * 10 pasos detallados para dominar S3
 */

import { AWSStep } from '../types';

export const phase2Steps: AWSStep[] = [
  {
    id: 'aws-2-1',
    stepNumber: 10,
    title: {
      es: 'Fundamentos de Amazon S3',
      en: 'Amazon S3 Fundamentals',
      pt: 'Fundamentos do Amazon S3'
    },
    description: {
      es: 'Entender quÃ© es S3, sus conceptos clave (buckets, objects, keys) y por quÃ© es el corazÃ³n del Data Lake.',
      en: 'Understand what S3 is, its key concepts (buckets, objects, keys) and why it\'s the heart of the Data Lake.',
      pt: 'Entender o que Ã© S3, seus conceitos-chave (buckets, objects, keys) e por que Ã© o coraÃ§Ã£o do Data Lake.'
    },
    theory: {
      es: `## Amazon S3 - Simple Storage Service

### Â¿QuÃ© es S3?
S3 es el servicio de almacenamiento de objetos de AWS. Imagina un sistema de archivos infinito, altamente disponible (99.999999999% durabilidad - "11 nines") y accesible desde cualquier parte del mundo.

### Conceptos clave
- **Bucket**: Contenedor de nivel superior (como una carpeta raÃ­z). Nombre Ãºnico globalmente.
- **Object**: Cualquier archivo almacenado (hasta 5TB por objeto)
- **Key**: La ruta completa del objeto dentro del bucket (ej: \`raw/2024/01/15/data.parquet\`)
- **Metadata**: Datos sobre el objeto (content-type, custom tags, etc.)

### Â¿Por quÃ© S3 para Data Lakes?
1. **Escalabilidad infinita**: Sin lÃ­mite de almacenamiento
2. **Durabilidad extrema**: 11 nines = prÃ¡cticamente imposible perder datos
3. **Costo bajo**: ~$0.023/GB/mes en Standard
4. **IntegraciÃ³n nativa**: Glue, Athena, EMR, Redshift Spectrum leen directamente
5. **SeparaciÃ³n compute-storage**: El paradigma moderno de Data Engineering

### Modelo de consistencia (actualizado 2020)
S3 ahora ofrece **strong consistency** para todas las operaciones. Antes de 2020 era eventual consistency, lo que causaba problemas en pipelines.

### Pricing S3 Standard
- Storage: $0.023/GB/mes (primeros 50TB)
- PUT/POST: $0.005 por 1000 requests
- GET: $0.0004 por 1000 requests
- Data Transfer OUT: $0.09/GB (hacia internet)`,
      en: `## Amazon S3 - Simple Storage Service

### What is S3?
S3 is AWS's object storage service. Think of it as an infinite file system, highly available (99.999999999% durability - "11 nines") and accessible from anywhere in the world.

### Key concepts
- **Bucket**: Top-level container (like a root folder). Globally unique name.
- **Object**: Any stored file (up to 5TB per object)
- **Key**: The complete path of the object within the bucket (e.g., \`raw/2024/01/15/data.parquet\`)
- **Metadata**: Data about the object (content-type, custom tags, etc.)

### Why S3 for Data Lakes?
1. **Infinite scalability**: No storage limit
2. **Extreme durability**: 11 nines = practically impossible to lose data
3. **Low cost**: ~$0.023/GB/month in Standard
4. **Native integration**: Glue, Athena, EMR, Redshift Spectrum read directly
5. **Compute-storage separation**: The modern Data Engineering paradigm

### Consistency model (updated 2020)
S3 now offers **strong consistency** for all operations. Before 2020 it was eventual consistency, which caused problems in pipelines.

### S3 Standard Pricing
- Storage: $0.023/GB/month (first 50TB)
- PUT/POST: $0.005 per 1000 requests
- GET: $0.0004 per 1000 requests
- Data Transfer OUT: $0.09/GB (to internet)`,
      pt: `## Amazon S3 - Simple Storage Service

### O que Ã© S3?
S3 Ã© o serviÃ§o de armazenamento de objetos da AWS. Pense nele como um sistema de arquivos infinito, altamente disponÃ­vel (99.999999999% durabilidade - "11 nines") e acessÃ­vel de qualquer lugar do mundo.

### Conceitos-chave
- **Bucket**: Container de nÃ­vel superior (como uma pasta raiz). Nome Ãºnico globalmente.
- **Object**: Qualquer arquivo armazenado (atÃ© 5TB por objeto)
- **Key**: O caminho completo do objeto dentro do bucket (ex: \`raw/2024/01/15/data.parquet\`)
- **Metadata**: Dados sobre o objeto (content-type, custom tags, etc.)

### Por que S3 para Data Lakes?
1. **Escalabilidade infinita**: Sem limite de armazenamento
2. **Durabilidade extrema**: 11 nines = praticamente impossÃ­vel perder dados
3. **Custo baixo**: ~$0.023/GB/mÃªs em Standard
4. **IntegraÃ§Ã£o nativa**: Glue, Athena, EMR, Redshift Spectrum leem diretamente
5. **SeparaÃ§Ã£o compute-storage**: O paradigma moderno de Data Engineering

### Modelo de consistÃªncia (atualizado 2020)
S3 agora oferece **strong consistency** para todas as operaÃ§Ãµes. Antes de 2020 era eventual consistency, o que causava problemas em pipelines.

### Pricing S3 Standard
- Storage: $0.023/GB/mÃªs (primeiros 50TB)
- PUT/POST: $0.005 por 1000 requests
- GET: $0.0004 por 1000 requests
- Data Transfer OUT: $0.09/GB (para internet)`
    },
    practicalTips: [
      { es: 'ğŸª£ Nombra buckets con prefijo Ãºnico (ej: empresa-proyecto-env-region)', en: 'ğŸª£ Name buckets with unique prefix (e.g., company-project-env-region)', pt: 'ğŸª£ Nomeie buckets com prefixo Ãºnico (ex: empresa-projeto-env-region)' },
      { es: 'ğŸ“ S3 NO tiene carpetas reales - las "carpetas" son solo prefijos en el key', en: 'ğŸ“ S3 does NOT have real folders - "folders" are just prefixes in the key', pt: 'ğŸ“ S3 NÃƒO tem pastas reais - as "pastas" sÃ£o apenas prefixos na key' },
      { es: 'ğŸ’° El costo principal suele ser Data Transfer OUT, no el storage', en: 'ğŸ’° The main cost is usually Data Transfer OUT, not storage', pt: 'ğŸ’° O custo principal costuma ser Data Transfer OUT, nÃ£o o storage' }
    ],
    externalLinks: [
      { title: 'Amazon S3 User Guide', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html', type: 'aws_docs' },
      { title: 'S3 Pricing', url: 'https://aws.amazon.com/s3/pricing/', type: 'aws_docs' },
      { title: 'S3 FAQs', url: 'https://aws.amazon.com/s3/faqs/', type: 'aws_docs' }
    ],
    checkpoint: { es: 'âœ… Â¿Puedes explicar la diferencia entre bucket, object y key?', en: 'âœ… Can you explain the difference between bucket, object and key?', pt: 'âœ… VocÃª consegue explicar a diferenÃ§a entre bucket, object e key?' },
    xpReward: 50,
    estimatedMinutes: 30,
    services: ['S3']
  },

  {
    id: 'aws-2-2',
    stepNumber: 11,
    title: {
      es: 'Crear y configurar tu primer bucket S3',
      en: 'Create and configure your first S3 bucket',
      pt: 'Criar e configurar seu primeiro bucket S3'
    },
    description: {
      es: 'Crear un bucket S3 con las configuraciones correctas de seguridad y mejores prÃ¡cticas.',
      en: 'Create an S3 bucket with correct security settings and best practices.',
      pt: 'Criar um bucket S3 com as configuraÃ§Ãµes corretas de seguranÃ§a e melhores prÃ¡ticas.'
    },
    theory: {
      es: `## Crear tu Primer Bucket S3

### Reglas de naming para buckets
- 3-63 caracteres, solo minÃºsculas, nÃºmeros y guiones
- Debe empezar con letra o nÃºmero
- NO puede ser formato IP (192.168.1.1)
- Ãšnico GLOBALMENTE en todo AWS

### Configuraciones importantes
1. **Block Public Access**: SIEMPRE activado por defecto
2. **Versioning**: Recomendado para datos importantes
3. **Encryption**: SSE-S3 (gratis) o SSE-KMS (mÃ¡s control)
4. **Tags**: Para tracking de costos

### ConvenciÃ³n de nombres recomendada
\`\`\`
{empresa}-{proyecto}-{ambiente}-{region}-{uso}

Ejemplos:
- miempresa-datalake-prod-useast1-raw
- miempresa-datalake-dev-useast1-processed
- miempresa-analytics-prod-useast1-exports
\`\`\`

### Block Public Access Settings
Estas son 4 configuraciones que BLOQUEAN acceso pÃºblico:
1. BlockPublicAcls
2. IgnorePublicAcls
3. BlockPublicPolicy
4. RestrictPublicBuckets

Para un Data Lake, las 4 deben estar ACTIVADAS.`,
      en: `## Create Your First S3 Bucket

### Bucket naming rules
- 3-63 characters, lowercase only, numbers and hyphens
- Must start with letter or number
- CANNOT be IP format (192.168.1.1)
- GLOBALLY unique across all AWS

### Important configurations
1. **Block Public Access**: ALWAYS enabled by default
2. **Versioning**: Recommended for important data
3. **Encryption**: SSE-S3 (free) or SSE-KMS (more control)
4. **Tags**: For cost tracking

### Recommended naming convention
\`\`\`
{company}-{project}-{environment}-{region}-{use}

Examples:
- mycompany-datalake-prod-useast1-raw
- mycompany-datalake-dev-useast1-processed
- mycompany-analytics-prod-useast1-exports
\`\`\`

### Block Public Access Settings
These are 4 settings that BLOCK public access:
1. BlockPublicAcls
2. IgnorePublicAcls
3. BlockPublicPolicy
4. RestrictPublicBuckets

For a Data Lake, all 4 should be ENABLED.`,
      pt: `## Criar Seu Primeiro Bucket S3

### Regras de naming para buckets
- 3-63 caracteres, apenas minÃºsculas, nÃºmeros e hÃ­fens
- Deve comeÃ§ar com letra ou nÃºmero
- NÃƒO pode ser formato IP (192.168.1.1)
- Ãšnico GLOBALMENTE em toda AWS

### ConfiguraÃ§Ãµes importantes
1. **Block Public Access**: SEMPRE ativado por padrÃ£o
2. **Versioning**: Recomendado para dados importantes
3. **Encryption**: SSE-S3 (grÃ¡tis) ou SSE-KMS (mais controle)
4. **Tags**: Para tracking de custos

### ConvenÃ§Ã£o de nomes recomendada
\`\`\`
{empresa}-{projeto}-{ambiente}-{region}-{uso}

Exemplos:
- minhaempresa-datalake-prod-useast1-raw
- minhaempresa-datalake-dev-useast1-processed
- minhaempresa-analytics-prod-useast1-exports
\`\`\`

### Block Public Access Settings
Estas sÃ£o 4 configuraÃ§Ãµes que BLOQUEIAM acesso pÃºblico:
1. BlockPublicAcls
2. IgnorePublicAcls
3. BlockPublicPolicy
4. RestrictPublicBuckets

Para um Data Lake, as 4 devem estar ATIVADAS.`
    },
    codeExample: {
      language: 'bash',
      code: `# Crear bucket con AWS CLI
aws s3 mb s3://tunombre-datalake-learning-useast1 --region us-east-1

# Verificar que se creÃ³
aws s3 ls

# Subir un archivo de prueba
echo "Hello S3!" > test.txt
aws s3 cp test.txt s3://tunombre-datalake-learning-useast1/test/

# Ver contenido del bucket
aws s3 ls s3://tunombre-datalake-learning-useast1/ --recursive

# Descargar el archivo
aws s3 cp s3://tunombre-datalake-learning-useast1/test/test.txt downloaded.txt

# Eliminar el archivo
aws s3 rm s3://tunombre-datalake-learning-useast1/test/test.txt`,
      explanation: { es: 'Comandos bÃ¡sicos para crear bucket y gestionar objetos', en: 'Basic commands to create bucket and manage objects', pt: 'Comandos bÃ¡sicos para criar bucket e gerenciar objetos' }
    },
    practicalTips: [
      { es: 'ğŸ”’ NUNCA desactives Block Public Access a menos que sea absolutamente necesario', en: 'ğŸ”’ NEVER disable Block Public Access unless absolutely necessary', pt: 'ğŸ”’ NUNCA desative Block Public Access a menos que seja absolutamente necessÃ¡rio' },
      { es: 'ğŸ·ï¸ Siempre usa tags: Environment, Project, Owner como mÃ­nimo', en: 'ğŸ·ï¸ Always use tags: Environment, Project, Owner as minimum', pt: 'ğŸ·ï¸ Sempre use tags: Environment, Project, Owner como mÃ­nimo' }
    ],
    externalLinks: [
      { title: 'Creating S3 Bucket', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html', type: 'aws_docs' }
    ],
    checkpoint: { es: 'âœ… Â¿Creaste un bucket, subiste un archivo y lo descargaste con CLI?', en: 'âœ… Did you create a bucket, upload a file and download it with CLI?', pt: 'âœ… VocÃª criou um bucket, fez upload de um arquivo e baixou com CLI?' },
    xpReward: 60,
    estimatedMinutes: 25,
    services: ['S3']
  },

  {
    id: 'aws-2-3',
    stepNumber: 12,
    title: {
      es: 'Storage Classes y Lifecycle Policies',
      en: 'Storage Classes and Lifecycle Policies',
      pt: 'Storage Classes e Lifecycle Policies'
    },
    description: {
      es: 'Entender las diferentes clases de almacenamiento y cÃ³mo automatizar el movimiento de datos para optimizar costos.',
      en: 'Understand different storage classes and how to automate data movement to optimize costs.',
      pt: 'Entender as diferentes classes de armazenamento e como automatizar o movimento de dados para otimizar custos.'
    },
    theory: {
      es: `## Storage Classes en S3

### Clases disponibles (2024)
| Clase | Uso | Costo Storage | Costo Retrieval |
|-------|-----|---------------|-----------------|
| **Standard** | Acceso frecuente | $0.023/GB | Gratis |
| **Intelligent-Tiering** | Patrones desconocidos | $0.0025/1000 obj | Auto |
| **Standard-IA** | Acceso infrecuente (>30 dÃ­as) | $0.0125/GB | $0.01/GB |
| **One Zone-IA** | IA + single AZ | $0.01/GB | $0.01/GB |
| **Glacier Instant** | Archivado con acceso inmediato | $0.004/GB | $0.03/GB |
| **Glacier Flexible** | Archivado (mins-12h retrieval) | $0.0036/GB | $0.03/GB |
| **Glacier Deep Archive** | Archivado largo plazo (12-48h) | $0.00099/GB | $0.02/GB |

### Lifecycle Policies - Automatiza el ahorro
Reglas automÃ¡ticas para mover/eliminar objetos:
\`\`\`yaml
# Ejemplo: Datos de logs
- DÃ­as 0-30: Standard (acceso activo)
- DÃ­as 30-90: Standard-IA (reportes mensuales)
- DÃ­as 90-365: Glacier Flexible (compliance)
- DÃ­a 365+: Delete (ya no necesario)
\`\`\`

### Para Data Lakes tÃ­picos
- **raw/**: Standard â†’ Glacier Flexible (90 dÃ­as)
- **processed/**: Standard â†’ Standard-IA (30 dÃ­as)
- **serving/**: Standard (acceso frecuente)
- **temp/**: Delete despuÃ©s de 7 dÃ­as`,
      en: `## Storage Classes in S3

### Available classes (2024)
| Class | Use | Storage Cost | Retrieval Cost |
|-------|-----|---------------|-----------------|
| **Standard** | Frequent access | $0.023/GB | Free |
| **Intelligent-Tiering** | Unknown patterns | $0.0025/1000 obj | Auto |
| **Standard-IA** | Infrequent access (>30 days) | $0.0125/GB | $0.01/GB |
| **One Zone-IA** | IA + single AZ | $0.01/GB | $0.01/GB |
| **Glacier Instant** | Archive with instant access | $0.004/GB | $0.03/GB |
| **Glacier Flexible** | Archive (mins-12h retrieval) | $0.0036/GB | $0.03/GB |
| **Glacier Deep Archive** | Long-term archive (12-48h) | $0.00099/GB | $0.02/GB |

### Lifecycle Policies - Automate savings
Automatic rules to move/delete objects:
\`\`\`yaml
# Example: Log data
- Days 0-30: Standard (active access)
- Days 30-90: Standard-IA (monthly reports)
- Days 90-365: Glacier Flexible (compliance)
- Day 365+: Delete (no longer needed)
\`\`\`

### For typical Data Lakes
- **raw/**: Standard â†’ Glacier Flexible (90 days)
- **processed/**: Standard â†’ Standard-IA (30 days)
- **serving/**: Standard (frequent access)
- **temp/**: Delete after 7 days`,
      pt: `## Storage Classes no S3

### Classes disponÃ­veis (2024)
| Classe | Uso | Custo Storage | Custo Retrieval |
|-------|-----|---------------|-----------------|
| **Standard** | Acesso frequente | $0.023/GB | GrÃ¡tis |
| **Intelligent-Tiering** | PadrÃµes desconhecidos | $0.0025/1000 obj | Auto |
| **Standard-IA** | Acesso infrequente (>30 dias) | $0.0125/GB | $0.01/GB |
| **One Zone-IA** | IA + single AZ | $0.01/GB | $0.01/GB |
| **Glacier Instant** | Arquivamento com acesso imediato | $0.004/GB | $0.03/GB |
| **Glacier Flexible** | Arquivamento (mins-12h retrieval) | $0.0036/GB | $0.03/GB |
| **Glacier Deep Archive** | Arquivamento longo prazo (12-48h) | $0.00099/GB | $0.02/GB |

### Lifecycle Policies - Automatize economia
Regras automÃ¡ticas para mover/deletar objetos:
\`\`\`yaml
# Exemplo: Dados de logs
- Dias 0-30: Standard (acesso ativo)
- Dias 30-90: Standard-IA (relatÃ³rios mensais)
- Dias 90-365: Glacier Flexible (compliance)
- Dia 365+: Delete (nÃ£o mais necessÃ¡rio)
\`\`\`

### Para Data Lakes tÃ­picos
- **raw/**: Standard â†’ Glacier Flexible (90 dias)
- **processed/**: Standard â†’ Standard-IA (30 dias)
- **serving/**: Standard (acesso frequente)
- **temp/**: Delete apÃ³s 7 dias`
    },
    practicalTips: [
      { es: 'ğŸ’¡ Intelligent-Tiering es perfecto cuando no conoces los patrones de acceso', en: 'ğŸ’¡ Intelligent-Tiering is perfect when you don\'t know access patterns', pt: 'ğŸ’¡ Intelligent-Tiering Ã© perfeito quando nÃ£o conhece os padrÃµes de acesso' },
      { es: 'âš ï¸ El retrieval de Glacier Deep Archive puede tardar 12-48h - planifica con anticipaciÃ³n', en: 'âš ï¸ Glacier Deep Archive retrieval can take 12-48h - plan ahead', pt: 'âš ï¸ O retrieval de Glacier Deep Archive pode levar 12-48h - planeje com antecedÃªncia' }
    ],
    externalLinks: [
      { title: 'S3 Storage Classes', url: 'https://aws.amazon.com/s3/storage-classes/', type: 'aws_docs' },
      { title: 'S3 Lifecycle Configuration', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html', type: 'aws_docs' }
    ],
    checkpoint: { es: 'âœ… Â¿Puedes explicar cuÃ¡ndo usar Standard vs IA vs Glacier?', en: 'âœ… Can you explain when to use Standard vs IA vs Glacier?', pt: 'âœ… VocÃª consegue explicar quando usar Standard vs IA vs Glacier?' },
    xpReward: 55,
    estimatedMinutes: 30,
    services: ['S3', 'S3 Glacier']
  },

  {
    id: 'aws-2-4',
    stepNumber: 13,
    title: {
      es: 'Particionamiento de datos para Data Lakes',
      en: 'Data partitioning for Data Lakes',
      pt: 'Particionamento de dados para Data Lakes'
    },
    description: {
      es: 'Aprender estrategias de particionamiento Hive-style que optimizan queries en Athena y Spark.',
      en: 'Learn Hive-style partitioning strategies that optimize queries in Athena and Spark.',
      pt: 'Aprender estratÃ©gias de particionamento Hive-style que otimizam queries no Athena e Spark.'
    },
    theory: {
      es: `## Particionamiento en S3 - Clave para Performance

### Â¿QuÃ© es particionamiento?
Organizar datos en "carpetas" basadas en valores de columnas para que las queries solo lean los datos necesarios.

### Formato Hive-Style (estÃ¡ndar de la industria)
\`\`\`
s3://bucket/tabla/
  â”œâ”€â”€ year=2024/
  â”‚   â”œâ”€â”€ month=01/
  â”‚   â”‚   â”œâ”€â”€ day=15/
  â”‚   â”‚   â”‚   â”œâ”€â”€ data_001.parquet
  â”‚   â”‚   â”‚   â””â”€â”€ data_002.parquet
  â”‚   â”‚   â””â”€â”€ day=16/
  â”‚   â””â”€â”€ month=02/
  â””â”€â”€ year=2023/
\`\`\`

### Beneficios del particionamiento
1. **Partition pruning**: Athena solo escanea particiones relevantes
2. **Menor costo**: Menos datos escaneados = menor costo en Athena
3. **Mejor performance**: Menos I/O = queries mÃ¡s rÃ¡pidas
4. **ParalelizaciÃ³n**: Spark puede procesar particiones en paralelo

### Estrategias comunes
| Datos | Particionamiento recomendado |
|-------|------------------------------|
| Logs/Eventos | year/month/day o year/month/day/hour |
| Transacciones | year/month/day |
| Datos geogrÃ¡ficos | country/region |
| Multi-tenant | tenant_id/year/month |

### âš ï¸ Anti-patrones a evitar
- Demasiadas particiones pequeÃ±as (< 128MB por particiÃ³n)
- Particiones muy grandes (> 1GB archivos individuales)
- Particionamiento por columna de alta cardinalidad (ej: user_id)`,
      en: `## Partitioning in S3 - Key for Performance

### What is partitioning?
Organizing data in "folders" based on column values so queries only read necessary data.

### Hive-Style format (industry standard)
\`\`\`
s3://bucket/table/
  â”œâ”€â”€ year=2024/
  â”‚   â”œâ”€â”€ month=01/
  â”‚   â”‚   â”œâ”€â”€ day=15/
  â”‚   â”‚   â”‚   â”œâ”€â”€ data_001.parquet
  â”‚   â”‚   â”‚   â””â”€â”€ data_002.parquet
  â”‚   â”‚   â””â”€â”€ day=16/
  â”‚   â””â”€â”€ month=02/
  â””â”€â”€ year=2023/
\`\`\`

### Partitioning benefits
1. **Partition pruning**: Athena only scans relevant partitions
2. **Lower cost**: Less data scanned = lower cost in Athena
3. **Better performance**: Less I/O = faster queries
4. **Parallelization**: Spark can process partitions in parallel

### Common strategies
| Data | Recommended partitioning |
|-------|------------------------------|
| Logs/Events | year/month/day or year/month/day/hour |
| Transactions | year/month/day |
| Geographic data | country/region |
| Multi-tenant | tenant_id/year/month |

### âš ï¸ Anti-patterns to avoid
- Too many small partitions (< 128MB per partition)
- Very large partitions (> 1GB individual files)
- Partitioning by high cardinality column (e.g., user_id)`,
      pt: `## Particionamento no S3 - Chave para Performance

### O que Ã© particionamento?
Organizar dados em "pastas" baseadas em valores de colunas para que as queries sÃ³ leiam os dados necessÃ¡rios.

### Formato Hive-Style (padrÃ£o da indÃºstria)
\`\`\`
s3://bucket/tabela/
  â”œâ”€â”€ year=2024/
  â”‚   â”œâ”€â”€ month=01/
  â”‚   â”‚   â”œâ”€â”€ day=15/
  â”‚   â”‚   â”‚   â”œâ”€â”€ data_001.parquet
  â”‚   â”‚   â”‚   â””â”€â”€ data_002.parquet
  â”‚   â”‚   â””â”€â”€ day=16/
  â”‚   â””â”€â”€ month=02/
  â””â”€â”€ year=2023/
\`\`\`

### BenefÃ­cios do particionamento
1. **Partition pruning**: Athena sÃ³ escaneia partiÃ§Ãµes relevantes
2. **Menor custo**: Menos dados escaneados = menor custo no Athena
3. **Melhor performance**: Menos I/O = queries mais rÃ¡pidas
4. **ParalelizaÃ§Ã£o**: Spark pode processar partiÃ§Ãµes em paralelo

### EstratÃ©gias comuns
| Dados | Particionamento recomendado |
|-------|------------------------------|
| Logs/Eventos | year/month/day ou year/month/day/hour |
| TransaÃ§Ãµes | year/month/day |
| Dados geogrÃ¡ficos | country/region |
| Multi-tenant | tenant_id/year/month |

### âš ï¸ Anti-padrÃµes a evitar
- Muitas partiÃ§Ãµes pequenas (< 128MB por partiÃ§Ã£o)
- PartiÃ§Ãµes muito grandes (> 1GB arquivos individuais)
- Particionamento por coluna de alta cardinalidade (ex: user_id)`
    },
    codeExample: {
      language: 'python',
      code: `import pandas as pd
from datetime import datetime
import pyarrow as pa
import pyarrow.parquet as pq

# Crear datos de ejemplo
df = pd.DataFrame({
    'event_id': range(1000),
    'user_id': [f'user_{i%100}' for i in range(1000)],
    'event_type': ['click', 'view', 'purchase'] * 333 + ['click'],
    'timestamp': pd.date_range('2024-01-01', periods=1000, freq='H'),
    'value': [round(i * 0.5, 2) for i in range(1000)]
})

# AÃ±adir columnas de particiÃ³n
df['year'] = df['timestamp'].dt.year
df['month'] = df['timestamp'].dt.month.astype(str).str.zfill(2)
df['day'] = df['timestamp'].dt.day.astype(str).str.zfill(2)

# Guardar con particionamiento Hive-style
# boto3 + pyarrow para subir a S3
table = pa.Table.from_pandas(df)
pq.write_to_dataset(
    table,
    root_path='s3://tu-bucket/events/',
    partition_cols=['year', 'month', 'day'],
    existing_data_behavior='overwrite_or_ignore'
)`,
      explanation: { es: 'CÃ³digo Python para crear datos particionados en formato Hive-style', en: 'Python code to create Hive-style partitioned data', pt: 'CÃ³digo Python para criar dados particionados no formato Hive-style' }
    },
    practicalTips: [
      { es: 'ğŸ“Š Objetivo: archivos de 128MB-1GB para optimal performance', en: 'ğŸ“Š Target: 128MB-1GB files for optimal performance', pt: 'ğŸ“Š Objetivo: arquivos de 128MB-1GB para performance optimal' },
      { es: 'ğŸ¯ Particiona por las columnas mÃ¡s usadas en filtros WHERE', en: 'ğŸ¯ Partition by columns most used in WHERE filters', pt: 'ğŸ¯ Particione pelas colunas mais usadas em filtros WHERE' }
    ],
    externalLinks: [
      { title: 'Athena Partitioning Best Practices', url: 'https://docs.aws.amazon.com/athena/latest/ug/partitions.html', type: 'aws_docs' },
      { title: 'AWS Blog - S3 Partitioning', url: 'https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/', type: 'article' }
    ],
    checkpoint: { es: 'âœ… Â¿Puedes crear una estructura de particiones Hive-style para datos de logs?', en: 'âœ… Can you create a Hive-style partition structure for log data?', pt: 'âœ… VocÃª consegue criar uma estrutura de partiÃ§Ãµes Hive-style para dados de logs?' },
    xpReward: 70,
    estimatedMinutes: 40,
    services: ['S3']
  },

  {
    id: 'aws-2-5',
    stepNumber: 14,
    title: {
      es: 'Formatos de archivo: Parquet vs JSON vs CSV',
      en: 'File formats: Parquet vs JSON vs CSV',
      pt: 'Formatos de arquivo: Parquet vs JSON vs CSV'
    },
    description: {
      es: 'Entender quÃ© formato usar y por quÃ© Parquet es el estÃ¡ndar para Data Lakes.',
      en: 'Understand which format to use and why Parquet is the standard for Data Lakes.',
      pt: 'Entender qual formato usar e por que Parquet Ã© o padrÃ£o para Data Lakes.'
    },
    theory: {
      es: `## Formatos de Archivo para Data Lakes

### ComparaciÃ³n de formatos
| Formato | Tipo | CompresiÃ³n | Query Speed | Uso |
|---------|------|------------|-------------|-----|
| **Parquet** | Columnar | Excelente (snappy/gzip) | Muy rÃ¡pido | Standard DL |
| **ORC** | Columnar | Excelente | Muy rÃ¡pido | Hive legacy |
| **Avro** | Row | Buena | Medio | Streaming, Kafka |
| **JSON** | Row | Pobre | Lento | APIs, logs |
| **CSV** | Row | Pobre | Lento | Legacy, exports |

### Â¿Por quÃ© Parquet para Data Lakes?
1. **Columnar**: Solo lee las columnas que necesitas
2. **CompresiÃ³n**: 80-90% menos espacio que CSV
3. **Schema embebido**: El schema estÃ¡ en el archivo
4. **Soporte universal**: Spark, Athena, Redshift, Pandas lo leen
5. **Predicate pushdown**: Filtra antes de leer

### Ejemplo prÃ¡ctico de ahorro
\`\`\`
1GB de datos CSV:
- Athena escanea: 1GB â†’ Costo: $0.005
- Storage: 1GB Ã— $0.023 = $0.023/mes

Mismo dato en Parquet:
- Athena escanea: 0.1GB â†’ Costo: $0.0005 (10x menos!)
- Storage: 0.15GB Ã— $0.023 = $0.003/mes
\`\`\`

### Tipos de compresiÃ³n
- **Snappy**: Balance velocidad/compresiÃ³n (default recomendado)
- **Gzip**: Mayor compresiÃ³n, mÃ¡s lento
- **LZ4**: Muy rÃ¡pido, menos compresiÃ³n
- **Zstd**: Nuevo, excelente balance`,
      en: `## File Formats for Data Lakes

### Format comparison
| Format | Type | Compression | Query Speed | Use |
|---------|------|------------|-------------|-----|
| **Parquet** | Columnar | Excellent (snappy/gzip) | Very fast | Standard DL |
| **ORC** | Columnar | Excellent | Very fast | Hive legacy |
| **Avro** | Row | Good | Medium | Streaming, Kafka |
| **JSON** | Row | Poor | Slow | APIs, logs |
| **CSV** | Row | Poor | Slow | Legacy, exports |

### Why Parquet for Data Lakes?
1. **Columnar**: Only reads columns you need
2. **Compression**: 80-90% less space than CSV
3. **Embedded schema**: Schema is in the file
4. **Universal support**: Spark, Athena, Redshift, Pandas read it
5. **Predicate pushdown**: Filters before reading

### Practical savings example
\`\`\`
1GB of CSV data:
- Athena scans: 1GB â†’ Cost: $0.005
- Storage: 1GB Ã— $0.023 = $0.023/month

Same data in Parquet:
- Athena scans: 0.1GB â†’ Cost: $0.0005 (10x less!)
- Storage: 0.15GB Ã— $0.023 = $0.003/month
\`\`\`

### Compression types
- **Snappy**: Speed/compression balance (recommended default)
- **Gzip**: Higher compression, slower
- **LZ4**: Very fast, less compression
- **Zstd**: New, excellent balance`,
      pt: `## Formatos de Arquivo para Data Lakes

### ComparaÃ§Ã£o de formatos
| Formato | Tipo | CompressÃ£o | Query Speed | Uso |
|---------|------|------------|-------------|-----|
| **Parquet** | Columnar | Excelente (snappy/gzip) | Muito rÃ¡pido | Standard DL |
| **ORC** | Columnar | Excelente | Muito rÃ¡pido | Hive legacy |
| **Avro** | Row | Boa | MÃ©dio | Streaming, Kafka |
| **JSON** | Row | Pobre | Lento | APIs, logs |
| **CSV** | Row | Pobre | Lento | Legacy, exports |

### Por que Parquet para Data Lakes?
1. **Columnar**: SÃ³ lÃª as colunas que vocÃª precisa
2. **CompressÃ£o**: 80-90% menos espaÃ§o que CSV
3. **Schema embarcado**: O schema estÃ¡ no arquivo
4. **Suporte universal**: Spark, Athena, Redshift, Pandas leem
5. **Predicate pushdown**: Filtra antes de ler

### Exemplo prÃ¡tico de economia
\`\`\`
1GB de dados CSV:
- Athena escaneia: 1GB â†’ Custo: $0.005
- Storage: 1GB Ã— $0.023 = $0.023/mÃªs

Mesmo dado em Parquet:
- Athena escaneia: 0.1GB â†’ Custo: $0.0005 (10x menos!)
- Storage: 0.15GB Ã— $0.023 = $0.003/mÃªs
\`\`\`

### Tipos de compressÃ£o
- **Snappy**: BalanÃ§o velocidade/compressÃ£o (default recomendado)
- **Gzip**: Maior compressÃ£o, mais lento
- **LZ4**: Muito rÃ¡pido, menos compressÃ£o
- **Zstd**: Novo, excelente balanÃ§o`
    },
    codeExample: {
      language: 'python',
      code: `import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

# Crear DataFrame
df = pd.DataFrame({
    'id': range(100000),
    'name': [f'User {i}' for i in range(100000)],
    'value': [i * 1.5 for i in range(100000)]
})

# Guardar como CSV
df.to_csv('data.csv', index=False)

# Guardar como Parquet con Snappy
df.to_parquet('data.parquet', compression='snappy')

# Guardar como Parquet con Gzip (mÃ¡s compresiÃ³n)
df.to_parquet('data_gzip.parquet', compression='gzip')

# Comparar tamaÃ±os
import os
print(f"CSV: {os.path.getsize('data.csv') / 1024:.2f} KB")
print(f"Parquet Snappy: {os.path.getsize('data.parquet') / 1024:.2f} KB")
print(f"Parquet Gzip: {os.path.getsize('data_gzip.parquet') / 1024:.2f} KB")`,
      explanation: { es: 'ComparaciÃ³n prÃ¡ctica de tamaÃ±os entre CSV y Parquet', en: 'Practical size comparison between CSV and Parquet', pt: 'ComparaÃ§Ã£o prÃ¡tica de tamanhos entre CSV e Parquet' }
    },
    practicalTips: [
      { es: 'âœ… Usa Parquet con Snappy para el 90% de los casos', en: 'âœ… Use Parquet with Snappy for 90% of cases', pt: 'âœ… Use Parquet com Snappy para 90% dos casos' },
      { es: 'ğŸ“¥ Para datos que vienen como JSON/CSV, conviÃ©rtelos a Parquet en la capa raw â†’ processed', en: 'ğŸ“¥ For data coming as JSON/CSV, convert to Parquet in raw â†’ processed layer', pt: 'ğŸ“¥ Para dados que vÃªm como JSON/CSV, converta para Parquet na camada raw â†’ processed' }
    ],
    externalLinks: [
      { title: 'Apache Parquet', url: 'https://parquet.apache.org/docs/', type: 'docs' },
      { title: 'AWS - Optimizing Data Storage', url: 'https://docs.aws.amazon.com/athena/latest/ug/data-types.html', type: 'aws_docs' }
    ],
    checkpoint: { es: 'âœ… Â¿Puedes convertir un CSV a Parquet y comparar los tamaÃ±os?', en: 'âœ… Can you convert a CSV to Parquet and compare sizes?', pt: 'âœ… VocÃª consegue converter um CSV para Parquet e comparar os tamanhos?' },
    xpReward: 55,
    estimatedMinutes: 30,
    services: ['S3']
  },

  {
    id: 'aws-2-6',
    stepNumber: 15,
    title: {
      es: 'Versionado y replicaciÃ³n en S3',
      en: 'Versioning and replication in S3',
      pt: 'Versionamento e replicaÃ§Ã£o no S3'
    },
    description: {
      es: 'Configurar versionado para proteger datos y replicaciÃ³n para disaster recovery.',
      en: 'Configure versioning to protect data and replication for disaster recovery.',
      pt: 'Configurar versionamento para proteger dados e replicaÃ§Ã£o para disaster recovery.'
    },
    theory: {
      es: `## Versionado y ReplicaciÃ³n en S3

### Versionado
Mantiene mÃºltiples versiones del mismo objeto:
- Cada PUT crea una nueva versiÃ³n
- DELETE marca como "delete marker" pero no borra
- Puedes restaurar versiones anteriores
- Importante para compliance y protecciÃ³n contra errores

### Estados del versionado
1. **Unversioned** (default): Sin historial
2. **Enabled**: Guarda todas las versiones
3. **Suspended**: Deja de crear versiones (las existentes persisten)

### Costos del versionado
âš ï¸ Cada versiÃ³n cuenta como almacenamiento adicional. Usa Lifecycle policies para:
- Mover versiones antiguas a IA/Glacier
- Eliminar versiones despuÃ©s de X dÃ­as
- Eliminar delete markers huÃ©rfanos

### ReplicaciÃ³n
Copia automÃ¡tica de objetos a otro bucket:

**Same-Region Replication (SRR)**:
- Compliance (mÃºltiples copias en misma regiÃ³n)
- AgregaciÃ³n de logs

**Cross-Region Replication (CRR)**:
- Disaster recovery
- Latencia geogrÃ¡fica
- Compliance multi-regiÃ³n

### ConfiguraciÃ³n de replicaciÃ³n
\`\`\`yaml
Replication Rule:
  Source: bucket-source/*
  Destination: bucket-dest-us-west-2
  IAM Role: ReplicationRole
  Options:
    - Replicate delete markers: Yes/No
    - Replica modifications sync: Yes
\`\`\``,
      en: `## Versioning and Replication in S3

### Versioning
Maintains multiple versions of the same object:
- Each PUT creates a new version
- DELETE marks as "delete marker" but doesn't delete
- You can restore previous versions
- Important for compliance and error protection

### Versioning states
1. **Unversioned** (default): No history
2. **Enabled**: Saves all versions
3. **Suspended**: Stops creating versions (existing ones persist)

### Versioning costs
âš ï¸ Each version counts as additional storage. Use Lifecycle policies to:
- Move old versions to IA/Glacier
- Delete versions after X days
- Delete orphan delete markers

### Replication
Automatic copy of objects to another bucket:

**Same-Region Replication (SRR)**:
- Compliance (multiple copies in same region)
- Log aggregation

**Cross-Region Replication (CRR)**:
- Disaster recovery
- Geographic latency
- Multi-region compliance

### Replication configuration
\`\`\`yaml
Replication Rule:
  Source: bucket-source/*
  Destination: bucket-dest-us-west-2
  IAM Role: ReplicationRole
  Options:
    - Replicate delete markers: Yes/No
    - Replica modifications sync: Yes
\`\`\``,
      pt: `## Versionamento e ReplicaÃ§Ã£o no S3

### Versionamento
MantÃ©m mÃºltiplas versÃµes do mesmo objeto:
- Cada PUT cria uma nova versÃ£o
- DELETE marca como "delete marker" mas nÃ£o deleta
- VocÃª pode restaurar versÃµes anteriores
- Importante para compliance e proteÃ§Ã£o contra erros

### Estados do versionamento
1. **Unversioned** (default): Sem histÃ³rico
2. **Enabled**: Salva todas as versÃµes
3. **Suspended**: Para de criar versÃµes (as existentes persistem)

### Custos do versionamento
âš ï¸ Cada versÃ£o conta como armazenamento adicional. Use Lifecycle policies para:
- Mover versÃµes antigas para IA/Glacier
- Deletar versÃµes apÃ³s X dias
- Deletar delete markers Ã³rfÃ£os

### ReplicaÃ§Ã£o
CÃ³pia automÃ¡tica de objetos para outro bucket:

**Same-Region Replication (SRR)**:
- Compliance (mÃºltiplas cÃ³pias na mesma regiÃ£o)
- AgregaÃ§Ã£o de logs

**Cross-Region Replication (CRR)**:
- Disaster recovery
- LatÃªncia geogrÃ¡fica
- Compliance multi-regiÃ£o

### ConfiguraÃ§Ã£o de replicaÃ§Ã£o
\`\`\`yaml
Replication Rule:
  Source: bucket-source/*
  Destination: bucket-dest-us-west-2
  IAM Role: ReplicationRole
  Options:
    - Replicate delete markers: Yes/No
    - Replica modifications sync: Yes
\`\`\``
    },
    practicalTips: [
      { es: 'ğŸ›¡ï¸ Habilita versionado en buckets de datos importantes (procesados, serving)', en: 'ğŸ›¡ï¸ Enable versioning on important data buckets (processed, serving)', pt: 'ğŸ›¡ï¸ Habilite versionamento em buckets de dados importantes (processados, serving)' },
      { es: 'ğŸ’° Siempre combina versionado con lifecycle policies para controlar costos', en: 'ğŸ’° Always combine versioning with lifecycle policies to control costs', pt: 'ğŸ’° Sempre combine versionamento com lifecycle policies para controlar custos' }
    ],
    externalLinks: [
      { title: 'S3 Versioning', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html', type: 'aws_docs' },
      { title: 'S3 Replication', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html', type: 'aws_docs' }
    ],
    checkpoint: { es: 'âœ… Â¿Habilitaste versionado en tu bucket y probaste restaurar una versiÃ³n anterior?', en: 'âœ… Did you enable versioning on your bucket and try restoring a previous version?', pt: 'âœ… VocÃª habilitou versionamento no seu bucket e testou restaurar uma versÃ£o anterior?' },
    xpReward: 50,
    estimatedMinutes: 30,
    services: ['S3']
  },

  {
    id: 'aws-2-7',
    stepNumber: 16,
    title: {
      es: 'EncriptaciÃ³n y seguridad en S3',
      en: 'Encryption and security in S3',
      pt: 'EncriptaÃ§Ã£o e seguranÃ§a no S3'
    },
    description: {
      es: 'Implementar encriptaciÃ³n at-rest y in-transit, y entender bucket policies.',
      en: 'Implement at-rest and in-transit encryption, and understand bucket policies.',
      pt: 'Implementar encriptaÃ§Ã£o at-rest e in-transit, e entender bucket policies.'
    },
    theory: {
      es: `## Seguridad en S3

### EncriptaciÃ³n At-Rest (datos almacenados)

**SSE-S3 (Server-Side Encryption with S3 keys)**:
- AWS maneja las keys automÃ¡ticamente
- Sin costo adicional
- Default recomendado para la mayorÃ­a de casos

**SSE-KMS (Server-Side Encryption with KMS)**:
- TÃº controlas las keys en KMS
- Audit trail de uso de keys
- Costo adicional por API calls a KMS
- Recomendado para datos sensibles

**SSE-C (Server-Side Encryption with Customer keys)**:
- TÃº provees la key en cada request
- AWS no guarda la key
- MÃ¡ximo control, mÃ¡s complejidad

### EncriptaciÃ³n In-Transit
SIEMPRE usa HTTPS (SSL/TLS). Puedes forzarlo con bucket policy:
\`\`\`json
{
  "Statement": [{
    "Effect": "Deny",
    "Principal": "*",
    "Action": "s3:*",
    "Resource": "arn:aws:s3:::bucket/*",
    "Condition": {
      "Bool": {"aws:SecureTransport": "false"}
    }
  }]
}
\`\`\`

### Bucket Policies vs IAM Policies
- **Bucket Policy**: Attached al bucket, controla quiÃ©n accede
- **IAM Policy**: Attached al usuario/rol, controla quÃ© puede hacer
- Ambas se evalÃºan juntas (explicit deny wins)

### Access Points
Puntos de acceso especÃ­ficos para diferentes usuarios/aplicaciones con sus propias policies.`,
      en: `## Security in S3

### At-Rest Encryption (stored data)

**SSE-S3 (Server-Side Encryption with S3 keys)**:
- AWS manages keys automatically
- No additional cost
- Recommended default for most cases

**SSE-KMS (Server-Side Encryption with KMS)**:
- You control keys in KMS
- Audit trail of key usage
- Additional cost for KMS API calls
- Recommended for sensitive data

**SSE-C (Server-Side Encryption with Customer keys)**:
- You provide the key in each request
- AWS doesn't store the key
- Maximum control, more complexity

### In-Transit Encryption
ALWAYS use HTTPS (SSL/TLS). You can enforce it with bucket policy:
\`\`\`json
{
  "Statement": [{
    "Effect": "Deny",
    "Principal": "*",
    "Action": "s3:*",
    "Resource": "arn:aws:s3:::bucket/*",
    "Condition": {
      "Bool": {"aws:SecureTransport": "false"}
    }
  }]
}
\`\`\`

### Bucket Policies vs IAM Policies
- **Bucket Policy**: Attached to bucket, controls who accesses
- **IAM Policy**: Attached to user/role, controls what they can do
- Both are evaluated together (explicit deny wins)

### Access Points
Specific access points for different users/applications with their own policies.`,
      pt: `## SeguranÃ§a no S3

### EncriptaÃ§Ã£o At-Rest (dados armazenados)

**SSE-S3 (Server-Side Encryption with S3 keys)**:
- AWS gerencia as keys automaticamente
- Sem custo adicional
- Default recomendado para a maioria dos casos

**SSE-KMS (Server-Side Encryption with KMS)**:
- VocÃª controla as keys no KMS
- Audit trail de uso das keys
- Custo adicional por API calls ao KMS
- Recomendado para dados sensÃ­veis

**SSE-C (Server-Side Encryption with Customer keys)**:
- VocÃª provÃª a key em cada request
- AWS nÃ£o guarda a key
- MÃ¡ximo controle, mais complexidade

### EncriptaÃ§Ã£o In-Transit
SEMPRE use HTTPS (SSL/TLS). VocÃª pode forÃ§Ã¡-lo com bucket policy:
\`\`\`json
{
  "Statement": [{
    "Effect": "Deny",
    "Principal": "*",
    "Action": "s3:*",
    "Resource": "arn:aws:s3:::bucket/*",
    "Condition": {
      "Bool": {"aws:SecureTransport": "false"}
    }
  }]
}
\`\`\`

### Bucket Policies vs IAM Policies
- **Bucket Policy**: Attached ao bucket, controla quem acessa
- **IAM Policy**: Attached ao usuÃ¡rio/role, controla o que pode fazer
- Ambas sÃ£o avaliadas juntas (explicit deny wins)

### Access Points
Pontos de acesso especÃ­ficos para diferentes usuÃ¡rios/aplicaÃ§Ãµes com suas prÃ³prias policies.`
    },
    practicalTips: [
      { es: 'ğŸ” Habilita SSE-S3 como default en todos tus buckets', en: 'ğŸ” Enable SSE-S3 as default on all your buckets', pt: 'ğŸ” Habilite SSE-S3 como default em todos os seus buckets' },
      { es: 'ğŸ“‹ Usa SSE-KMS cuando necesites audit trail o compliance especÃ­fico', en: 'ğŸ“‹ Use SSE-KMS when you need audit trail or specific compliance', pt: 'ğŸ“‹ Use SSE-KMS quando precisar de audit trail ou compliance especÃ­fico' }
    ],
    externalLinks: [
      { title: 'S3 Encryption', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html', type: 'aws_docs' },
      { title: 'S3 Bucket Policies', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html', type: 'aws_docs' }
    ],
    checkpoint: { es: 'âœ… Â¿Configuraste default encryption en tu bucket?', en: 'âœ… Did you configure default encryption on your bucket?', pt: 'âœ… VocÃª configurou default encryption no seu bucket?' },
    xpReward: 55,
    estimatedMinutes: 35,
    services: ['S3', 'KMS']
  },

  {
    id: 'aws-2-8',
    stepNumber: 17,
    title: {
      es: 'S3 Select y optimizaciÃ³n de queries',
      en: 'S3 Select and query optimization',
      pt: 'S3 Select e otimizaÃ§Ã£o de queries'
    },
    description: {
      es: 'Usar S3 Select para filtrar datos antes de descargarlos y optimizar transferencia.',
      en: 'Use S3 Select to filter data before downloading and optimize transfer.',
      pt: 'Usar S3 Select para filtrar dados antes de baixar e otimizar transferÃªncia.'
    },
    theory: {
      es: `## S3 Select - SQL Directamente en S3

### Â¿QuÃ© es S3 Select?
Ejecuta SQL simple directamente sobre objetos en S3 sin descargarlos completamente. Solo transfieres los datos que necesitas.

### Beneficios
- **Hasta 400% mÃ¡s rÃ¡pido**: Menos datos transferidos
- **Hasta 80% menos costoso**: Solo pagas por datos escaneados
- **Soporta**: CSV, JSON, Parquet

### Ejemplo de uso
\`\`\`python
import boto3

s3 = boto3.client('s3')

response = s3.select_object_content(
    Bucket='my-bucket',
    Key='data/users.parquet',
    ExpressionType='SQL',
    Expression="SELECT name, email FROM s3object WHERE age > 25",
    InputSerialization={'Parquet': {}},
    OutputSerialization={'JSON': {}}
)

for event in response['Payload']:
    if 'Records' in event:
        print(event['Records']['Payload'].decode())
\`\`\`

### Limitaciones
- Queries simples (sin JOINs, subqueries limitadas)
- Max 256 columnas
- Parquet: solo columnas de tipos soportados
- No reemplaza Athena para queries complejas

### CuÃ¡ndo usar S3 Select vs Athena
| S3 Select | Athena |
|-----------|--------|
| Filtros simples en un archivo | Queries complejas multi-archivo |
| IntegraciÃ³n en cÃ³digo | AnÃ¡lisis ad-hoc |
| Bajo latencia, archivo Ãºnico | Grandes volÃºmenes, JOINs |`,
      en: `## S3 Select - SQL Directly on S3

### What is S3 Select?
Execute simple SQL directly on objects in S3 without downloading them completely. You only transfer the data you need.

### Benefits
- **Up to 400% faster**: Less data transferred
- **Up to 80% less expensive**: You only pay for data scanned
- **Supports**: CSV, JSON, Parquet

### Usage example
\`\`\`python
import boto3

s3 = boto3.client('s3')

response = s3.select_object_content(
    Bucket='my-bucket',
    Key='data/users.parquet',
    ExpressionType='SQL',
    Expression="SELECT name, email FROM s3object WHERE age > 25",
    InputSerialization={'Parquet': {}},
    OutputSerialization={'JSON': {}}
)

for event in response['Payload']:
    if 'Records' in event:
        print(event['Records']['Payload'].decode())
\`\`\`

### Limitations
- Simple queries (no JOINs, limited subqueries)
- Max 256 columns
- Parquet: only supported column types
- Doesn't replace Athena for complex queries

### When to use S3 Select vs Athena
| S3 Select | Athena |
|-----------|--------|
| Simple filters on one file | Complex multi-file queries |
| Code integration | Ad-hoc analysis |
| Low latency, single file | Large volumes, JOINs |`,
      pt: `## S3 Select - SQL Diretamente no S3

### O que Ã© S3 Select?
Execute SQL simples diretamente sobre objetos no S3 sem baixÃ¡-los completamente. VocÃª sÃ³ transfere os dados que precisa.

### BenefÃ­cios
- **AtÃ© 400% mais rÃ¡pido**: Menos dados transferidos
- **AtÃ© 80% menos custoso**: VocÃª sÃ³ paga pelos dados escaneados
- **Suporta**: CSV, JSON, Parquet

### Exemplo de uso
\`\`\`python
import boto3

s3 = boto3.client('s3')

response = s3.select_object_content(
    Bucket='my-bucket',
    Key='data/users.parquet',
    ExpressionType='SQL',
    Expression="SELECT name, email FROM s3object WHERE age > 25",
    InputSerialization={'Parquet': {}},
    OutputSerialization={'JSON': {}}
)

for event in response['Payload']:
    if 'Records' in event:
        print(event['Records']['Payload'].decode())
\`\`\`

### LimitaÃ§Ãµes
- Queries simples (sem JOINs, subqueries limitadas)
- Max 256 colunas
- Parquet: apenas tipos de colunas suportados
- NÃ£o substitui Athena para queries complexas

### Quando usar S3 Select vs Athena
| S3 Select | Athena |
|-----------|--------|
| Filtros simples em um arquivo | Queries complexas multi-arquivo |
| IntegraÃ§Ã£o em cÃ³digo | AnÃ¡lise ad-hoc |
| Baixa latÃªncia, arquivo Ãºnico | Grandes volumes, JOINs |`
    },
    practicalTips: [
      { es: 'âš¡ Usa S3 Select en Lambda functions para procesar archivos grandes sin cargarlos completos en memoria', en: 'âš¡ Use S3 Select in Lambda functions to process large files without loading them completely in memory', pt: 'âš¡ Use S3 Select em Lambda functions para processar arquivos grandes sem carregÃ¡-los completamente na memÃ³ria' }
    ],
    externalLinks: [
      { title: 'S3 Select Documentation', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html', type: 'aws_docs' }
    ],
    checkpoint: { es: 'âœ… Â¿Probaste S3 Select con un archivo CSV o Parquet?', en: 'âœ… Did you try S3 Select with a CSV or Parquet file?', pt: 'âœ… VocÃª testou S3 Select com um arquivo CSV ou Parquet?' },
    xpReward: 50,
    estimatedMinutes: 30,
    services: ['S3']
  },

  {
    id: 'aws-2-9',
    stepNumber: 18,
    title: {
      es: 'Event Notifications y integraciÃ³n con Lambda',
      en: 'Event Notifications and Lambda integration',
      pt: 'Event Notifications e integraÃ§Ã£o com Lambda'
    },
    description: {
      es: 'Configurar notificaciones de eventos S3 para disparar pipelines automÃ¡ticamente.',
      en: 'Configure S3 event notifications to trigger pipelines automatically.',
      pt: 'Configurar notificaÃ§Ãµes de eventos S3 para disparar pipelines automaticamente.'
    },
    theory: {
      es: `## S3 Event Notifications

### Â¿QuÃ© son?
Notificaciones automÃ¡ticas cuando algo pasa en un bucket: crear, eliminar, restaurar objetos.

### Destinos posibles
1. **Lambda**: Ejecutar cÃ³digo serverless
2. **SQS**: Encolar mensajes para procesamiento
3. **SNS**: Notificaciones a mÃºltiples suscriptores
4. **EventBridge**: Routing avanzado de eventos

### Eventos disponibles
- \`s3:ObjectCreated:*\`: Cualquier creaciÃ³n
- \`s3:ObjectCreated:Put\`: PUT especÃ­fico
- \`s3:ObjectRemoved:*\`: Cualquier eliminaciÃ³n
- \`s3:ObjectRestore:Completed\`: Restore de Glacier

### Caso de uso tÃ­pico: Pipeline automÃ¡tico
\`\`\`
Archivo llega a s3://bucket/raw/
    â†“
S3 Event Notification
    â†“
Lambda function dispara Glue Job
    â†“
Glue procesa a s3://bucket/processed/
    â†“
S3 Event Notification (processed)
    â†“
Lambda actualiza Data Catalog
\`\`\`

### Filtros de prefijo y sufijo
Puedes filtrar quÃ© objetos disparan eventos:
- Prefijo: \`raw/incoming/\`
- Sufijo: \`.csv\` o \`.parquet\``,
      en: `## S3 Event Notifications

### What are they?
Automatic notifications when something happens in a bucket: create, delete, restore objects.

### Possible destinations
1. **Lambda**: Execute serverless code
2. **SQS**: Queue messages for processing
3. **SNS**: Notifications to multiple subscribers
4. **EventBridge**: Advanced event routing

### Available events
- \`s3:ObjectCreated:*\`: Any creation
- \`s3:ObjectCreated:Put\`: Specific PUT
- \`s3:ObjectRemoved:*\`: Any deletion
- \`s3:ObjectRestore:Completed\`: Glacier restore

### Typical use case: Automatic pipeline
\`\`\`
File arrives at s3://bucket/raw/
    â†“
S3 Event Notification
    â†“
Lambda function triggers Glue Job
    â†“
Glue processes to s3://bucket/processed/
    â†“
S3 Event Notification (processed)
    â†“
Lambda updates Data Catalog
\`\`\`

### Prefix and suffix filters
You can filter which objects trigger events:
- Prefix: \`raw/incoming/\`
- Suffix: \`.csv\` or \`.parquet\``,
      pt: `## S3 Event Notifications

### O que sÃ£o?
NotificaÃ§Ãµes automÃ¡ticas quando algo acontece em um bucket: criar, deletar, restaurar objetos.

### Destinos possÃ­veis
1. **Lambda**: Executar cÃ³digo serverless
2. **SQS**: Enfileirar mensagens para processamento
3. **SNS**: NotificaÃ§Ãµes para mÃºltiplos assinantes
4. **EventBridge**: Roteamento avanÃ§ado de eventos

### Eventos disponÃ­veis
- \`s3:ObjectCreated:*\`: Qualquer criaÃ§Ã£o
- \`s3:ObjectCreated:Put\`: PUT especÃ­fico
- \`s3:ObjectRemoved:*\`: Qualquer deleÃ§Ã£o
- \`s3:ObjectRestore:Completed\`: Restore do Glacier

### Caso de uso tÃ­pico: Pipeline automÃ¡tico
\`\`\`
Arquivo chega em s3://bucket/raw/
    â†“
S3 Event Notification
    â†“
Lambda function dispara Glue Job
    â†“
Glue processa para s3://bucket/processed/
    â†“
S3 Event Notification (processed)
    â†“
Lambda atualiza Data Catalog
\`\`\`

### Filtros de prefixo e sufixo
VocÃª pode filtrar quais objetos disparam eventos:
- Prefixo: \`raw/incoming/\`
- Sufixo: \`.csv\` ou \`.parquet\``
    },
    codeExample: {
      language: 'python',
      code: `# Lambda function example triggered by S3
import json
import boto3

def lambda_handler(event, context):
    # Parse S3 event
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        size = record['s3']['object']['size']
        
        print(f"New file: s3://{bucket}/{key} ({size} bytes)")
        
        # Trigger Glue job if it's a CSV
        if key.endswith('.csv'):
            glue = boto3.client('glue')
            glue.start_job_run(
                JobName='my-etl-job',
                Arguments={
                    '--input_path': f's3://{bucket}/{key}'
                }
            )
    
    return {'statusCode': 200}`,
      explanation: { es: 'Lambda que procesa eventos S3 y dispara un Glue job', en: 'Lambda that processes S3 events and triggers a Glue job', pt: 'Lambda que processa eventos S3 e dispara um Glue job' }
    },
    practicalTips: [
      { es: 'ğŸ”” Usa EventBridge en lugar de SNS/SQS cuando necesites routing complejo de eventos', en: 'ğŸ”” Use EventBridge instead of SNS/SQS when you need complex event routing', pt: 'ğŸ”” Use EventBridge em vez de SNS/SQS quando precisar de roteamento complexo de eventos' }
    ],
    externalLinks: [
      { title: 'S3 Event Notifications', url: 'https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html', type: 'aws_docs' }
    ],
    checkpoint: { es: 'âœ… Â¿Configuraste una notificaciÃ³n que dispara Lambda cuando subes un archivo?', en: 'âœ… Did you configure a notification that triggers Lambda when you upload a file?', pt: 'âœ… VocÃª configurou uma notificaÃ§Ã£o que dispara Lambda quando faz upload de um arquivo?' },
    xpReward: 65,
    estimatedMinutes: 40,
    services: ['S3', 'Lambda', 'EventBridge']
  },

  {
    id: 'aws-2-10',
    stepNumber: 19,
    title: {
      es: 'Arquitectura de Data Lake con S3',
      en: 'Data Lake architecture with S3',
      pt: 'Arquitetura de Data Lake com S3'
    },
    description: {
      es: 'DiseÃ±ar la estructura completa de un Data Lake con zonas raw, processed, y serving.',
      en: 'Design the complete structure of a Data Lake with raw, processed, and serving zones.',
      pt: 'Projetar a estrutura completa de um Data Lake com zonas raw, processed e serving.'
    },
    theory: {
      es: `## Arquitectura de Data Lake en S3

### Modelo de zonas (Medallion Architecture)
\`\`\`
s3://company-datalake-{env}/
â”œâ”€â”€ raw/              # Bronze: Datos crudos, sin modificar
â”‚   â”œâ”€â”€ source1/
â”‚   â”‚   â””â”€â”€ year=2024/month=01/
â”‚   â””â”€â”€ source2/
â”‚
â”œâ”€â”€ processed/        # Silver: Datos limpios, transformados
â”‚   â”œâ”€â”€ domain1/
â”‚   â”‚   â””â”€â”€ table1/
â”‚   â””â”€â”€ domain2/
â”‚
â”œâ”€â”€ serving/          # Gold: Datos listos para consumo
â”‚   â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ ml/
â”‚   â””â”€â”€ reporting/
â”‚
â”œâ”€â”€ temp/             # Archivos temporales (lifecycle: 7 dÃ­as)
â”‚
â””â”€â”€ archive/          # Datos histÃ³ricos (Glacier)
\`\`\`

### Convenciones por zona

**Raw (Bronze)**:
- Formato original (JSON, CSV, etc.)
- Particionado por fecha de llegada
- Inmutable (nunca modificar)
- Versionado habilitado

**Processed (Silver)**:
- Formato Parquet
- Particionado por lÃ³gica de negocio
- Schema validado
- Datos deduplicados y limpios

**Serving (Gold)**:
- Formato Parquet optimizado
- Agregaciones pre-calculadas
- MÃ©tricas de negocio
- Acceso directo por analistas/BI

### Tags recomendados
\`\`\`
Environment: dev|staging|prod
DataClassification: public|internal|confidential|restricted
Owner: data-team
CostCenter: analytics
Retention: 30d|90d|1y|7y|forever
\`\`\``,
      en: `## Data Lake Architecture in S3

### Zone model (Medallion Architecture)
\`\`\`
s3://company-datalake-{env}/
â”œâ”€â”€ raw/              # Bronze: Raw, unmodified data
â”‚   â”œâ”€â”€ source1/
â”‚   â”‚   â””â”€â”€ year=2024/month=01/
â”‚   â””â”€â”€ source2/
â”‚
â”œâ”€â”€ processed/        # Silver: Clean, transformed data
â”‚   â”œâ”€â”€ domain1/
â”‚   â”‚   â””â”€â”€ table1/
â”‚   â””â”€â”€ domain2/
â”‚
â”œâ”€â”€ serving/          # Gold: Ready-to-consume data
â”‚   â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ ml/
â”‚   â””â”€â”€ reporting/
â”‚
â”œâ”€â”€ temp/             # Temporary files (lifecycle: 7 days)
â”‚
â””â”€â”€ archive/          # Historical data (Glacier)
\`\`\`

### Conventions by zone

**Raw (Bronze)**:
- Original format (JSON, CSV, etc.)
- Partitioned by arrival date
- Immutable (never modify)
- Versioning enabled

**Processed (Silver)**:
- Parquet format
- Partitioned by business logic
- Validated schema
- Deduplicated and clean data

**Serving (Gold)**:
- Optimized Parquet format
- Pre-calculated aggregations
- Business metrics
- Direct access by analysts/BI

### Recommended tags
\`\`\`
Environment: dev|staging|prod
DataClassification: public|internal|confidential|restricted
Owner: data-team
CostCenter: analytics
Retention: 30d|90d|1y|7y|forever
\`\`\``,
      pt: `## Arquitetura de Data Lake no S3

### Modelo de zonas (Medallion Architecture)
\`\`\`
s3://company-datalake-{env}/
â”œâ”€â”€ raw/              # Bronze: Dados brutos, nÃ£o modificados
â”‚   â”œâ”€â”€ source1/
â”‚   â”‚   â””â”€â”€ year=2024/month=01/
â”‚   â””â”€â”€ source2/
â”‚
â”œâ”€â”€ processed/        # Silver: Dados limpos, transformados
â”‚   â”œâ”€â”€ domain1/
â”‚   â”‚   â””â”€â”€ table1/
â”‚   â””â”€â”€ domain2/
â”‚
â”œâ”€â”€ serving/          # Gold: Dados prontos para consumo
â”‚   â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ ml/
â”‚   â””â”€â”€ reporting/
â”‚
â”œâ”€â”€ temp/             # Arquivos temporÃ¡rios (lifecycle: 7 dias)
â”‚
â””â”€â”€ archive/          # Dados histÃ³ricos (Glacier)
\`\`\`

### ConvenÃ§Ãµes por zona

**Raw (Bronze)**:
- Formato original (JSON, CSV, etc.)
- Particionado por data de chegada
- ImutÃ¡vel (nunca modificar)
- Versionamento habilitado

**Processed (Silver)**:
- Formato Parquet
- Particionado por lÃ³gica de negÃ³cio
- Schema validado
- Dados deduplicados e limpos

**Serving (Gold)**:
- Formato Parquet otimizado
- AgregaÃ§Ãµes prÃ©-calculadas
- MÃ©tricas de negÃ³cio
- Acesso direto por analistas/BI

### Tags recomendadas
\`\`\`
Environment: dev|staging|prod
DataClassification: public|internal|confidential|restricted
Owner: data-team
CostCenter: analytics
Retention: 30d|90d|1y|7y|forever
\`\`\``
    },
    practicalTips: [
      { es: 'ğŸ—ï¸ Crea buckets separados por ambiente (dev/staging/prod) para mejor aislamiento', en: 'ğŸ—ï¸ Create separate buckets by environment (dev/staging/prod) for better isolation', pt: 'ğŸ—ï¸ Crie buckets separados por ambiente (dev/staging/prod) para melhor isolamento' },
      { es: 'ğŸ“Š La zona serving debe ser la mÃ¡s optimizada - aquÃ­ es donde los usuarios consultan', en: 'ğŸ“Š The serving zone should be the most optimized - this is where users query', pt: 'ğŸ“Š A zona serving deve ser a mais otimizada - aqui Ã© onde os usuÃ¡rios consultam' }
    ],
    externalLinks: [
      { title: 'Building Data Lakes on AWS', url: 'https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html', type: 'aws_docs' },
      { title: 'Data Lake Architecture Best Practices', url: 'https://aws.amazon.com/blogs/big-data/build-a-lake-house-architecture-on-aws/', type: 'article' }
    ],
    checkpoint: { es: 'âœ… Â¿Puedes diseÃ±ar un Data Lake con las 3 zonas y explicar quÃ© va en cada una?', en: 'âœ… Can you design a Data Lake with 3 zones and explain what goes in each?', pt: 'âœ… VocÃª consegue projetar um Data Lake com 3 zonas e explicar o que vai em cada uma?' },
    interviewTips: [
      { es: 'Pregunta MUY comÃºn: "DiseÃ±a un Data Lake en AWS". Dibuja las zonas, menciona S3, Glue Catalog, Athena, y Lake Formation', en: 'VERY common question: "Design a Data Lake in AWS". Draw the zones, mention S3, Glue Catalog, Athena, and Lake Formation', pt: 'Pergunta MUITO comum: "Projete um Data Lake na AWS". Desenhe as zonas, mencione S3, Glue Catalog, Athena e Lake Formation' }
    ],
    certificationNotes: { es: 'Este tema es CRÃTICO para la certificaciÃ³n. El 20%+ de las preguntas involucran arquitectura de Data Lake', en: 'This topic is CRITICAL for certification. 20%+ of questions involve Data Lake architecture', pt: 'Este tema Ã© CRÃTICO para a certificaÃ§Ã£o. 20%+ das perguntas envolvem arquitetura de Data Lake' },
    xpReward: 80,
    estimatedMinutes: 45,
    services: ['S3', 'Glue Data Catalog', 'Lake Formation']
  }
];








