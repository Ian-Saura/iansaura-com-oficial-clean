/**
 * FASE 4: AWS GLUE - ETL SERVERLESS
 * 10 pasos para dominar Glue
 */

import { AWSStep } from '../types';

export const phase4Steps: AWSStep[] = [
  {
    id: 'aws-4-1', stepNumber: 28,
    title: { es: 'Introducci√≥n a AWS Glue', en: 'Introduction to AWS Glue', pt: 'Introdu√ß√£o ao AWS Glue' },
    description: { es: 'Entender qu√© es Glue, sus componentes y cu√°ndo usarlo.', en: 'Understand what Glue is, its components and when to use it.', pt: 'Entender o que √© Glue, seus componentes e quando us√°-lo.' },
    theory: {
      es: `## AWS Glue - ETL Serverless

### Componentes de Glue
1. **Data Catalog**: Metastore central (bases de datos, tablas, schemas)
2. **Crawlers**: Descubren schema autom√°ticamente
3. **ETL Jobs**: Transformaciones con PySpark/Python
4. **Workflows**: Orquestaci√≥n de jobs
5. **Studio**: IDE visual para ETL

### ¬øPor qu√© Glue?
- **Serverless**: Sin servidores que gestionar
- **PySpark**: Lenguaje est√°ndar de la industria
- **Integrado**: S3, Athena, Redshift lo usan nativamente
- **Escalable**: De MBs a PBs

### Pricing
- **Crawler**: $0.44/DPU-hora
- **ETL Job**: $0.44/DPU-hora
- **Data Catalog**: 1M objetos gratis, luego $1/100K objetos
- **Studio**: Basado en sesiones

### DPU (Data Processing Unit)
1 DPU = 4 vCPU + 16GB RAM
Jobs usan m√≠nimo 2 DPUs (Spark) o 0.0625 DPU (Python Shell)`,
      en: `## AWS Glue - Serverless ETL

### Glue Components
1. **Data Catalog**: Central metastore (databases, tables, schemas)
2. **Crawlers**: Discover schema automatically
3. **ETL Jobs**: Transformations with PySpark/Python
4. **Workflows**: Job orchestration
5. **Studio**: Visual IDE for ETL

### Why Glue?
- **Serverless**: No servers to manage
- **PySpark**: Industry standard language
- **Integrated**: S3, Athena, Redshift use it natively
- **Scalable**: From MBs to PBs

### Pricing
- **Crawler**: $0.44/DPU-hour
- **ETL Job**: $0.44/DPU-hour
- **Data Catalog**: 1M objects free, then $1/100K objects
- **Studio**: Session-based

### DPU (Data Processing Unit)
1 DPU = 4 vCPU + 16GB RAM
Jobs use minimum 2 DPUs (Spark) or 0.0625 DPU (Python Shell)`,
      pt: `## AWS Glue - ETL Serverless

### Componentes do Glue
1. **Data Catalog**: Metastore central (bases de dados, tabelas, schemas)
2. **Crawlers**: Descobrem schema automaticamente
3. **ETL Jobs**: Transforma√ß√µes com PySpark/Python
4. **Workflows**: Orquestra√ß√£o de jobs
5. **Studio**: IDE visual para ETL

### Por que Glue?
- **Serverless**: Sem servidores para gerenciar
- **PySpark**: Linguagem padr√£o da ind√∫stria
- **Integrado**: S3, Athena, Redshift usam nativamente
- **Escal√°vel**: De MBs a PBs

### Pricing
- **Crawler**: $0.44/DPU-hora
- **ETL Job**: $0.44/DPU-hora
- **Data Catalog**: 1M objetos gr√°tis, depois $1/100K objetos
- **Studio**: Baseado em sess√µes

### DPU (Data Processing Unit)
1 DPU = 4 vCPU + 16GB RAM
Jobs usam m√≠nimo 2 DPUs (Spark) ou 0.0625 DPU (Python Shell)`
    },
    practicalTips: [
      { es: 'üí° Glue aparece en 80%+ de arquitecturas AWS de datos - es esencial dominarlo', en: 'üí° Glue appears in 80%+ of AWS data architectures - mastering it is essential', pt: 'üí° Glue aparece em 80%+ das arquiteturas AWS de dados - √© essencial domin√°-lo' }
    ],
    externalLinks: [
      { title: 'AWS Glue Developer Guide', url: 'https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øPuedes explicar los 5 componentes principales de Glue?', en: '‚úÖ Can you explain the 5 main components of Glue?', pt: '‚úÖ Voc√™ consegue explicar os 5 componentes principais do Glue?' },
    xpReward: 50, estimatedMinutes: 30, services: ['Glue']
  },

  {
    id: 'aws-4-2', stepNumber: 29,
    title: { es: 'Glue Data Catalog', en: 'Glue Data Catalog', pt: 'Glue Data Catalog' },
    description: { es: 'Usar el Data Catalog como metastore central para tu Data Lake.', en: 'Use Data Catalog as central metastore for your Data Lake.', pt: 'Usar o Data Catalog como metastore central para seu Data Lake.' },
    theory: {
      es: `## Glue Data Catalog - Tu Metastore Central

### Jerarqu√≠a
\`\`\`
Account
‚îî‚îÄ‚îÄ Databases
    ‚îî‚îÄ‚îÄ Tables
        ‚îú‚îÄ‚îÄ Columns
        ‚îú‚îÄ‚îÄ Partitions
        ‚îî‚îÄ‚îÄ Properties
\`\`\`

### ¬øQu√© almacena?
- **Schema**: Nombres y tipos de columnas
- **Location**: Path S3 de los datos
- **Partitions**: Keys de particionamiento
- **Table properties**: SerDe, formato, compresi√≥n
- **Statistics**: Para optimizaci√≥n de queries

### Integraci√≥n nativa con
- **Athena**: Usa Catalog como metastore
- **Redshift Spectrum**: Query externas
- **EMR**: Hive metastore compatible
- **Lake Formation**: Permisos granulares

### Crear tabla manualmente
\`\`\`sql
CREATE EXTERNAL TABLE events (
  event_id STRING,
  user_id STRING,
  event_type STRING,
  timestamp TIMESTAMP
)
PARTITIONED BY (year STRING, month STRING, day STRING)
STORED AS PARQUET
LOCATION 's3://bucket/processed/events/';
\`\`\``,
      en: `## Glue Data Catalog - Your Central Metastore

### Hierarchy
\`\`\`
Account
‚îî‚îÄ‚îÄ Databases
    ‚îî‚îÄ‚îÄ Tables
        ‚îú‚îÄ‚îÄ Columns
        ‚îú‚îÄ‚îÄ Partitions
        ‚îî‚îÄ‚îÄ Properties
\`\`\`

### What does it store?
- **Schema**: Column names and types
- **Location**: S3 path of data
- **Partitions**: Partition keys
- **Table properties**: SerDe, format, compression
- **Statistics**: For query optimization

### Native integration with
- **Athena**: Uses Catalog as metastore
- **Redshift Spectrum**: External queries
- **EMR**: Hive metastore compatible
- **Lake Formation**: Granular permissions

### Create table manually
\`\`\`sql
CREATE EXTERNAL TABLE events (
  event_id STRING,
  user_id STRING,
  event_type STRING,
  timestamp TIMESTAMP
)
PARTITIONED BY (year STRING, month STRING, day STRING)
STORED AS PARQUET
LOCATION 's3://bucket/processed/events/';
\`\`\``,
      pt: `## Glue Data Catalog - Seu Metastore Central

### Hierarquia
\`\`\`
Account
‚îî‚îÄ‚îÄ Databases
    ‚îî‚îÄ‚îÄ Tables
        ‚îú‚îÄ‚îÄ Columns
        ‚îú‚îÄ‚îÄ Partitions
        ‚îî‚îÄ‚îÄ Properties
\`\`\`

### O que armazena?
- **Schema**: Nomes e tipos de colunas
- **Location**: Path S3 dos dados
- **Partitions**: Keys de particionamento
- **Table properties**: SerDe, formato, compress√£o
- **Statistics**: Para otimiza√ß√£o de queries

### Integra√ß√£o nativa com
- **Athena**: Usa Catalog como metastore
- **Redshift Spectrum**: Queries externas
- **EMR**: Compat√≠vel com Hive metastore
- **Lake Formation**: Permiss√µes granulares

### Criar tabela manualmente
\`\`\`sql
CREATE EXTERNAL TABLE events (
  event_id STRING,
  user_id STRING,
  event_type STRING,
  timestamp TIMESTAMP
)
PARTITIONED BY (year STRING, month STRING, day STRING)
STORED AS PARQUET
LOCATION 's3://bucket/processed/events/';
\`\`\``
    },
    practicalTips: [
      { es: 'üìö El Data Catalog es el "diccionario" de tu Data Lake - mantenlo organizado', en: 'üìö Data Catalog is the "dictionary" of your Data Lake - keep it organized', pt: 'üìö O Data Catalog √© o "dicion√°rio" do seu Data Lake - mantenha-o organizado' }
    ],
    externalLinks: [
      { title: 'Glue Data Catalog', url: 'https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øCreaste una database y tabla en el Data Catalog?', en: '‚úÖ Did you create a database and table in Data Catalog?', pt: '‚úÖ Voc√™ criou uma database e tabela no Data Catalog?' },
    xpReward: 55, estimatedMinutes: 30, services: ['Glue Data Catalog']
  },

  {
    id: 'aws-4-3', stepNumber: 30,
    title: { es: 'Glue Crawlers', en: 'Glue Crawlers', pt: 'Glue Crawlers' },
    description: { es: 'Crear crawlers para descubrir y catalogar datos autom√°ticamente.', en: 'Create crawlers to automatically discover and catalog data.', pt: 'Criar crawlers para descobrir e catalogar dados automaticamente.' },
    theory: {
      es: `## Glue Crawlers - Descubrimiento Autom√°tico

### ¬øQu√© hace un Crawler?
1. Escanea path S3 (u otra fuente)
2. Infiere el schema de los datos
3. Detecta particiones
4. Crea/actualiza tablas en Data Catalog

### Configuraci√≥n de Crawler
\`\`\`yaml
Crawler: raw-events-crawler
  Data sources: s3://bucket/raw/events/
  IAM role: GlueCrawlerRole
  Database: raw_db
  Table prefix: raw_
  Schedule: Run on demand / Every hour
  Schema change policy: Update in place
  Partition behavior: Add new partitions only
\`\`\`

### Classifiers
Glue usa classifiers para detectar formatos:
- **Built-in**: JSON, CSV, Parquet, ORC, Avro, XML
- **Custom**: Grok patterns, JSON paths, XML tags

### ‚ö†Ô∏è Consideraciones
- Crawlers pueden ser lentos para millones de archivos
- Cuidado con detecci√≥n de tipos incorrecta
- Usa exclusions para ignorar archivos no deseados
- Considera crear tablas manualmente para mayor control`,
      en: `## Glue Crawlers - Automatic Discovery

### What does a Crawler do?
1. Scans S3 path (or other source)
2. Infers data schema
3. Detects partitions
4. Creates/updates tables in Data Catalog

### Crawler Configuration
\`\`\`yaml
Crawler: raw-events-crawler
  Data sources: s3://bucket/raw/events/
  IAM role: GlueCrawlerRole
  Database: raw_db
  Table prefix: raw_
  Schedule: Run on demand / Every hour
  Schema change policy: Update in place
  Partition behavior: Add new partitions only
\`\`\`

### Classifiers
Glue uses classifiers to detect formats:
- **Built-in**: JSON, CSV, Parquet, ORC, Avro, XML
- **Custom**: Grok patterns, JSON paths, XML tags

### ‚ö†Ô∏è Considerations
- Crawlers can be slow for millions of files
- Watch out for incorrect type detection
- Use exclusions to ignore unwanted files
- Consider creating tables manually for more control`,
      pt: `## Glue Crawlers - Descobrimento Autom√°tico

### O que faz um Crawler?
1. Escaneia path S3 (ou outra fonte)
2. Infere o schema dos dados
3. Detecta parti√ß√µes
4. Cria/atualiza tabelas no Data Catalog

### Configura√ß√£o de Crawler
\`\`\`yaml
Crawler: raw-events-crawler
  Data sources: s3://bucket/raw/events/
  IAM role: GlueCrawlerRole
  Database: raw_db
  Table prefix: raw_
  Schedule: Run on demand / Every hour
  Schema change policy: Update in place
  Partition behavior: Add new partitions only
\`\`\`

### Classifiers
Glue usa classifiers para detectar formatos:
- **Built-in**: JSON, CSV, Parquet, ORC, Avro, XML
- **Custom**: Grok patterns, JSON paths, XML tags

### ‚ö†Ô∏è Considera√ß√µes
- Crawlers podem ser lentos para milh√µes de arquivos
- Cuidado com detec√ß√£o de tipos incorreta
- Use exclusions para ignorar arquivos n√£o desejados
- Considere criar tabelas manualmente para maior controle`
    },
    practicalTips: [
      { es: 'üîÑ Usa MSCK REPAIR TABLE en Athena como alternativa m√°s r√°pida para a√±adir particiones', en: 'üîÑ Use MSCK REPAIR TABLE in Athena as a faster alternative to add partitions', pt: 'üîÑ Use MSCK REPAIR TABLE no Athena como alternativa mais r√°pida para adicionar parti√ß√µes' }
    ],
    externalLinks: [
      { title: 'Glue Crawlers', url: 'https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øCreaste un crawler que catalog√≥ datos de S3?', en: '‚úÖ Did you create a crawler that cataloged S3 data?', pt: '‚úÖ Voc√™ criou um crawler que catalogou dados do S3?' },
    xpReward: 60, estimatedMinutes: 35, services: ['Glue Crawlers']
  },

  {
    id: 'aws-4-4', stepNumber: 31,
    title: { es: 'Tu primer Glue ETL Job', en: 'Your first Glue ETL Job', pt: 'Seu primeiro Glue ETL Job' },
    description: { es: 'Crear un job b√°sico que lee, transforma y escribe datos.', en: 'Create a basic job that reads, transforms and writes data.', pt: 'Criar um job b√°sico que l√™, transforma e escreve dados.' },
    theory: {
      es: `## Tu Primer Glue Job

### Tipos de Jobs
- **Spark**: Para transformaciones complejas (PySpark)
- **Python Shell**: Para scripts simples (Pandas, boto3)
- **Streaming**: Para datos en tiempo real

### Estructura b√°sica de un Glue Job
\`\`\`python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

# Inicializaci√≥n
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Leer datos
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="raw_db",
    table_name="events"
)

# Transformar
transformed = datasource.toDF()
transformed = transformed.filter(transformed['value'] > 0)

# Escribir
glueContext.write_dynamic_frame.from_options(
    frame=DynamicFrame.fromDF(transformed, glueContext, "transformed"),
    connection_type="s3",
    connection_options={"path": "s3://bucket/processed/events/"},
    format="parquet"
)

job.commit()
\`\`\``,
      en: `## Your First Glue Job

### Job Types
- **Spark**: For complex transformations (PySpark)
- **Python Shell**: For simple scripts (Pandas, boto3)
- **Streaming**: For real-time data

### Basic Glue Job structure
\`\`\`python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

# Initialization
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Read data
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="raw_db",
    table_name="events"
)

# Transform
transformed = datasource.toDF()
transformed = transformed.filter(transformed['value'] > 0)

# Write
glueContext.write_dynamic_frame.from_options(
    frame=DynamicFrame.fromDF(transformed, glueContext, "transformed"),
    connection_type="s3",
    connection_options={"path": "s3://bucket/processed/events/"},
    format="parquet"
)

job.commit()
\`\`\``,
      pt: `## Seu Primeiro Glue Job

### Tipos de Jobs
- **Spark**: Para transforma√ß√µes complexas (PySpark)
- **Python Shell**: Para scripts simples (Pandas, boto3)
- **Streaming**: Para dados em tempo real

### Estrutura b√°sica de um Glue Job
\`\`\`python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

# Inicializa√ß√£o
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Ler dados
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="raw_db",
    table_name="events"
)

# Transformar
transformed = datasource.toDF()
transformed = transformed.filter(transformed['value'] > 0)

# Escrever
glueContext.write_dynamic_frame.from_options(
    frame=DynamicFrame.fromDF(transformed, glueContext, "transformed"),
    connection_type="s3",
    connection_options={"path": "s3://bucket/processed/events/"},
    format="parquet"
)

job.commit()
\`\`\``
    },
    practicalTips: [
      { es: '‚ö° Empieza con 2 DPUs para desarrollo, escala a m√°s solo cuando sea necesario', en: '‚ö° Start with 2 DPUs for development, scale to more only when needed', pt: '‚ö° Comece com 2 DPUs para desenvolvimento, escale para mais s√≥ quando necess√°rio' }
    ],
    externalLinks: [
      { title: 'Authoring Glue Jobs', url: 'https://docs.aws.amazon.com/glue/latest/dg/author-job.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øEjecutaste un job que ley√≥ de Data Catalog y escribi√≥ a S3?', en: '‚úÖ Did you run a job that read from Data Catalog and wrote to S3?', pt: '‚úÖ Voc√™ executou um job que leu do Data Catalog e escreveu no S3?' },
    xpReward: 75, estimatedMinutes: 50, services: ['Glue ETL']
  },

  {
    id: 'aws-4-5', stepNumber: 32,
    title: { es: 'DynamicFrames vs DataFrames', en: 'DynamicFrames vs DataFrames', pt: 'DynamicFrames vs DataFrames' },
    description: { es: 'Entender cu√°ndo usar DynamicFrames de Glue vs DataFrames de Spark.', en: 'Understand when to use Glue DynamicFrames vs Spark DataFrames.', pt: 'Entender quando usar DynamicFrames do Glue vs DataFrames do Spark.' },
    theory: {
      es: `## DynamicFrames vs DataFrames

### DynamicFrame (Glue)
- Schema flexible (cada record puede tener schema diferente)
- Mejor para datos semi-estructurados
- Transformaciones built-in de Glue
- Lazy evaluation

### DataFrame (Spark)
- Schema estricto y uniforme
- Mejor performance para datos estructurados
- Acceso a toda la API de Spark
- M√°s flexible para transformaciones complejas

### Conversi√≥n
\`\`\`python
# DynamicFrame ‚Üí DataFrame
df = dynamic_frame.toDF()

# DataFrame ‚Üí DynamicFrame
from awsglue.dynamicframe import DynamicFrame
dyf = DynamicFrame.fromDF(df, glueContext, "name")
\`\`\`

### Cu√°ndo usar cu√°l
| Caso | Usar |
|------|------|
| Datos JSON anidados | DynamicFrame |
| Datos Parquet limpios | DataFrame |
| Transformaciones complejas | DataFrame |
| Flatten/Relationalize | DynamicFrame |
| Machine Learning | DataFrame |`,
      en: `## DynamicFrames vs DataFrames

### DynamicFrame (Glue)
- Flexible schema (each record can have different schema)
- Better for semi-structured data
- Glue built-in transformations
- Lazy evaluation

### DataFrame (Spark)
- Strict and uniform schema
- Better performance for structured data
- Access to full Spark API
- More flexible for complex transformations

### Conversion
\`\`\`python
# DynamicFrame ‚Üí DataFrame
df = dynamic_frame.toDF()

# DataFrame ‚Üí DynamicFrame
from awsglue.dynamicframe import DynamicFrame
dyf = DynamicFrame.fromDF(df, glueContext, "name")
\`\`\`

### When to use which
| Case | Use |
|------|------|
| Nested JSON data | DynamicFrame |
| Clean Parquet data | DataFrame |
| Complex transformations | DataFrame |
| Flatten/Relationalize | DynamicFrame |
| Machine Learning | DataFrame |`,
      pt: `## DynamicFrames vs DataFrames

### DynamicFrame (Glue)
- Schema flex√≠vel (cada record pode ter schema diferente)
- Melhor para dados semi-estruturados
- Transforma√ß√µes built-in do Glue
- Lazy evaluation

### DataFrame (Spark)
- Schema estrito e uniforme
- Melhor performance para dados estruturados
- Acesso a toda a API do Spark
- Mais flex√≠vel para transforma√ß√µes complexas

### Convers√£o
\`\`\`python
# DynamicFrame ‚Üí DataFrame
df = dynamic_frame.toDF()

# DataFrame ‚Üí DynamicFrame
from awsglue.dynamicframe import DynamicFrame
dyf = DynamicFrame.fromDF(df, glueContext, "name")
\`\`\`

### Quando usar qual
| Caso | Usar |
|------|------|
| Dados JSON aninhados | DynamicFrame |
| Dados Parquet limpos | DataFrame |
| Transforma√ß√µes complexas | DataFrame |
| Flatten/Relationalize | DynamicFrame |
| Machine Learning | DataFrame |`
    },
    practicalTips: [
      { es: 'üîÑ En la pr√°ctica, convierte a DataFrame para transformaciones complejas y vuelve a DynamicFrame para escribir', en: 'üîÑ In practice, convert to DataFrame for complex transformations and back to DynamicFrame to write', pt: 'üîÑ Na pr√°tica, converta para DataFrame para transforma√ß√µes complexas e volte para DynamicFrame para escrever' }
    ],
    externalLinks: [
      { title: 'DynamicFrame Class', url: 'https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øProbaste convertir entre DynamicFrame y DataFrame en un job?', en: '‚úÖ Did you try converting between DynamicFrame and DataFrame in a job?', pt: '‚úÖ Voc√™ testou converter entre DynamicFrame e DataFrame em um job?' },
    xpReward: 55, estimatedMinutes: 30, services: ['Glue ETL']
  },

  {
    id: 'aws-4-6', stepNumber: 33,
    title: { es: 'Glue Studio para desarrollo visual', en: 'Glue Studio for visual development', pt: 'Glue Studio para desenvolvimento visual' },
    description: { es: 'Usar la interfaz visual de Glue Studio para crear ETL sin escribir c√≥digo.', en: 'Use Glue Studio visual interface to create ETL without writing code.', pt: 'Usar a interface visual do Glue Studio para criar ETL sem escrever c√≥digo.' },
    theory: {
      es: `## Glue Studio - ETL Visual

### Componentes de Glue Studio
1. **Visual Job Editor**: Drag & drop de transformaciones
2. **Notebooks**: Desarrollo interactivo
3. **Job Monitoring**: M√©tricas y logs
4. **Schema Preview**: Visualiza el schema en cada paso

### Nodos disponibles
**Source nodes**:
- S3 bucket
- Glue Data Catalog
- JDBC connections
- Kafka/Kinesis

**Transform nodes**:
- ApplyMapping
- Filter
- Join
- Aggregate
- Custom SQL
- Custom code (Python)

**Target nodes**:
- S3
- Glue Data Catalog
- JDBC
- Redshift

### Workflow t√≠pico
1. A√±ade Source (ej: tabla del Catalog)
2. A√±ade transformaciones (Filter, Join, etc.)
3. Preview data en cada paso
4. A√±ade Target (S3 + Catalog)
5. Run job y monitorea`,
      en: `## Glue Studio - Visual ETL

### Glue Studio Components
1. **Visual Job Editor**: Drag & drop transformations
2. **Notebooks**: Interactive development
3. **Job Monitoring**: Metrics and logs
4. **Schema Preview**: Visualize schema at each step

### Available nodes
**Source nodes**:
- S3 bucket
- Glue Data Catalog
- JDBC connections
- Kafka/Kinesis

**Transform nodes**:
- ApplyMapping
- Filter
- Join
- Aggregate
- Custom SQL
- Custom code (Python)

**Target nodes**:
- S3
- Glue Data Catalog
- JDBC
- Redshift

### Typical workflow
1. Add Source (e.g., Catalog table)
2. Add transformations (Filter, Join, etc.)
3. Preview data at each step
4. Add Target (S3 + Catalog)
5. Run job and monitor`,
      pt: `## Glue Studio - ETL Visual

### Componentes do Glue Studio
1. **Visual Job Editor**: Drag & drop de transforma√ß√µes
2. **Notebooks**: Desenvolvimento interativo
3. **Job Monitoring**: M√©tricas e logs
4. **Schema Preview**: Visualize o schema em cada passo

### N√≥s dispon√≠veis
**Source nodes**:
- S3 bucket
- Glue Data Catalog
- JDBC connections
- Kafka/Kinesis

**Transform nodes**:
- ApplyMapping
- Filter
- Join
- Aggregate
- Custom SQL
- Custom code (Python)

**Target nodes**:
- S3
- Glue Data Catalog
- JDBC
- Redshift

### Workflow t√≠pico
1. Adicione Source (ex: tabela do Catalog)
2. Adicione transforma√ß√µes (Filter, Join, etc.)
3. Preview data em cada passo
4. Adicione Target (S3 + Catalog)
5. Run job e monitore`
    },
    practicalTips: [
      { es: 'üé® Glue Studio es perfecto para prototipar pipelines r√°pidamente antes de optimizar c√≥digo', en: 'üé® Glue Studio is perfect for quickly prototyping pipelines before optimizing code', pt: 'üé® Glue Studio √© perfeito para prototipar pipelines rapidamente antes de otimizar c√≥digo' }
    ],
    externalLinks: [
      { title: 'AWS Glue Studio', url: 'https://docs.aws.amazon.com/glue/latest/ug/what-is-glue-studio.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øCreaste un job visual en Glue Studio con al menos 3 transformaciones?', en: '‚úÖ Did you create a visual job in Glue Studio with at least 3 transformations?', pt: '‚úÖ Voc√™ criou um job visual no Glue Studio com pelo menos 3 transforma√ß√µes?' },
    xpReward: 50, estimatedMinutes: 35, services: ['Glue Studio']
  },

  {
    id: 'aws-4-7', stepNumber: 34,
    title: { es: 'Job Bookmarks e incremental processing', en: 'Job Bookmarks and incremental processing', pt: 'Job Bookmarks e processamento incremental' },
    description: { es: 'Implementar procesamiento incremental para no reprocesar datos antiguos.', en: 'Implement incremental processing to avoid reprocessing old data.', pt: 'Implementar processamento incremental para n√£o reprocessar dados antigos.' },
    theory: {
      es: `## Job Bookmarks - Procesamiento Incremental

### ¬øQu√© son Job Bookmarks?
Mecanismo de Glue para recordar qu√© datos ya proces√≥ y solo leer los nuevos.

### C√≥mo funciona
1. Job lee datos de S3
2. Glue guarda un "bookmark" (timestamp/path del √∫ltimo procesamiento)
3. En la siguiente ejecuci√≥n, solo lee datos nuevos

### Activar bookmarks
\`\`\`python
# En la configuraci√≥n del job
job.init(args['JOB_NAME'], args)
# Bookmark se activa en Job properties: --job-bookmark-option = job-bookmark-enable
\`\`\`

### Opciones de bookmark
- **job-bookmark-enable**: Procesa solo datos nuevos
- **job-bookmark-disable**: Procesa todos los datos
- **job-bookmark-pause**: Mantiene bookmark pero procesa todo

### ‚ö†Ô∏è Limitaciones
- Solo funciona con sources de S3 o Catalog
- Requiere estructura de archivos ordenada
- No funciona bien con archivos que se modifican

### Alternativa: Partition filtering
\`\`\`python
# Filtrar por partici√≥n de fecha
push_down_predicate = "year='2024' AND month='01'"
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="db",
    table_name="events",
    push_down_predicate=push_down_predicate
)
\`\`\``,
      en: `## Job Bookmarks - Incremental Processing

### What are Job Bookmarks?
Glue mechanism to remember what data was already processed and only read new data.

### How it works
1. Job reads data from S3
2. Glue saves a "bookmark" (timestamp/path of last processing)
3. On next run, only reads new data

### Enable bookmarks
\`\`\`python
# In job configuration
job.init(args['JOB_NAME'], args)
# Bookmark is enabled in Job properties: --job-bookmark-option = job-bookmark-enable
\`\`\`

### Bookmark options
- **job-bookmark-enable**: Process only new data
- **job-bookmark-disable**: Process all data
- **job-bookmark-pause**: Keep bookmark but process all

### ‚ö†Ô∏è Limitations
- Only works with S3 or Catalog sources
- Requires ordered file structure
- Doesn't work well with modified files

### Alternative: Partition filtering
\`\`\`python
# Filter by date partition
push_down_predicate = "year='2024' AND month='01'"
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="db",
    table_name="events",
    push_down_predicate=push_down_predicate
)
\`\`\``,
      pt: `## Job Bookmarks - Processamento Incremental

### O que s√£o Job Bookmarks?
Mecanismo do Glue para lembrar quais dados j√° foram processados e s√≥ ler os novos.

### Como funciona
1. Job l√™ dados do S3
2. Glue salva um "bookmark" (timestamp/path do √∫ltimo processamento)
3. Na pr√≥xima execu√ß√£o, s√≥ l√™ dados novos

### Ativar bookmarks
\`\`\`python
# Na configura√ß√£o do job
job.init(args['JOB_NAME'], args)
# Bookmark √© ativado nas Job properties: --job-bookmark-option = job-bookmark-enable
\`\`\`

### Op√ß√µes de bookmark
- **job-bookmark-enable**: Processa s√≥ dados novos
- **job-bookmark-disable**: Processa todos os dados
- **job-bookmark-pause**: Mant√©m bookmark mas processa tudo

### ‚ö†Ô∏è Limita√ß√µes
- S√≥ funciona com sources de S3 ou Catalog
- Requer estrutura de arquivos ordenada
- N√£o funciona bem com arquivos que s√£o modificados

### Alternativa: Partition filtering
\`\`\`python
# Filtrar por parti√ß√£o de data
push_down_predicate = "year='2024' AND month='01'"
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="db",
    table_name="events",
    push_down_predicate=push_down_predicate
)
\`\`\``
    },
    practicalTips: [
      { es: '‚è∞ Job bookmarks funcionan mejor con datos inmutables y paths ordenados por tiempo', en: '‚è∞ Job bookmarks work best with immutable data and time-ordered paths', pt: '‚è∞ Job bookmarks funcionam melhor com dados imut√°veis e paths ordenados por tempo' }
    ],
    externalLinks: [
      { title: 'Job Bookmarks', url: 'https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øEjecutaste el mismo job 2 veces y verificaste que solo proces√≥ datos nuevos?', en: '‚úÖ Did you run the same job 2 times and verify it only processed new data?', pt: '‚úÖ Voc√™ executou o mesmo job 2 vezes e verificou que s√≥ processou dados novos?' },
    xpReward: 65, estimatedMinutes: 40, services: ['Glue ETL']
  },

  {
    id: 'aws-4-8', stepNumber: 35,
    title: { es: 'Optimizaci√≥n de Glue Jobs', en: 'Glue Jobs optimization', pt: 'Otimiza√ß√£o de Glue Jobs' },
    description: { es: 'T√©cnicas para mejorar performance y reducir costos de jobs.', en: 'Techniques to improve performance and reduce job costs.', pt: 'T√©cnicas para melhorar performance e reduzir custos de jobs.' },
    theory: {
      es: `## Optimizaci√≥n de Glue Jobs

### 1. Elegir el tipo de worker correcto
| Worker | vCPU | RAM | Uso |
|--------|------|-----|-----|
| Standard | 4 | 16GB | General |
| G.1X | 4 | 16GB | Memory-intensive |
| G.2X | 8 | 32GB | ML/Large datasets |

### 2. Auto-scaling
\`\`\`
# Configuraci√≥n recomendada
Number of workers: 2 (m√≠nimo)
Maximum workers: 10 (auto-scale up to)
\`\`\`

### 3. Pushdown predicates
\`\`\`python
# BIEN: filtra en la fuente
datasource = glueContext.create_dynamic_frame.from_catalog(
    push_down_predicate="year='2024'"
)

# MAL: filtra despu√©s de cargar todo
datasource = datasource.filter(lambda x: x['year'] == '2024')
\`\`\`

### 4. Particionamiento de output
\`\`\`python
glueContext.write_dynamic_frame.from_options(
    frame=df,
    connection_options={
        "path": "s3://bucket/output/",
        "partitionKeys": ["year", "month"]
    }
)
\`\`\`

### 5. Compaction de archivos peque√±os
Si tienes muchos archivos peque√±os, consol√≠dalos con coalesce/repartition.`,
      en: `## Glue Jobs Optimization

### 1. Choose correct worker type
| Worker | vCPU | RAM | Use |
|--------|------|-----|-----|
| Standard | 4 | 16GB | General |
| G.1X | 4 | 16GB | Memory-intensive |
| G.2X | 8 | 32GB | ML/Large datasets |

### 2. Auto-scaling
\`\`\`
# Recommended configuration
Number of workers: 2 (minimum)
Maximum workers: 10 (auto-scale up to)
\`\`\`

### 3. Pushdown predicates
\`\`\`python
# GOOD: filter at source
datasource = glueContext.create_dynamic_frame.from_catalog(
    push_down_predicate="year='2024'"
)

# BAD: filter after loading everything
datasource = datasource.filter(lambda x: x['year'] == '2024')
\`\`\`

### 4. Output partitioning
\`\`\`python
glueContext.write_dynamic_frame.from_options(
    frame=df,
    connection_options={
        "path": "s3://bucket/output/",
        "partitionKeys": ["year", "month"]
    }
)
\`\`\`

### 5. Small files compaction
If you have many small files, consolidate them with coalesce/repartition.`,
      pt: `## Otimiza√ß√£o de Glue Jobs

### 1. Escolher o tipo de worker correto
| Worker | vCPU | RAM | Uso |
|--------|------|-----|-----|
| Standard | 4 | 16GB | Geral |
| G.1X | 4 | 16GB | Memory-intensive |
| G.2X | 8 | 32GB | ML/Large datasets |

### 2. Auto-scaling
\`\`\`
# Configura√ß√£o recomendada
Number of workers: 2 (m√≠nimo)
Maximum workers: 10 (auto-scale up to)
\`\`\`

### 3. Pushdown predicates
\`\`\`python
# BOM: filtra na fonte
datasource = glueContext.create_dynamic_frame.from_catalog(
    push_down_predicate="year='2024'"
)

# RUIM: filtra depois de carregar tudo
datasource = datasource.filter(lambda x: x['year'] == '2024')
\`\`\`

### 4. Particionamento de output
\`\`\`python
glueContext.write_dynamic_frame.from_options(
    frame=df,
    connection_options={
        "path": "s3://bucket/output/",
        "partitionKeys": ["year", "month"]
    }
)
\`\`\`

### 5. Compacta√ß√£o de arquivos pequenos
Se voc√™ tem muitos arquivos pequenos, consolide-os com coalesce/repartition.`
    },
    practicalTips: [
      { es: 'üí∞ Monitorea los jobs con CloudWatch - la m√©trica "DPU Hours" te dice cu√°nto cost√≥', en: 'üí∞ Monitor jobs with CloudWatch - the "DPU Hours" metric tells you how much it cost', pt: 'üí∞ Monitore os jobs com CloudWatch - a m√©trica "DPU Hours" te diz quanto custou' }
    ],
    externalLinks: [
      { title: 'Glue Best Practices', url: 'https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øRedujiste el tiempo de un job usando pushdown predicates?', en: '‚úÖ Did you reduce a job\'s time using pushdown predicates?', pt: '‚úÖ Voc√™ reduziu o tempo de um job usando pushdown predicates?' },
    xpReward: 70, estimatedMinutes: 45, services: ['Glue ETL']
  },

  {
    id: 'aws-4-9', stepNumber: 36,
    title: { es: 'Glue Workflows para orquestaci√≥n', en: 'Glue Workflows for orchestration', pt: 'Glue Workflows para orquestra√ß√£o' },
    description: { es: 'Crear workflows que coordinan m√∫ltiples crawlers y jobs.', en: 'Create workflows that coordinate multiple crawlers and jobs.', pt: 'Criar workflows que coordenam m√∫ltiplos crawlers e jobs.' },
    theory: {
      es: `## Glue Workflows

### Componentes
- **Triggers**: Inician acciones (schedule, on-demand, conditional)
- **Jobs**: ETL jobs a ejecutar
- **Crawlers**: Crawlers a ejecutar
- **Conditions**: L√≥gica condicional (job succeeded/failed)

### Ejemplo de pipeline
\`\`\`
[Trigger: Daily 6AM]
    ‚Üì
[Crawler: raw-data]
    ‚Üì (on success)
[Job: raw-to-processed]
    ‚Üì (on success)
[Crawler: processed-data]
    ‚Üì (on success)
[Job: processed-to-serving]
\`\`\`

### Triggers disponibles
1. **Schedule**: Cron expression
2. **On-demand**: Manual o API
3. **Conditional**: Basado en estado de otro job/crawler

### ‚ö†Ô∏è Limitaciones de Glue Workflows
- Sin GUI para monitoreo avanzado
- Sin reintentos configurables
- Sin notificaciones built-in

Para pipelines complejos, considera Step Functions o Airflow (MWAA).`,
      en: `## Glue Workflows

### Components
- **Triggers**: Initiate actions (schedule, on-demand, conditional)
- **Jobs**: ETL jobs to execute
- **Crawlers**: Crawlers to execute
- **Conditions**: Conditional logic (job succeeded/failed)

### Pipeline example
\`\`\`
[Trigger: Daily 6AM]
    ‚Üì
[Crawler: raw-data]
    ‚Üì (on success)
[Job: raw-to-processed]
    ‚Üì (on success)
[Crawler: processed-data]
    ‚Üì (on success)
[Job: processed-to-serving]
\`\`\`

### Available triggers
1. **Schedule**: Cron expression
2. **On-demand**: Manual or API
3. **Conditional**: Based on another job/crawler state

### ‚ö†Ô∏è Glue Workflows limitations
- No GUI for advanced monitoring
- No configurable retries
- No built-in notifications

For complex pipelines, consider Step Functions or Airflow (MWAA).`,
      pt: `## Glue Workflows

### Componentes
- **Triggers**: Iniciam a√ß√µes (schedule, on-demand, conditional)
- **Jobs**: ETL jobs a executar
- **Crawlers**: Crawlers a executar
- **Conditions**: L√≥gica condicional (job succeeded/failed)

### Exemplo de pipeline
\`\`\`
[Trigger: Daily 6AM]
    ‚Üì
[Crawler: raw-data]
    ‚Üì (on success)
[Job: raw-to-processed]
    ‚Üì (on success)
[Crawler: processed-data]
    ‚Üì (on success)
[Job: processed-to-serving]
\`\`\`

### Triggers dispon√≠veis
1. **Schedule**: Cron expression
2. **On-demand**: Manual ou API
3. **Conditional**: Baseado no estado de outro job/crawler

### ‚ö†Ô∏è Limita√ß√µes de Glue Workflows
- Sem GUI para monitoramento avan√ßado
- Sem retries configur√°veis
- Sem notifica√ß√µes built-in

Para pipelines complexos, considere Step Functions ou Airflow (MWAA).`
    },
    practicalTips: [
      { es: 'üîÑ Usa Glue Workflows para pipelines simples, Step Functions para complejos', en: 'üîÑ Use Glue Workflows for simple pipelines, Step Functions for complex ones', pt: 'üîÑ Use Glue Workflows para pipelines simples, Step Functions para complexos' }
    ],
    externalLinks: [
      { title: 'Glue Workflows', url: 'https://docs.aws.amazon.com/glue/latest/dg/workflows_overview.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øCreaste un workflow con al menos un crawler y un job encadenados?', en: '‚úÖ Did you create a workflow with at least one crawler and one job chained?', pt: '‚úÖ Voc√™ criou um workflow com pelo menos um crawler e um job encadeados?' },
    xpReward: 60, estimatedMinutes: 40, services: ['Glue Workflows']
  },

  {
    id: 'aws-4-10', stepNumber: 37,
    title: { es: 'Conexiones JDBC y fuentes externas', en: 'JDBC connections and external sources', pt: 'Conex√µes JDBC e fontes externas' },
    description: { es: 'Conectar Glue a bases de datos RDS, Redshift y otras fuentes.', en: 'Connect Glue to RDS, Redshift databases and other sources.', pt: 'Conectar Glue a bancos de dados RDS, Redshift e outras fontes.' },
    theory: {
      es: `## Conexiones en Glue

### Tipos de conexi√≥n
- **JDBC**: RDS, Redshift, Aurora, bases externas
- **MongoDB**: Atlas o self-hosted
- **Kafka**: MSK o clusters externos
- **Network**: Custom endpoints en VPC

### Crear conexi√≥n JDBC
\`\`\`yaml
Connection: rds-postgres
  Type: JDBC
  JDBC URL: jdbc:postgresql://mydb.xyz.us-east-1.rds.amazonaws.com:5432/mydb
  Username: (desde Secrets Manager)
  Password: (desde Secrets Manager)
  VPC: vpc-xyz
  Security Groups: sg-glue
  Subnet: subnet-private
\`\`\`

### Usar conexi√≥n en Job
\`\`\`python
# Leer desde RDS
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="connections_db",
    table_name="my_rds_table",
    additional_options={"jobBookmarkKeys": ["id"]}
)

# O directamente con JDBC
jdbc_df = glueContext.create_dynamic_frame.from_options(
    connection_type="postgresql",
    connection_options={
        "url": "jdbc:postgresql://host:5432/db",
        "user": "user",
        "password": "pass",
        "dbtable": "schema.table"
    }
)
\`\`\`

### VPC Configuration
Glue necesita acceso de red a la DB:
1. Glue en VPC privada
2. Security group permite tr√°fico desde Glue
3. Subnet con route a la DB (o NAT Gateway)`,
      en: `## Glue Connections

### Connection types
- **JDBC**: RDS, Redshift, Aurora, external databases
- **MongoDB**: Atlas or self-hosted
- **Kafka**: MSK or external clusters
- **Network**: Custom endpoints in VPC

### Create JDBC connection
\`\`\`yaml
Connection: rds-postgres
  Type: JDBC
  JDBC URL: jdbc:postgresql://mydb.xyz.us-east-1.rds.amazonaws.com:5432/mydb
  Username: (from Secrets Manager)
  Password: (from Secrets Manager)
  VPC: vpc-xyz
  Security Groups: sg-glue
  Subnet: subnet-private
\`\`\`

### Use connection in Job
\`\`\`python
# Read from RDS
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="connections_db",
    table_name="my_rds_table",
    additional_options={"jobBookmarkKeys": ["id"]}
)

# Or directly with JDBC
jdbc_df = glueContext.create_dynamic_frame.from_options(
    connection_type="postgresql",
    connection_options={
        "url": "jdbc:postgresql://host:5432/db",
        "user": "user",
        "password": "pass",
        "dbtable": "schema.table"
    }
)
\`\`\`

### VPC Configuration
Glue needs network access to the DB:
1. Glue in private VPC
2. Security group allows traffic from Glue
3. Subnet with route to DB (or NAT Gateway)`,
      pt: `## Conex√µes no Glue

### Tipos de conex√£o
- **JDBC**: RDS, Redshift, Aurora, bancos externos
- **MongoDB**: Atlas ou self-hosted
- **Kafka**: MSK ou clusters externos
- **Network**: Custom endpoints em VPC

### Criar conex√£o JDBC
\`\`\`yaml
Connection: rds-postgres
  Type: JDBC
  JDBC URL: jdbc:postgresql://mydb.xyz.us-east-1.rds.amazonaws.com:5432/mydb
  Username: (do Secrets Manager)
  Password: (do Secrets Manager)
  VPC: vpc-xyz
  Security Groups: sg-glue
  Subnet: subnet-private
\`\`\`

### Usar conex√£o no Job
\`\`\`python
# Ler do RDS
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="connections_db",
    table_name="my_rds_table",
    additional_options={"jobBookmarkKeys": ["id"]}
)

# Ou diretamente com JDBC
jdbc_df = glueContext.create_dynamic_frame.from_options(
    connection_type="postgresql",
    connection_options={
        "url": "jdbc:postgresql://host:5432/db",
        "user": "user",
        "password": "pass",
        "dbtable": "schema.table"
    }
)
\`\`\`

### Configura√ß√£o de VPC
Glue precisa de acesso de rede ao DB:
1. Glue em VPC privada
2. Security group permite tr√°fego do Glue
3. Subnet com rota ao DB (ou NAT Gateway)`
    },
    practicalTips: [
      { es: 'üîó Siempre guarda credenciales JDBC en Secrets Manager, nunca en el c√≥digo', en: 'üîó Always store JDBC credentials in Secrets Manager, never in code', pt: 'üîó Sempre guarde credenciais JDBC no Secrets Manager, nunca no c√≥digo' }
    ],
    externalLinks: [
      { title: 'Glue Connections', url: 'https://docs.aws.amazon.com/glue/latest/dg/populate-add-connection.html', type: 'aws_docs' }
    ],
    checkpoint: { es: '‚úÖ ¬øCreaste una conexi√≥n y le√≠ste datos de una base de datos externa?', en: '‚úÖ Did you create a connection and read data from an external database?', pt: '‚úÖ Voc√™ criou uma conex√£o e leu dados de um banco de dados externo?' },
    xpReward: 65, estimatedMinutes: 45, services: ['Glue Connections', 'VPC']
  }
];








