/* eslint-disable no-template-curly-in-string, no-useless-escape */
/**
 * EJERCICIOS DE AWS DATA ENGINEERING
 * 35+ ejercicios prÃ¡cticos organizados por fase
 */
import { AWSExercise } from './types';

export const awsExercises: AWSExercise[] = [
  // === FASE 1: FUNDAMENTOS ===
  { id: 'aws-ex-1', phaseId: 'phase1', title: { es: 'Crear cuenta AWS con MFA', en: 'Create AWS account with MFA', pt: 'Criar conta AWS com MFA' }, description: { es: 'Configura una cuenta AWS segura con MFA y budget alerts.', en: 'Set up a secure AWS account with MFA and budget alerts.', pt: 'Configure uma conta AWS segura com MFA e alertas de budget.' }, difficulty: 'easy', type: 'cli', starterCode: '# 1. Ir a aws.amazon.com\n# 2. Click "Create an AWS Account"\n# 3. Seguir los pasos...', solution: '# Checklist:\n# âœ“ MFA habilitado en root\n# âœ“ Usuario IAM admin creado\n# âœ“ MFA en usuario admin\n# âœ“ Budget de $10 con alerta a 80%\n# âœ“ CloudTrail habilitado', testCode: '# Verifica en IAM Dashboard:\n# - MFA status: Enabled\n# - Users: 1+ admin user', hints: [{ es: 'Usa una app como Google Authenticator para MFA', en: 'Use an app like Google Authenticator for MFA', pt: 'Use um app como Google Authenticator para MFA' }], xpReward: 50, tags: ['iam', 'security', 'account'] },
  { id: 'aws-ex-2', phaseId: 'phase1', title: { es: 'Instalar y configurar AWS CLI', en: 'Install and configure AWS CLI', pt: 'Instalar e configurar AWS CLI' }, description: { es: 'Instala AWS CLI v2 y configura credenciales.', en: 'Install AWS CLI v2 and configure credentials.', pt: 'Instale AWS CLI v2 e configure credenciais.' }, difficulty: 'easy', type: 'cli', starterCode: '# Instalar AWS CLI\ncurl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"\nunzip awscliv2.zip\nsudo ./aws/install\n\n# Configurar\naws configure', solution: '# Verificar instalaciÃ³n\naws --version\n# aws-cli/2.x.x Python/3.x.x ...\n\n# Verificar configuraciÃ³n\naws sts get-caller-identity\n# {\n#   "UserId": "AIDAXXXXXXXXX",\n#   "Account": "123456789012",\n#   "Arn": "arn:aws:iam::123456789012:user/admin"\n# }', testCode: 'aws sts get-caller-identity', hints: [{ es: 'Usa aws configure --profile para mÃºltiples cuentas', en: 'Use aws configure --profile for multiple accounts', pt: 'Use aws configure --profile para mÃºltiplas contas' }], xpReward: 40, tags: ['cli', 'setup'] },

  // === FASE 2: S3 ===
  { id: 'aws-ex-3', phaseId: 'phase2', title: { es: 'Crear Data Lake bÃ¡sico en S3', en: 'Create basic Data Lake in S3', pt: 'Criar Data Lake bÃ¡sico no S3' }, description: { es: 'Crea una estructura de Data Lake con zonas raw, processed y serving.', en: 'Create a Data Lake structure with raw, processed, and serving zones.', pt: 'Crie uma estrutura de Data Lake com zonas raw, processed e serving.' }, difficulty: 'medium', type: 'cli', starterCode: 'ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nBUCKET_NAME="datalake-${ACCOUNT_ID}"\n\n# TODO: Crear bucket con versioning y encryption', solution: 'ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nBUCKET_NAME="datalake-${ACCOUNT_ID}"\n\n# Crear bucket\naws s3api create-bucket --bucket $BUCKET_NAME --region us-east-1\n\n# Habilitar versioning\naws s3api put-bucket-versioning --bucket $BUCKET_NAME \\\n  --versioning-configuration Status=Enabled\n\n# Habilitar encryption\naws s3api put-bucket-encryption --bucket $BUCKET_NAME \\\n  --server-side-encryption-configuration \'{\n    "Rules": [{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}]\n  }\'\n\n# Crear estructura de zonas\nfor zone in raw processed serving; do\n  aws s3api put-object --bucket $BUCKET_NAME --key "${zone}/"\ndone', testCode: 'aws s3 ls s3://$BUCKET_NAME/', hints: [{ es: 'Usa nombres Ãºnicos globalmente para buckets', en: 'Use globally unique names for buckets', pt: 'Use nomes globalmente Ãºnicos para buckets' }], xpReward: 70, tags: ['s3', 'datalake'] },
  { id: 'aws-ex-4', phaseId: 'phase2', title: { es: 'Subir datos particionados', en: 'Upload partitioned data', pt: 'Fazer upload de dados particionados' }, description: { es: 'Sube datos CSV con estructura de particiones por fecha.', en: 'Upload CSV data with date partition structure.', pt: 'FaÃ§a upload de dados CSV com estrutura de partiÃ§Ãµes por data.' }, difficulty: 'medium', type: 'python', starterCode: 'import boto3\nfrom datetime import datetime, timedelta\nimport csv\nimport io\n\ns3 = boto3.client("s3")\nBUCKET = "your-bucket-name"\n\n# TODO: Generar datos de ejemplo y subirlos con particiones year/month/day', solution: 'import boto3\nfrom datetime import datetime, timedelta\nimport csv\nimport io\n\ns3 = boto3.client("s3")\nBUCKET = "your-bucket-name"\n\ndef generate_sample_data(date):\n    """Genera datos de ventas de ejemplo"""\n    return [\n        {"id": i, "product": f"Product_{i}", "amount": i * 10, "date": date.isoformat()}\n        for i in range(1, 101)\n    ]\n\ndef upload_partitioned(data, date):\n    """Sube datos con particiones de fecha"""\n    buffer = io.StringIO()\n    writer = csv.DictWriter(buffer, fieldnames=data[0].keys())\n    writer.writeheader()\n    writer.writerows(data)\n    \n    key = f"raw/sales/year={date.year}/month={date.month:02d}/day={date.day:02d}/data.csv"\n    s3.put_object(Bucket=BUCKET, Key=key, Body=buffer.getvalue())\n    print(f"Uploaded: {key}")\n\n# Generar 7 dÃ­as de datos\nfor i in range(7):\n    date = datetime.now() - timedelta(days=i)\n    data = generate_sample_data(date)\n    upload_partitioned(data, date)', testCode: 'aws s3 ls s3://$BUCKET/raw/sales/ --recursive', hints: [{ es: 'Las particiones Hive-style (key=value) funcionan mejor con Athena', en: 'Hive-style partitions (key=value) work best with Athena', pt: 'PartiÃ§Ãµes Hive-style (key=value) funcionam melhor com Athena' }], xpReward: 80, tags: ['s3', 'partitioning', 'python'] },
  { id: 'aws-ex-5', phaseId: 'phase2', title: { es: 'Configurar Lifecycle Rules', en: 'Configure Lifecycle Rules', pt: 'Configurar Lifecycle Rules' }, description: { es: 'Configura reglas de lifecycle para optimizar costos.', en: 'Configure lifecycle rules to optimize costs.', pt: 'Configure regras de lifecycle para otimizar custos.' }, difficulty: 'easy', type: 'cli', starterCode: '# TODO: Crear lifecycle rule que:\n# 1. Mueva raw/ a IA despuÃ©s de 30 dÃ­as\n# 2. Mueva a Glacier despuÃ©s de 90 dÃ­as\n# 3. Elimine despuÃ©s de 365 dÃ­as', solution: 'aws s3api put-bucket-lifecycle-configuration --bucket $BUCKET_NAME \\\n  --lifecycle-configuration \'{\n    "Rules": [\n      {\n        "ID": "RawDataLifecycle",\n        "Status": "Enabled",\n        "Filter": {"Prefix": "raw/"},\n        "Transitions": [\n          {"Days": 30, "StorageClass": "STANDARD_IA"},\n          {"Days": 90, "StorageClass": "GLACIER"}\n        ],\n        "Expiration": {"Days": 365}\n      },\n      {\n        "ID": "ProcessedDataLifecycle",  \n        "Status": "Enabled",\n        "Filter": {"Prefix": "processed/"},\n        "Transitions": [\n          {"Days": 60, "StorageClass": "STANDARD_IA"}\n        ]\n      }\n    ]\n  }\'', testCode: 'aws s3api get-bucket-lifecycle-configuration --bucket $BUCKET_NAME', hints: [{ es: 'Los objetos pequeÃ±os no se benefician de IA (mÃ­nimo 128KB)', en: 'Small objects don\'t benefit from IA (minimum 128KB)', pt: 'Objetos pequenos nÃ£o se beneficiam de IA (mÃ­nimo 128KB)' }], xpReward: 50, tags: ['s3', 'lifecycle', 'cost-optimization'] },

  // === FASE 3: IAM ===
  { id: 'aws-ex-6', phaseId: 'phase3', title: { es: 'Crear IAM Role para Glue', en: 'Create IAM Role for Glue', pt: 'Criar IAM Role para Glue' }, description: { es: 'Crea un rol IAM con permisos para ejecutar jobs de Glue.', en: 'Create an IAM role with permissions to run Glue jobs.', pt: 'Crie um role IAM com permissÃµes para executar jobs de Glue.' }, difficulty: 'medium', type: 'cli', starterCode: '# TODO: Crear rol con:\n# 1. Trust policy para glue.amazonaws.com\n# 2. Permisos para S3 y CloudWatch Logs', solution: '# Crear trust policy\ncat > trust-policy.json << EOF\n{\n  "Version": "2012-10-17",\n  "Statement": [\n    {\n      "Effect": "Allow",\n      "Principal": {"Service": "glue.amazonaws.com"},\n      "Action": "sts:AssumeRole"\n    }\n  ]\n}\nEOF\n\n# Crear rol\naws iam create-role \\\n  --role-name GlueDataEngRole \\\n  --assume-role-policy-document file://trust-policy.json\n\n# Adjuntar polÃ­ticas necesarias\naws iam attach-role-policy --role-name GlueDataEngRole \\\n  --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\n\n# Crear polÃ­tica custom para S3\ncat > s3-policy.json << EOF\n{\n  "Version": "2012-10-17",\n  "Statement": [\n    {\n      "Effect": "Allow",\n      "Action": ["s3:GetObject", "s3:PutObject", "s3:DeleteObject"],\n      "Resource": "arn:aws:s3:::datalake-*/*"\n    },\n    {\n      "Effect": "Allow",\n      "Action": "s3:ListBucket",\n      "Resource": "arn:aws:s3:::datalake-*"\n    }\n  ]\n}\nEOF\n\naws iam put-role-policy --role-name GlueDataEngRole \\\n  --policy-name S3Access \\\n  --policy-document file://s3-policy.json', testCode: 'aws iam get-role --role-name GlueDataEngRole', hints: [{ es: 'Usa el principio de mÃ­nimo privilegio - solo los permisos necesarios', en: 'Use the principle of least privilege - only necessary permissions', pt: 'Use o princÃ­pio de mÃ­nimo privilÃ©gio - apenas permissÃµes necessÃ¡rias' }], xpReward: 70, tags: ['iam', 'glue', 'security'] },
  { id: 'aws-ex-7', phaseId: 'phase3', title: { es: 'Configurar KMS para encriptaciÃ³n', en: 'Configure KMS for encryption', pt: 'Configurar KMS para criptografia' }, description: { es: 'Crea una KMS key y Ãºsala para encriptar datos en S3.', en: 'Create a KMS key and use it to encrypt data in S3.', pt: 'Crie uma KMS key e use-a para criptografar dados no S3.' }, difficulty: 'medium', type: 'cli', starterCode: '# TODO: Crear KMS key con alias y usarla en S3', solution: '# Crear KMS key\nKEY_ID=$(aws kms create-key \\\n  --description "Data Lake encryption key" \\\n  --query KeyMetadata.KeyId --output text)\n\n# Crear alias\naws kms create-alias \\\n  --alias-name alias/datalake-key \\\n  --target-key-id $KEY_ID\n\n# Configurar bucket para usar KMS\naws s3api put-bucket-encryption --bucket $BUCKET_NAME \\\n  --server-side-encryption-configuration \'{\n    "Rules": [{\n      "ApplyServerSideEncryptionByDefault": {\n        "SSEAlgorithm": "aws:kms",\n        "KMSMasterKeyID": "alias/datalake-key"\n      },\n      "BucketKeyEnabled": true\n    }]\n  }\'\n\necho "KMS Key ID: $KEY_ID"', testCode: 'aws kms describe-key --key-id alias/datalake-key', hints: [{ es: 'Usa Bucket Key para reducir costos de KMS en S3', en: 'Use Bucket Key to reduce KMS costs in S3', pt: 'Use Bucket Key para reduzir custos de KMS no S3' }], xpReward: 60, tags: ['kms', 's3', 'encryption'] },

  // === FASE 4: GLUE ===
  { id: 'aws-ex-8', phaseId: 'phase4', title: { es: 'Crear Glue Crawler', en: 'Create Glue Crawler', pt: 'Criar Glue Crawler' }, description: { es: 'Crea un crawler que detecte el schema de datos CSV en S3.', en: 'Create a crawler that detects the schema of CSV data in S3.', pt: 'Crie um crawler que detecte o schema de dados CSV no S3.' }, difficulty: 'medium', type: 'cli', starterCode: '# TODO: Crear crawler para s3://bucket/raw/sales/', solution: 'aws glue create-crawler \\\n  --name sales-crawler \\\n  --role GlueDataEngRole \\\n  --database-name datalake_db \\\n  --targets \'{\n    "S3Targets": [{\n      "Path": "s3://\'$BUCKET_NAME\'/raw/sales/",\n      "Exclusions": ["_*"]\n    }]\n  }\'\n\n# Crear database primero si no existe\naws glue create-database --database-input \'{"Name": "datalake_db"}\' 2>/dev/null\n\n# Ejecutar crawler\naws glue start-crawler --name sales-crawler\n\n# Esperar y verificar\naws glue get-tables --database-name datalake_db', testCode: 'aws glue get-tables --database-name datalake_db --query "TableList[].Name"', hints: [{ es: 'Usa Exclusions para ignorar archivos como _SUCCESS', en: 'Use Exclusions to ignore files like _SUCCESS', pt: 'Use Exclusions para ignorar arquivos como _SUCCESS' }], xpReward: 70, tags: ['glue', 'crawler', 'catalog'] },
  { id: 'aws-ex-9', phaseId: 'phase4', title: { es: 'Crear Glue Job PySpark', en: 'Create Glue PySpark Job', pt: 'Criar Glue Job PySpark' }, description: { es: 'Crea un job que transforme CSV a Parquet.', en: 'Create a job that transforms CSV to Parquet.', pt: 'Crie um job que transforme CSV para Parquet.' }, difficulty: 'hard', type: 'python', starterCode: '# Glue Job Script\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n# TODO: Leer de Catalog, transformar y escribir en Parquet', solution: '# Glue Job Script - csv_to_parquet.py\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\n\nargs = getResolvedOptions(sys.argv, ["JOB_NAME", "OUTPUT_PATH"])\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args["JOB_NAME"], args)\n\n# Leer desde Glue Catalog\ndatasource = glueContext.create_dynamic_frame.from_catalog(\n    database="datalake_db",\n    table_name="sales",\n    transformation_ctx="datasource"\n)\n\n# Transformaciones\n# 1. Eliminar duplicados\ndf = datasource.toDF().dropDuplicates()\n\n# 2. Convertir tipos\nfrom pyspark.sql.functions import col\ndf = df.withColumn("amount", col("amount").cast("double"))\n\n# 3. Agregar timestamp de procesamiento\nfrom pyspark.sql.functions import current_timestamp\ndf = df.withColumn("processed_at", current_timestamp())\n\n# Convertir de vuelta a DynamicFrame\nprocessed = DynamicFrame.fromDF(df, glueContext, "processed")\n\n# Escribir en Parquet particionado\nglueContext.write_dynamic_frame.from_options(\n    frame=processed,\n    connection_type="s3",\n    format="parquet",\n    connection_options={\n        "path": args["OUTPUT_PATH"],\n        "partitionKeys": ["year", "month"]\n    },\n    transformation_ctx="output"\n)\n\njob.commit()', testCode: 'aws s3 ls s3://$BUCKET_NAME/processed/sales/ --recursive | head', hints: [{ es: 'Usa DynamicFrame para transformaciones que necesitan schema inference', en: 'Use DynamicFrame for transformations that need schema inference', pt: 'Use DynamicFrame para transformaÃ§Ãµes que precisam de schema inference' }], xpReward: 100, tags: ['glue', 'pyspark', 'etl'] },
  { id: 'aws-ex-10', phaseId: 'phase4', title: { es: 'Crear Glue Workflow', en: 'Create Glue Workflow', pt: 'Criar Glue Workflow' }, description: { es: 'Orquesta crawler â†’ job â†’ crawler como workflow.', en: 'Orchestrate crawler â†’ job â†’ crawler as a workflow.', pt: 'Orquestre crawler â†’ job â†’ crawler como workflow.' }, difficulty: 'hard', type: 'cli', starterCode: '# TODO: Crear workflow que:\n# 1. Ejecute crawler de raw\n# 2. Ejecute job de transformaciÃ³n\n# 3. Ejecute crawler de processed', solution: '# Crear workflow\naws glue create-workflow --name sales-etl-workflow\n\n# Trigger inicial (on-demand)\naws glue create-trigger \\\n  --name start-sales-etl \\\n  --workflow-name sales-etl-workflow \\\n  --type ON_DEMAND \\\n  --actions \'[{"CrawlerName": "sales-raw-crawler"}]\'\n\n# Trigger para job despuÃ©s del crawler\naws glue create-trigger \\\n  --name after-raw-crawler \\\n  --workflow-name sales-etl-workflow \\\n  --type CONDITIONAL \\\n  --predicate \'{\n    "Conditions": [{\n      "CrawlerName": "sales-raw-crawler",\n      "CrawlState": "SUCCEEDED",\n      "LogicalOperator": "EQUALS"\n    }]\n  }\' \\\n  --actions \'[{"JobName": "csv-to-parquet"}]\'\n\n# Trigger para crawler final\naws glue create-trigger \\\n  --name after-transform-job \\\n  --workflow-name sales-etl-workflow \\\n  --type CONDITIONAL \\\n  --predicate \'{\n    "Conditions": [{\n      "JobName": "csv-to-parquet",\n      "State": "SUCCEEDED",\n      "LogicalOperator": "EQUALS"\n    }]\n  }\' \\\n  --actions \'[{"CrawlerName": "sales-processed-crawler"}]\'\n\n# Activar triggers\naws glue start-trigger --name start-sales-etl', testCode: 'aws glue get-workflow --name sales-etl-workflow', hints: [{ es: 'Los workflows son mejores que Glue Triggers individuales para pipelines complejos', en: 'Workflows are better than individual Glue Triggers for complex pipelines', pt: 'Workflows sÃ£o melhores que Glue Triggers individuais para pipelines complexos' }], xpReward: 90, tags: ['glue', 'workflow', 'orchestration'] },

  // === FASE 5: ATHENA ===
  { id: 'aws-ex-11', phaseId: 'phase5', title: { es: 'Queries optimizadas en Athena', en: 'Optimized queries in Athena', pt: 'Queries otimizadas no Athena' }, description: { es: 'Escribe queries Athena usando particiones y proyecciÃ³n.', en: 'Write Athena queries using partitions and projection.', pt: 'Escreva queries Athena usando partiÃ§Ãµes e projeÃ§Ã£o.' }, difficulty: 'medium', type: 'sql', starterCode: '-- TODO: Query que use partition pruning y solo columnas necesarias', solution: '-- Configurar workgroup con output location\n-- aws athena create-work-group --name data-eng-wg --configuration "ResultConfiguration={OutputLocation=s3://bucket/athena-results/}"\n\n-- Query optimizada con partition pruning\nSELECT \n    product,\n    SUM(amount) as total_sales,\n    COUNT(*) as num_transactions\nFROM datalake_db.sales\nWHERE year = 2024 \n  AND month = 12\nGROUP BY product\nORDER BY total_sales DESC\nLIMIT 10;\n\n-- Esta query solo escanea la particiÃ³n de diciembre 2024\n-- vs escanear todo el histÃ³rico', testCode: '-- Verificar bytes escaneados en Query History', hints: [{ es: 'Siempre filtra por columnas de particiÃ³n al inicio del WHERE', en: 'Always filter by partition columns at the start of WHERE', pt: 'Sempre filtre por colunas de partiÃ§Ã£o no inÃ­cio do WHERE' }], xpReward: 70, tags: ['athena', 'sql', 'optimization'] },
  { id: 'aws-ex-12', phaseId: 'phase5', title: { es: 'Crear Vista materializada en Athena', en: 'Create materialized view in Athena', pt: 'Criar View materializada no Athena' }, description: { es: 'Crea una vista y una CTAS para pre-agregar datos.', en: 'Create a view and CTAS to pre-aggregate data.', pt: 'Crie uma view e CTAS para prÃ©-agregar dados.' }, difficulty: 'medium', type: 'sql', starterCode: '-- TODO: Crear CTAS que agregue ventas por producto/mes', solution: '-- Vista simple (se ejecuta cada vez)\nCREATE OR REPLACE VIEW datalake_db.v_sales_summary AS\nSELECT \n    year,\n    month,\n    product,\n    SUM(amount) as total_sales,\n    COUNT(*) as num_transactions,\n    AVG(amount) as avg_sale\nFROM datalake_db.sales\nGROUP BY year, month, product;\n\n-- CTAS para materializar (se ejecuta una vez, crea tabla)\nCREATE TABLE datalake_db.sales_monthly_summary\nWITH (\n    format = \'PARQUET\',\n    external_location = \'s3://bucket/serving/sales_summary/\',\n    partitioned_by = ARRAY[\'year\']\n) AS\nSELECT \n    month,\n    product,\n    SUM(amount) as total_sales,\n    COUNT(*) as num_transactions,\n    AVG(amount) as avg_sale,\n    year\nFROM datalake_db.sales\nGROUP BY year, month, product;', testCode: 'SELECT * FROM datalake_db.sales_monthly_summary WHERE year = 2024 LIMIT 5;', hints: [{ es: 'Las columnas de particiÃ³n deben ir al final en CTAS', en: 'Partition columns must go at the end in CTAS', pt: 'Colunas de partiÃ§Ã£o devem ir no final em CTAS' }], xpReward: 80, tags: ['athena', 'sql', 'ctas'] },

  // === FASE 6: REDSHIFT ===
  { id: 'aws-ex-13', phaseId: 'phase6', title: { es: 'Crear Redshift Serverless', en: 'Create Redshift Serverless', pt: 'Criar Redshift Serverless' }, description: { es: 'Configura un namespace y workgroup de Redshift Serverless.', en: 'Configure a Redshift Serverless namespace and workgroup.', pt: 'Configure um namespace e workgroup do Redshift Serverless.' }, difficulty: 'medium', type: 'cli', starterCode: '# TODO: Crear namespace y workgroup', solution: '# Crear namespace (storage)\naws redshift-serverless create-namespace \\\n  --namespace-name datalake-ns \\\n  --admin-username admin \\\n  --admin-user-password "SecureP@ss123!" \\\n  --db-name datalake\n\n# Crear workgroup (compute)\naws redshift-serverless create-workgroup \\\n  --workgroup-name datalake-wg \\\n  --namespace-name datalake-ns \\\n  --base-capacity 32 \\\n  --publicly-accessible\n\n# Obtener endpoint\naws redshift-serverless get-workgroup \\\n  --workgroup-name datalake-wg \\\n  --query "workgroup.endpoint.address" --output text', testCode: 'aws redshift-serverless list-workgroups', hints: [{ es: 'Base capacity 32 es suficiente para desarrollo (escala automÃ¡ticamente)', en: 'Base capacity 32 is enough for development (scales automatically)', pt: 'Base capacity 32 Ã© suficiente para desenvolvimento (escala automaticamente)' }], xpReward: 80, tags: ['redshift', 'serverless'] },
  { id: 'aws-ex-14', phaseId: 'phase6', title: { es: 'COPY desde S3 a Redshift', en: 'COPY from S3 to Redshift', pt: 'COPY do S3 para Redshift' }, description: { es: 'Carga datos desde S3 usando el comando COPY.', en: 'Load data from S3 using the COPY command.', pt: 'Carregue dados do S3 usando o comando COPY.' }, difficulty: 'medium', type: 'sql', starterCode: '-- TODO: Crear tabla y cargar con COPY', solution: '-- Crear tabla\nCREATE TABLE sales (\n    id INTEGER,\n    product VARCHAR(100),\n    amount DECIMAL(10,2),\n    sale_date DATE,\n    year INTEGER,\n    month INTEGER\n)\nDISTKEY(product)\nSORTKEY(sale_date);\n\n-- COPY desde S3 (Parquet)\nCOPY sales\nFROM \'s3://bucket/processed/sales/\'\nIAM_ROLE \'arn:aws:iam::123456789012:role/RedshiftS3Role\'\nFORMAT AS PARQUET;\n\n-- Verificar carga\nSELECT COUNT(*) FROM sales;\n\n-- Ver distribuciÃ³n\nSELECT "table", diststyle, sortkey1 \nFROM svv_table_info \nWHERE "table" = \'sales\';', testCode: 'SELECT COUNT(*) FROM sales;', hints: [{ es: 'PARQUET es mÃ¡s eficiente que CSV para COPY', en: 'PARQUET is more efficient than CSV for COPY', pt: 'PARQUET Ã© mais eficiente que CSV para COPY' }], xpReward: 75, tags: ['redshift', 'copy', 's3'] },
  { id: 'aws-ex-15', phaseId: 'phase6', title: { es: 'Redshift Spectrum', en: 'Redshift Spectrum', pt: 'Redshift Spectrum' }, description: { es: 'Consulta datos en S3 directamente desde Redshift.', en: 'Query data in S3 directly from Redshift.', pt: 'Consulte dados no S3 diretamente do Redshift.' }, difficulty: 'hard', type: 'sql', starterCode: '-- TODO: Crear external schema y consultar S3', solution: '-- Crear external schema apuntando al Glue Catalog\nCREATE EXTERNAL SCHEMA datalake_external\nFROM DATA CATALOG\nDATABASE \'datalake_db\'\nIAM_ROLE \'arn:aws:iam::123456789012:role/RedshiftSpectrumRole\'\nREGION \'us-east-1\';\n\n-- Query que une tabla local con externa\nSELECT \n    l.product,\n    SUM(l.amount) as local_sales,\n    SUM(e.amount) as external_sales\nFROM sales l\nJOIN datalake_external.sales_historical e \n    ON l.product = e.product\nWHERE e.year < 2024\nGROUP BY l.product;\n\n-- Verificar tablas externas disponibles\nSELECT * FROM svv_external_tables;', testCode: 'SELECT * FROM svv_external_schemas;', hints: [{ es: 'Spectrum usa el Glue Data Catalog - no dupliques schemas', en: 'Spectrum uses Glue Data Catalog - don\'t duplicate schemas', pt: 'Spectrum usa o Glue Data Catalog - nÃ£o duplique schemas' }], xpReward: 90, tags: ['redshift', 'spectrum', 's3'] },

  // === FASE 7: EMR ===
  { id: 'aws-ex-16', phaseId: 'phase7', title: { es: 'Crear cluster EMR Serverless', en: 'Create EMR Serverless cluster', pt: 'Criar cluster EMR Serverless' }, description: { es: 'Configura una aplicaciÃ³n EMR Serverless para Spark.', en: 'Configure an EMR Serverless application for Spark.', pt: 'Configure uma aplicaÃ§Ã£o EMR Serverless para Spark.' }, difficulty: 'medium', type: 'cli', starterCode: '# TODO: Crear aplicaciÃ³n EMR Serverless', solution: '# Crear aplicaciÃ³n\nAPP_ID=$(aws emr-serverless create-application \\\n  --name spark-etl-app \\\n  --type SPARK \\\n  --release-label emr-6.10.0 \\\n  --query applicationId --output text)\n\necho "Application ID: $APP_ID"\n\n# Verificar estado\naws emr-serverless get-application --application-id $APP_ID\n\n# Iniciar aplicaciÃ³n (warm start)\naws emr-serverless start-application --application-id $APP_ID', testCode: 'aws emr-serverless list-applications', hints: [{ es: 'EMR Serverless elimina la gestiÃ³n de clusters - ideal para ETL', en: 'EMR Serverless eliminates cluster management - ideal for ETL', pt: 'EMR Serverless elimina a gestÃ£o de clusters - ideal para ETL' }], xpReward: 75, tags: ['emr', 'serverless', 'spark'] },
  { id: 'aws-ex-17', phaseId: 'phase7', title: { es: 'Job Spark en EMR Serverless', en: 'Spark job on EMR Serverless', pt: 'Job Spark no EMR Serverless' }, description: { es: 'Ejecuta un job PySpark en EMR Serverless.', en: 'Run a PySpark job on EMR Serverless.', pt: 'Execute um job PySpark no EMR Serverless.' }, difficulty: 'hard', type: 'python', starterCode: '# spark_job.py - subir a S3\nfrom pyspark.sql import SparkSession\n\n# TODO: Leer Parquet, transformar, escribir', solution: '# spark_job.py\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, sum, avg, count, current_timestamp\n\nspark = SparkSession.builder.appName("SalesAggregation").getOrCreate()\n\n# Leer datos\ndf = spark.read.parquet("s3://bucket/processed/sales/")\n\n# Transformaciones\nresult = df.groupBy("product", "year", "month").agg(\n    sum("amount").alias("total_sales"),\n    avg("amount").alias("avg_sale"),\n    count("*").alias("num_transactions")\n).withColumn("processed_at", current_timestamp())\n\n# Escribir resultado\nresult.write.mode("overwrite").partitionBy("year", "month").parquet(\n    "s3://bucket/serving/sales_aggregated/"\n)\n\nspark.stop()\n\n# ---\n# Ejecutar job\n# aws emr-serverless start-job-run \\\n#   --application-id $APP_ID \\\n#   --execution-role-arn arn:aws:iam::123456789012:role/EMRServerlessRole \\\n#   --job-driver \'{\n#     "sparkSubmit": {\n#       "entryPoint": "s3://bucket/scripts/spark_job.py"\n#     }\n#   }\'', testCode: 'aws emr-serverless list-job-runs --application-id $APP_ID', hints: [{ es: 'El job se ejecuta en contenedores serverless - no hay cluster que mantener', en: 'The job runs in serverless containers - no cluster to maintain', pt: 'O job executa em containers serverless - sem cluster para manter' }], xpReward: 100, tags: ['emr', 'spark', 'pyspark'] },

  // === FASE 8: KINESIS ===
  { id: 'aws-ex-18', phaseId: 'phase8', title: { es: 'Crear Kinesis Data Stream', en: 'Create Kinesis Data Stream', pt: 'Criar Kinesis Data Stream' }, description: { es: 'Configura un stream para ingesta en tiempo real.', en: 'Configure a stream for real-time ingestion.', pt: 'Configure um stream para ingestÃ£o em tempo real.' }, difficulty: 'medium', type: 'cli', starterCode: '# TODO: Crear stream on-demand', solution: '# Crear stream (on-demand - escala automÃ¡tica)\naws kinesis create-stream \\\n  --stream-name events-stream \\\n  --stream-mode-details StreamMode=ON_DEMAND\n\n# Esperar a que estÃ© activo\naws kinesis wait stream-exists --stream-name events-stream\n\n# Verificar\naws kinesis describe-stream-summary --stream-name events-stream', testCode: 'aws kinesis list-streams', hints: [{ es: 'On-demand es mÃ¡s simple que provisioned para cargas variables', en: 'On-demand is simpler than provisioned for variable loads', pt: 'On-demand Ã© mais simples que provisioned para cargas variÃ¡veis' }], xpReward: 60, tags: ['kinesis', 'streaming'] },
  { id: 'aws-ex-19', phaseId: 'phase8', title: { es: 'Productor Python para Kinesis', en: 'Python producer for Kinesis', pt: 'Produtor Python para Kinesis' }, description: { es: 'Escribe un productor que envÃ­e eventos a Kinesis.', en: 'Write a producer that sends events to Kinesis.', pt: 'Escreva um produtor que envie eventos para Kinesis.' }, difficulty: 'medium', type: 'python', starterCode: 'import boto3\nimport json\nimport random\nimport time\n\nkinesis = boto3.client("kinesis")\n\n# TODO: Enviar eventos de ejemplo', solution: 'import boto3\nimport json\nimport random\nimport time\nfrom datetime import datetime\n\nkinesis = boto3.client("kinesis")\nSTREAM_NAME = "events-stream"\n\ndef generate_event():\n    """Genera un evento de ejemplo"""\n    return {\n        "event_id": f"evt-{random.randint(1000, 9999)}",\n        "user_id": f"user-{random.randint(1, 100)}",\n        "action": random.choice(["view", "click", "purchase"]),\n        "amount": round(random.uniform(10, 500), 2),\n        "timestamp": datetime.utcnow().isoformat()\n    }\n\ndef send_event(event):\n    """EnvÃ­a evento a Kinesis"""\n    response = kinesis.put_record(\n        StreamName=STREAM_NAME,\n        Data=json.dumps(event),\n        PartitionKey=event["user_id"]  # Garantiza orden por usuario\n    )\n    return response["SequenceNumber"]\n\n# Enviar 100 eventos\nfor i in range(100):\n    event = generate_event()\n    seq = send_event(event)\n    print(f"Sent event {i+1}: {seq}")\n    time.sleep(0.1)  # 10 events/second\n\nprint("Done!")', testCode: '# Verificar mÃ©tricas en CloudWatch: IncomingRecords', hints: [{ es: 'Usa el user_id como partition key para mantener orden de eventos por usuario', en: 'Use user_id as partition key to maintain event order per user', pt: 'Use user_id como partition key para manter ordem de eventos por usuÃ¡rio' }], xpReward: 75, tags: ['kinesis', 'python', 'streaming'] },
  { id: 'aws-ex-20', phaseId: 'phase8', title: { es: 'Kinesis Firehose a S3', en: 'Kinesis Firehose to S3', pt: 'Kinesis Firehose para S3' }, description: { es: 'Configura Firehose para entregar datos a S3 en Parquet.', en: 'Configure Firehose to deliver data to S3 in Parquet.', pt: 'Configure Firehose para entregar dados no S3 em Parquet.' }, difficulty: 'hard', type: 'cli', starterCode: '# TODO: Crear Firehose con conversiÃ³n a Parquet', solution: '# Crear tabla en Glue Catalog para el schema\naws glue create-table --database-name datalake_db --table-input \'{\n  "Name": "events_schema",\n  "StorageDescriptor": {\n    "Columns": [\n      {"Name": "event_id", "Type": "string"},\n      {"Name": "user_id", "Type": "string"},\n      {"Name": "action", "Type": "string"},\n      {"Name": "amount", "Type": "double"},\n      {"Name": "timestamp", "Type": "string"}\n    ],\n    "Location": "s3://bucket/raw/events/",\n    "InputFormat": "org.apache.hadoop.mapred.TextInputFormat",\n    "OutputFormat": "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat",\n    "SerdeInfo": {"SerializationLibrary": "org.openx.data.jsonserde.JsonSerDe"}\n  }\n}\'\n\n# Crear Firehose delivery stream\naws firehose create-delivery-stream \\\n  --delivery-stream-name events-to-s3 \\\n  --delivery-stream-type DirectPut \\\n  --extended-s3-destination-configuration \'{\n    "RoleARN": "arn:aws:iam::123456789012:role/FirehoseS3Role",\n    "BucketARN": "arn:aws:s3:::bucket",\n    "Prefix": "raw/events/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/",\n    "ErrorOutputPrefix": "errors/events/",\n    "BufferingHints": {"SizeInMBs": 128, "IntervalInSeconds": 300},\n    "CompressionFormat": "UNCOMPRESSED",\n    "DataFormatConversionConfiguration": {\n      "Enabled": true,\n      "SchemaConfiguration": {\n        "RoleARN": "arn:aws:iam::123456789012:role/FirehoseS3Role",\n        "DatabaseName": "datalake_db",\n        "TableName": "events_schema",\n        "Region": "us-east-1"\n      },\n      "InputFormatConfiguration": {"Deserializer": {"OpenXJsonSerDe": {}}},\n      "OutputFormatConfiguration": {"Serializer": {"ParquetSerDe": {"Compression": "SNAPPY"}}}\n    }\n  }\'', testCode: 'aws firehose describe-delivery-stream --delivery-stream-name events-to-s3', hints: [{ es: 'Firehose puede convertir JSON a Parquet automÃ¡ticamente con el Glue schema', en: 'Firehose can convert JSON to Parquet automatically with Glue schema', pt: 'Firehose pode converter JSON para Parquet automaticamente com o schema do Glue' }], xpReward: 100, tags: ['kinesis', 'firehose', 's3', 'parquet'] },

  // === FASE 9: ORQUESTACIÃ“N ===
  { id: 'aws-ex-21', phaseId: 'phase9', title: { es: 'Step Functions para ETL', en: 'Step Functions for ETL', pt: 'Step Functions para ETL' }, description: { es: 'Crea una state machine que orqueste un pipeline ETL.', en: 'Create a state machine that orchestrates an ETL pipeline.', pt: 'Crie uma state machine que orquestre um pipeline ETL.' }, difficulty: 'hard', type: 'json', starterCode: '// TODO: Definir state machine con:\n// 1. Start Crawler\n// 2. Wait for Crawler\n// 3. Run Glue Job\n// 4. Check Job Status', solution: '{\n  "Comment": "ETL Pipeline Orchestration",\n  "StartAt": "StartCrawler",\n  "States": {\n    "StartCrawler": {\n      "Type": "Task",\n      "Resource": "arn:aws:states:::aws-sdk:glue:startCrawler",\n      "Parameters": {"Name": "sales-crawler"},\n      "Next": "WaitForCrawler"\n    },\n    "WaitForCrawler": {\n      "Type": "Wait",\n      "Seconds": 30,\n      "Next": "CheckCrawlerStatus"\n    },\n    "CheckCrawlerStatus": {\n      "Type": "Task",\n      "Resource": "arn:aws:states:::aws-sdk:glue:getCrawler",\n      "Parameters": {"Name": "sales-crawler"},\n      "Next": "IsCrawlerReady"\n    },\n    "IsCrawlerReady": {\n      "Type": "Choice",\n      "Choices": [\n        {\n          "Variable": "$.Crawler.State",\n          "StringEquals": "READY",\n          "Next": "StartGlueJob"\n        }\n      ],\n      "Default": "WaitForCrawler"\n    },\n    "StartGlueJob": {\n      "Type": "Task",\n      "Resource": "arn:aws:states:::glue:startJobRun.sync",\n      "Parameters": {\n        "JobName": "csv-to-parquet",\n        "Arguments": {"--OUTPUT_PATH": "s3://bucket/processed/"}\n      },\n      "Next": "Success"\n    },\n    "Success": {\n      "Type": "Succeed"\n    }\n  }\n}', testCode: 'aws stepfunctions list-state-machines', hints: [{ es: 'Usa .sync para esperar que el Glue job termine', en: 'Use .sync to wait for the Glue job to finish', pt: 'Use .sync para esperar que o Glue job termine' }], xpReward: 100, tags: ['stepfunctions', 'orchestration', 'glue'] },
  { id: 'aws-ex-22', phaseId: 'phase9', title: { es: 'EventBridge Schedule', en: 'EventBridge Schedule', pt: 'EventBridge Schedule' }, description: { es: 'Programa la ejecuciÃ³n diaria del pipeline.', en: 'Schedule daily execution of the pipeline.', pt: 'Programe a execuÃ§Ã£o diÃ¡ria do pipeline.' }, difficulty: 'medium', type: 'cli', starterCode: '# TODO: Crear regla que ejecute Step Functions diariamente a las 6 AM UTC', solution: '# Crear regla de schedule\naws events put-rule \\\n  --name daily-etl-schedule \\\n  --schedule-expression "cron(0 6 * * ? *)" \\\n  --state ENABLED \\\n  --description "Daily ETL at 6 AM UTC"\n\n# Agregar target (Step Functions)\naws events put-targets \\\n  --rule daily-etl-schedule \\\n  --targets \'[{\n    "Id": "etl-pipeline",\n    "Arn": "arn:aws:states:us-east-1:123456789012:stateMachine:ETLPipeline",\n    "RoleArn": "arn:aws:iam::123456789012:role/EventBridgeStepFunctionsRole",\n    "Input": "{\\"source\\": \\"scheduled\\"}"\n  }]\'\n\n# Verificar\naws events list-rules --name-prefix daily-etl', testCode: 'aws events list-rules', hints: [{ es: 'Las expresiones cron de EventBridge usan 6 campos (incluye aÃ±o)', en: 'EventBridge cron expressions use 6 fields (includes year)', pt: 'ExpressÃµes cron do EventBridge usam 6 campos (inclui ano)' }], xpReward: 65, tags: ['eventbridge', 'scheduling'] },

  // === FASE 10: IaC ===
  { id: 'aws-ex-23', phaseId: 'phase10', title: { es: 'Terraform para S3 + Glue', en: 'Terraform for S3 + Glue', pt: 'Terraform para S3 + Glue' }, description: { es: 'Define infraestructura de Data Lake con Terraform.', en: 'Define Data Lake infrastructure with Terraform.', pt: 'Defina infraestrutura de Data Lake com Terraform.' }, difficulty: 'hard', type: 'hcl', starterCode: '# main.tf\n# TODO: S3 bucket, Glue database, Glue crawler', solution: '# main.tf\nterraform {\n  required_providers {\n    aws = {\n      source  = "hashicorp/aws"\n      version = "~> 5.0"\n    }\n  }\n}\n\nprovider "aws" {\n  region = "us-east-1"\n}\n\ndata "aws_caller_identity" "current" {}\n\n# S3 Bucket\nresource "aws_s3_bucket" "datalake" {\n  bucket = "datalake-${data.aws_caller_identity.current.account_id}"\n}\n\nresource "aws_s3_bucket_versioning" "datalake" {\n  bucket = aws_s3_bucket.datalake.id\n  versioning_configuration {\n    status = "Enabled"\n  }\n}\n\nresource "aws_s3_bucket_server_side_encryption_configuration" "datalake" {\n  bucket = aws_s3_bucket.datalake.id\n  rule {\n    apply_server_side_encryption_by_default {\n      sse_algorithm = "AES256"\n    }\n  }\n}\n\n# Glue Database\nresource "aws_glue_catalog_database" "datalake" {\n  name = "datalake_db"\n}\n\n# Glue Crawler\nresource "aws_glue_crawler" "sales" {\n  name          = "sales-crawler"\n  role          = aws_iam_role.glue.arn\n  database_name = aws_glue_catalog_database.datalake.name\n\n  s3_target {\n    path = "s3://${aws_s3_bucket.datalake.bucket}/raw/sales/"\n  }\n}\n\n# IAM Role for Glue\nresource "aws_iam_role" "glue" {\n  name = "GlueServiceRole"\n\n  assume_role_policy = jsonencode({\n    Version = "2012-10-17"\n    Statement = [{\n      Action = "sts:AssumeRole"\n      Effect = "Allow"\n      Principal = {\n        Service = "glue.amazonaws.com"\n      }\n    }]\n  })\n}\n\nresource "aws_iam_role_policy_attachment" "glue" {\n  role       = aws_iam_role.glue.name\n  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole"\n}\n\noutput "bucket_name" {\n  value = aws_s3_bucket.datalake.bucket\n}', testCode: 'terraform plan', hints: [{ es: 'Usa data sources para evitar hardcodear account IDs', en: 'Use data sources to avoid hardcoding account IDs', pt: 'Use data sources para evitar hardcodear account IDs' }], xpReward: 100, tags: ['terraform', 'iac', 's3', 'glue'] },
  { id: 'aws-ex-24', phaseId: 'phase10', title: { es: 'GitHub Actions CI/CD', en: 'GitHub Actions CI/CD', pt: 'GitHub Actions CI/CD' }, description: { es: 'Configura CI/CD para desplegar Terraform automÃ¡ticamente.', en: 'Configure CI/CD to deploy Terraform automatically.', pt: 'Configure CI/CD para fazer deploy de Terraform automaticamente.' }, difficulty: 'hard', type: 'yaml', starterCode: '# .github/workflows/terraform.yml\n# TODO: Plan on PR, Apply on merge to main', solution: '# .github/workflows/terraform.yml\nname: Terraform CI/CD\n\non:\n  pull_request:\n    branches: [main]\n  push:\n    branches: [main]\n\nenv:\n  TF_VERSION: "1.6.0"\n  AWS_REGION: "us-east-1"\n\njobs:\n  plan:\n    name: Terraform Plan\n    runs-on: ubuntu-latest\n    if: github.event_name == \'pull_request\'\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: ${{ env.TF_VERSION }}\n      \n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: arn:aws:iam::123456789012:role/GitHubActionsRole\n          aws-region: ${{ env.AWS_REGION }}\n      \n      - name: Terraform Init\n        run: terraform init\n        \n      - name: Terraform Plan\n        run: terraform plan -out=tfplan\n        \n      - name: Post Plan to PR\n        uses: actions/github-script@v7\n        with:\n          script: |\n            const output = require(\'fs\').readFileSync(\'tfplan\', \'utf8\');\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: \'```terraform\\n\' + output + \'\\n```\'\n            });\n\n  apply:\n    name: Terraform Apply\n    runs-on: ubuntu-latest\n    if: github.event_name == \'push\' && github.ref == \'refs/heads/main\'\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v3\n        with:\n          terraform_version: ${{ env.TF_VERSION }}\n      \n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: arn:aws:iam::123456789012:role/GitHubActionsRole\n          aws-region: ${{ env.AWS_REGION }}\n      \n      - name: Terraform Init\n        run: terraform init\n        \n      - name: Terraform Apply\n        run: terraform apply -auto-approve', testCode: 'gh workflow list', hints: [{ es: 'Usa OIDC para autenticaciÃ³n sin secrets de AWS', en: 'Use OIDC for authentication without AWS secrets', pt: 'Use OIDC para autenticaÃ§Ã£o sem secrets da AWS' }], xpReward: 90, tags: ['github-actions', 'cicd', 'terraform'] },

  // === FASE 11: MONITORING ===
  { id: 'aws-ex-25', phaseId: 'phase11', title: { es: 'Dashboard CloudWatch', en: 'CloudWatch Dashboard', pt: 'Dashboard CloudWatch' }, description: { es: 'Crea un dashboard para monitorear pipelines.', en: 'Create a dashboard to monitor pipelines.', pt: 'Crie um dashboard para monitorar pipelines.' }, difficulty: 'medium', type: 'json', starterCode: '// TODO: Dashboard con mÃ©tricas de Glue, Athena y S3', solution: '{\n  "widgets": [\n    {\n      "type": "text",\n      "x": 0, "y": 0, "width": 24, "height": 1,\n      "properties": {\n        "markdown": "# ğŸ“Š Data Platform Dashboard"\n      }\n    },\n    {\n      "type": "metric",\n      "x": 0, "y": 1, "width": 8, "height": 6,\n      "properties": {\n        "title": "Glue Job Duration",\n        "metrics": [\n          ["Glue", "glue.driver.aggregate.elapsedTime", "JobName", "csv-to-parquet", "JobRunId", "ALL", "Type", "gauge"]\n        ],\n        "period": 300,\n        "stat": "Average",\n        "region": "us-east-1"\n      }\n    },\n    {\n      "type": "metric",\n      "x": 8, "y": 1, "width": 8, "height": 6,\n      "properties": {\n        "title": "Athena Query Bytes Scanned",\n        "metrics": [\n          ["AWS/Athena", "ProcessedBytes", "WorkGroup", "data-eng-wg"]\n        ],\n        "period": 3600,\n        "stat": "Sum"\n      }\n    },\n    {\n      "type": "metric",\n      "x": 16, "y": 1, "width": 8, "height": 6,\n      "properties": {\n        "title": "S3 Bucket Size",\n        "metrics": [\n          ["AWS/S3", "BucketSizeBytes", "BucketName", "datalake-bucket", "StorageType", "StandardStorage"]\n        ],\n        "period": 86400,\n        "stat": "Average"\n      }\n    },\n    {\n      "type": "alarm",\n      "x": 0, "y": 7, "width": 24, "height": 4,\n      "properties": {\n        "title": "Active Alarms",\n        "alarms": [\n          "arn:aws:cloudwatch:us-east-1:123456789012:alarm:GlueJobFailed",\n          "arn:aws:cloudwatch:us-east-1:123456789012:alarm:AthenaHighCost"\n        ]\n      }\n    }\n  ]\n}', testCode: 'aws cloudwatch list-dashboards', hints: [{ es: 'Agrupa mÃ©tricas relacionadas para una vista coherente', en: 'Group related metrics for a coherent view', pt: 'Agrupe mÃ©tricas relacionadas para uma visÃ£o coerente' }], xpReward: 75, tags: ['cloudwatch', 'monitoring', 'dashboard'] },
  { id: 'aws-ex-26', phaseId: 'phase11', title: { es: 'Alarmas de costo', en: 'Cost alarms', pt: 'Alarmes de custo' }, description: { es: 'Configura alarmas para controlar costos de data services.', en: 'Configure alarms to control data service costs.', pt: 'Configure alarmes para controlar custos de serviÃ§os de dados.' }, difficulty: 'easy', type: 'cli', starterCode: '# TODO: Alarma cuando Athena escanee mÃ¡s de 1TB/dÃ­a', solution: '# Crear SNS topic para alertas\nTOPIC_ARN=$(aws sns create-topic --name data-cost-alerts --query TopicArn --output text)\n\n# Suscribir email\naws sns subscribe --topic-arn $TOPIC_ARN --protocol email --notification-endpoint tu@email.com\n\n# Alarma de Athena\naws cloudwatch put-metric-alarm \\\n  --alarm-name AthenaHighDataScanned \\\n  --alarm-description "Athena scanned more than 1TB in 24h" \\\n  --metric-name ProcessedBytes \\\n  --namespace AWS/Athena \\\n  --dimensions Name=WorkGroup,Value=data-eng-wg \\\n  --statistic Sum \\\n  --period 86400 \\\n  --threshold 1099511627776 \\\n  --comparison-operator GreaterThanThreshold \\\n  --evaluation-periods 1 \\\n  --alarm-actions $TOPIC_ARN\n\n# Alarma de Glue\naws cloudwatch put-metric-alarm \\\n  --alarm-name GlueJobFailed \\\n  --alarm-description "Glue job failed" \\\n  --metric-name "glue.driver.aggregate.numFailedTask" \\\n  --namespace Glue \\\n  --dimensions Name=JobName,Value=csv-to-parquet Name=JobRunId,Value=ALL Name=Type,Value=gauge \\\n  --statistic Maximum \\\n  --period 300 \\\n  --threshold 0 \\\n  --comparison-operator GreaterThanThreshold \\\n  --evaluation-periods 1 \\\n  --alarm-actions $TOPIC_ARN', testCode: 'aws cloudwatch describe-alarms --alarm-name-prefix "Athena"', hints: [{ es: '1TB = 1099511627776 bytes', en: '1TB = 1099511627776 bytes', pt: '1TB = 1099511627776 bytes' }], xpReward: 55, tags: ['cloudwatch', 'alarms', 'cost'] },

  // === FASE 12: CERTIFICACIÃ“N ===
  { id: 'aws-ex-27', phaseId: 'phase12', title: { es: 'DiseÃ±o de Data Lake completo', en: 'Complete Data Lake design', pt: 'Design de Data Lake completo' }, description: { es: 'DiseÃ±a arquitectura para un caso de negocio real.', en: 'Design architecture for a real business case.', pt: 'Projete arquitetura para um caso de negÃ³cio real.' }, difficulty: 'hard', type: 'diagram', starterCode: '# Caso: E-commerce necesita:\n# - Ingerir clickstream (10K eventos/segundo)\n# - Procesar ventas diarias (500GB/dÃ­a)\n# - Dashboard de mÃ©tricas en tiempo real\n# - Reportes histÃ³ricos ad-hoc\n# - ML para recomendaciones\n#\n# TODO: DiseÃ±ar arquitectura', solution: '# Arquitectura propuesta:\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Clickstream â”‚â”€â”€â”€â”€â–¶â”‚   Kinesis    â”‚â”€â”€â”€â”€â–¶â”‚   Lambda/Flink  â”‚\nâ”‚   (10K/s)    â”‚     â”‚ Data Streams â”‚     â”‚  (real-time)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                   â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â–¼\nâ”‚   Sales DB   â”‚â”€â”€â”€â”€â–¶â”‚     DMS      â”‚â”€â”€â”€â”€â–¶â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  (500GB/dÃ­a) â”‚     â”‚              â”‚     â”‚   S3 Data Lake  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n                                          â”‚  â”‚   raw/    â”‚  â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚  â”‚processed/ â”‚  â”‚\nâ”‚   API Data  â”‚â”€â”€â”€â”€â–¶ Lambda â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  â”‚ serving/  â”‚  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                   â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚                              â”‚                              â”‚\n                    â–¼                              â–¼                              â–¼\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n          â”‚     Athena      â”‚          â”‚    Redshift     â”‚          â”‚   SageMaker     â”‚\n          â”‚   (ad-hoc SQL)  â”‚          â”‚  (BI dashboards)â”‚          â”‚     (ML)        â”‚\n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                   â”‚                            â”‚                            â”‚\n                   â–¼                            â–¼                            â–¼\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n          â”‚  Data Analysts  â”‚          â”‚   QuickSight    â”‚          â”‚ Recommendations â”‚\n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n# Servicios:\n- Kinesis Data Streams: Clickstream ingestion\n- Kinesis Data Analytics (Flink): Real-time processing\n- DMS: CDC from transactional DB\n- S3: Data Lake storage (Parquet/Iceberg)\n- Glue: ETL and Data Catalog\n- Athena: Ad-hoc queries\n- Redshift: BI workloads\n- SageMaker: ML training/inference\n- Step Functions: Orchestration\n- CloudWatch: Monitoring\n- Lake Formation: Governance', testCode: '# Validar que la arquitectura cumple todos los requisitos', hints: [{ es: 'Separa los flujos batch y streaming desde el inicio', en: 'Separate batch and streaming flows from the start', pt: 'Separe os fluxos batch e streaming desde o inÃ­cio' }], xpReward: 150, tags: ['architecture', 'design', 'certification'] },
  { id: 'aws-ex-28', phaseId: 'phase12', title: { es: 'Quiz de certificaciÃ³n', en: 'Certification quiz', pt: 'Quiz de certificaÃ§Ã£o' }, description: { es: 'Responde preguntas estilo examen DAS-C01.', en: 'Answer DAS-C01 exam-style questions.', pt: 'Responda perguntas estilo exame DAS-C01.' }, difficulty: 'hard', type: 'quiz', starterCode: '# Responde las siguientes preguntas:', solution: '# Q1: Una empresa tiene logs en S3 (1TB/dÃ­a) y necesita queries ad-hoc ocasionales.\n# Â¿QuÃ© soluciÃ³n es mÃ¡s cost-effective?\n# A) Redshift provisioned\n# B) Redshift Serverless\n# C) Athena âœ“ (pago por query, sin infra)\n# D) EMR con Spark\n\n# Q2: Â¿CuÃ¡l es la mejor prÃ¡ctica para particionar datos en S3 para Athena?\n# A) Usar nombres aleatorios\n# B) Usar formato Hive (year=2024/month=01) âœ“\n# C) No particionar, Athena es eficiente\n# D) Particionar por tamaÃ±o de archivo\n\n# Q3: Un Glue job estÃ¡ fallando por OOM. Â¿QuÃ© intentas primero?\n# A) Cambiar a EMR\n# B) Aumentar el nÃºmero de workers âœ“\n# C) Reducir los datos de entrada\n# D) Usar Glue 4.0\n\n# Q4: Â¿CuÃ¡l servicio usarÃ­as para CDC desde RDS a S3?\n# A) Kinesis Firehose\n# B) AWS DMS âœ“\n# C) AWS DataSync\n# D) S3 Replication\n\n# Q5: Â¿QuÃ© formato es mÃ¡s eficiente para queries analÃ­ticas en Athena?\n# A) CSV\n# B) JSON\n# C) Parquet con Snappy âœ“\n# D) Avro', testCode: '# Verifica tus respuestas con la soluciÃ³n', hints: [{ es: 'Piensa siempre en costo vs rendimiento vs complejidad', en: 'Always think cost vs performance vs complexity', pt: 'Sempre pense em custo vs desempenho vs complexidade' }], xpReward: 80, tags: ['certification', 'quiz'] },

  // === EJERCICIOS ADICIONALES ===
  { id: 'aws-ex-29', phaseId: 'phase4', title: { es: 'Glue Data Quality', en: 'Glue Data Quality', pt: 'Glue Data Quality' }, description: { es: 'Implementa validaciones de calidad en un job.', en: 'Implement quality validations in a job.', pt: 'Implemente validaÃ§Ãµes de qualidade em um job.' }, difficulty: 'medium', type: 'python', starterCode: '# TODO: Agregar reglas de calidad al job', solution: 'from awsglue.context import GlueContext\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.data_quality.quality import DQRuleset\n\n# Definir reglas de calidad\nruleset = DQRuleset(\n    name="SalesDataQuality",\n    rules=[\n        "RowCount between 1000 and 1000000",\n        "Completeness \\"product\\" > 0.99",\n        "Completeness \\"amount\\" > 0.95",\n        "ColumnValues \\"amount\\" > 0",\n        "Uniqueness \\"id\\" > 0.99",\n        "ColumnDataType \\"amount\\" = \\"Double\\""\n    ]\n)\n\n# Evaluar calidad\nresults = glueContext.evaluate_data_quality(\n    frame=datasource,\n    ruleset=ruleset,\n    publishing_options={\n        "dataQualityEvaluationContext": "SalesQuality",\n        "enableDataQualityCloudWatchMetrics": True,\n        "enableDataQualityResultsPublishing": True\n    }\n)\n\n# Fallar si hay problemas crÃ­ticos\nif results.overallScore < 0.9:\n    raise Exception(f"Data quality score {results.overallScore} below threshold 0.9")', testCode: '# Verificar mÃ©tricas de calidad en CloudWatch', hints: [{ es: 'Define umbrales de calidad segÃºn el criticidad del dato', en: 'Define quality thresholds based on data criticality', pt: 'Defina umbrais de qualidade baseado na criticidade do dado' }], xpReward: 80, tags: ['glue', 'data-quality'] },
  { id: 'aws-ex-30', phaseId: 'phase5', title: { es: 'Athena Federated Query', en: 'Athena Federated Query', pt: 'Athena Federated Query' }, description: { es: 'Consulta datos en DynamoDB desde Athena.', en: 'Query DynamoDB data from Athena.', pt: 'Consulte dados no DynamoDB a partir do Athena.' }, difficulty: 'hard', type: 'sql', starterCode: '-- TODO: Configurar conector y ejecutar query federada', solution: '-- 1. Desplegar conector desde Serverless Application Repository\n-- aws serverlessrepo create-cloud-formation-change-set ...\n\n-- 2. Crear catÃ¡logo para DynamoDB\n-- aws athena create-data-catalog \\\n--   --name dynamodb \\\n--   --type LAMBDA \\\n--   --parameters function=arn:aws:lambda:...:AthenaDynamoDBConnector\n\n-- 3. Query federada\nSELECT \n    s.product,\n    s.total_sales,\n    p.description,\n    p.category\nFROM datalake_db.sales_summary s\nJOIN dynamodb.default.products p \n    ON s.product = p.product_id\nWHERE s.year = 2024;\n\n-- Esto consulta S3 (sales_summary) y DynamoDB (products) en una sola query!', testCode: 'aws athena list-data-catalogs', hints: [{ es: 'Federated queries son mÃ¡s lentas pero evitan duplicar datos', en: 'Federated queries are slower but avoid duplicating data', pt: 'Federated queries sÃ£o mais lentas mas evitam duplicar dados' }], xpReward: 85, tags: ['athena', 'federated', 'dynamodb'] },
  { id: 'aws-ex-31', phaseId: 'phase8', title: { es: 'Lambda consumer para Kinesis', en: 'Lambda consumer for Kinesis', pt: 'Lambda consumer para Kinesis' }, description: { es: 'Procesa eventos de Kinesis con Lambda.', en: 'Process Kinesis events with Lambda.', pt: 'Processe eventos do Kinesis com Lambda.' }, difficulty: 'medium', type: 'python', starterCode: '# lambda_function.py\nimport json\nimport base64\n\ndef lambda_handler(event, context):\n    # TODO: Procesar records de Kinesis', solution: '# lambda_function.py\nimport json\nimport base64\nimport boto3\nfrom datetime import datetime\n\ndynamodb = boto3.resource("dynamodb")\ntable = dynamodb.Table("events-processed")\n\ndef lambda_handler(event, context):\n    processed = 0\n    failed = 0\n    \n    for record in event["Records"]:\n        try:\n            # Decodificar payload (viene en base64)\n            payload = base64.b64decode(record["kinesis"]["data"]).decode("utf-8")\n            data = json.loads(payload)\n            \n            # Enriquecer evento\n            data["processed_at"] = datetime.utcnow().isoformat()\n            data["partition_key"] = record["kinesis"]["partitionKey"]\n            data["sequence_number"] = record["kinesis"]["sequenceNumber"]\n            \n            # Guardar en DynamoDB\n            table.put_item(Item=data)\n            processed += 1\n            \n        except Exception as e:\n            print(f"Error processing record: {e}")\n            failed += 1\n    \n    return {\n        "statusCode": 200,\n        "body": json.dumps({\n            "processed": processed,\n            "failed": failed\n        })\n    }', testCode: 'aws lambda invoke --function-name kinesis-processor output.json', hints: [{ es: 'Lambda recibe batches de Kinesis - procesa todos los records', en: 'Lambda receives Kinesis batches - process all records', pt: 'Lambda recebe batches do Kinesis - processe todos os records' }], xpReward: 75, tags: ['lambda', 'kinesis', 'streaming'] },
  { id: 'aws-ex-32', phaseId: 'phase9', title: { es: 'Step Functions error handling', en: 'Step Functions error handling', pt: 'Step Functions error handling' }, description: { es: 'Implementa manejo de errores y reintentos.', en: 'Implement error handling and retries.', pt: 'Implemente tratamento de erros e retentativas.' }, difficulty: 'hard', type: 'json', starterCode: '// TODO: Agregar Catch, Retry y fallback', solution: '{\n  "StartAt": "ProcessData",\n  "States": {\n    "ProcessData": {\n      "Type": "Task",\n      "Resource": "arn:aws:states:::glue:startJobRun.sync",\n      "Parameters": {\n        "JobName": "critical-etl"\n      },\n      "Retry": [\n        {\n          "ErrorEquals": ["Glue.ConcurrentRunsExceededException"],\n          "IntervalSeconds": 60,\n          "MaxAttempts": 3,\n          "BackoffRate": 2.0\n        },\n        {\n          "ErrorEquals": ["States.TaskFailed"],\n          "IntervalSeconds": 30,\n          "MaxAttempts": 2,\n          "BackoffRate": 1.5\n        }\n      ],\n      "Catch": [\n        {\n          "ErrorEquals": ["States.ALL"],\n          "Next": "NotifyFailure",\n          "ResultPath": "$.error"\n        }\n      ],\n      "Next": "Success"\n    },\n    "NotifyFailure": {\n      "Type": "Task",\n      "Resource": "arn:aws:states:::sns:publish",\n      "Parameters": {\n        "TopicArn": "arn:aws:sns:us-east-1:123456789012:pipeline-alerts",\n        "Message.$": "States.Format(\'Pipeline failed: {}\', $.error.Cause)",\n        "Subject": "âŒ ETL Pipeline Failed"\n      },\n      "Next": "Fail"\n    },\n    "Fail": {\n      "Type": "Fail",\n      "Error": "PipelineFailed",\n      "Cause": "ETL job failed after retries"\n    },\n    "Success": {\n      "Type": "Succeed"\n    }\n  }\n}', testCode: 'aws stepfunctions start-execution --state-machine-arn ...', hints: [{ es: 'Usa BackoffRate > 1 para aumentar el tiempo entre reintentos', en: 'Use BackoffRate > 1 to increase time between retries', pt: 'Use BackoffRate > 1 para aumentar o tempo entre retentativas' }], xpReward: 85, tags: ['stepfunctions', 'error-handling'] },
  { id: 'aws-ex-33', phaseId: 'phase11', title: { es: 'Logs Insights queries', en: 'Logs Insights queries', pt: 'Queries Logs Insights' }, description: { es: 'Escribe queries para analizar logs de Glue.', en: 'Write queries to analyze Glue logs.', pt: 'Escreva queries para analisar logs do Glue.' }, difficulty: 'medium', type: 'sql', starterCode: '# TODO: Queries para encontrar errores y mÃ©tricas', solution: '-- Errores en los Ãºltimos 7 dÃ­as\nfields @timestamp, @message\n| filter @message like /ERROR|Exception|Failed/\n| sort @timestamp desc\n| limit 100\n\n-- Jobs mÃ¡s lentos\nfields @timestamp, @message\n| filter @message like /Job.*completed/\n| parse @message /Job .* completed in (?<duration>\\d+) seconds/\n| stats avg(duration) as avg_duration, max(duration) as max_duration by bin(1d)\n\n-- Memoria utilizada por job\nfields @timestamp, @message  \n| filter @message like /memory/\n| parse @message /used: (?<used_mb>\\d+) MB.*total: (?<total_mb>\\d+) MB/\n| stats avg(used_mb/total_mb*100) as memory_pct by bin(5m)\n\n-- Top errores por tipo\nfields @timestamp, @message\n| filter @message like /Exception/\n| parse @message /(?<exception_type>\\w+Exception)/\n| stats count() as count by exception_type\n| sort count desc\n| limit 10', testCode: 'aws logs start-query ...', hints: [{ es: 'parse extrae valores usando regex named groups', en: 'parse extracts values using regex named groups', pt: 'parse extrai valores usando regex named groups' }], xpReward: 70, tags: ['cloudwatch', 'logs', 'analytics'] },
  { id: 'aws-ex-34', phaseId: 'phase3', title: { es: 'Lake Formation setup', en: 'Lake Formation setup', pt: 'ConfiguraÃ§Ã£o Lake Formation' }, description: { es: 'Configura governance con Lake Formation.', en: 'Configure governance with Lake Formation.', pt: 'Configure governanÃ§a com Lake Formation.' }, difficulty: 'hard', type: 'cli', starterCode: '# TODO: Registrar S3, crear permisos por columna', solution: '# Registrar ubicaciÃ³n de datos\naws lakeformation register-resource \\\n  --resource-arn arn:aws:s3:::datalake-bucket \\\n  --use-service-linked-role\n\n# Otorgar permisos a base de datos\naws lakeformation grant-permissions \\\n  --principal DataLakePrincipalIdentifier=arn:aws:iam::123456789012:role/DataAnalystRole \\\n  --resource \'{"Database": {"Name": "datalake_db"}}\' \\\n  --permissions DESCRIBE\n\n# Permisos a tabla con column-level\naws lakeformation grant-permissions \\\n  --principal DataLakePrincipalIdentifier=arn:aws:iam::123456789012:role/DataAnalystRole \\\n  --resource \'{\n    "TableWithColumns": {\n      "DatabaseName": "datalake_db",\n      "Name": "sales",\n      "ColumnNames": ["product", "amount", "date"]\n    }\n  }\' \\\n  --permissions SELECT\n\n# Los analistas NO verÃ¡n columnas sensibles como customer_email, credit_card, etc.', testCode: 'aws lakeformation list-permissions', hints: [{ es: 'Lake Formation simplifica IAM para Data Lake - usa permisos declarativos', en: 'Lake Formation simplifies IAM for Data Lake - use declarative permissions', pt: 'Lake Formation simplifica IAM para Data Lake - use permissÃµes declarativas' }], xpReward: 90, tags: ['lakeformation', 'governance', 'security'] },
  { id: 'aws-ex-35', phaseId: 'phase12', title: { es: 'Documentar arquitectura', en: 'Document architecture', pt: 'Documentar arquitetura' }, description: { es: 'Crea documentaciÃ³n profesional de tu Data Platform.', en: 'Create professional documentation for your Data Platform.', pt: 'Crie documentaÃ§Ã£o profissional da sua Data Platform.' }, difficulty: 'medium', type: 'markdown', starterCode: '# TODO: README.md con arquitectura, setup, costos', solution: '# Data Platform - E-Commerce Analytics\n\n## ğŸ—ï¸ Arquitectura\n\n![Architecture Diagram](docs/architecture.png)\n\n### Componentes\n\n| Servicio | PropÃ³sito | Costo Estimado |\n|----------|-----------|----------------|\n| S3 | Data Lake storage | ~$23/TB/mes |\n| Glue | ETL y Catalog | ~$0.44/DPU-hora |\n| Athena | Ad-hoc queries | ~$5/TB escaneado |\n| Kinesis | Streaming | ~$0.015/shard-hora |\n| Step Functions | OrquestaciÃ³n | ~$0.025/1000 transitions |\n\n### Data Flow\n\n1. **Ingesta Batch**: DMS â†’ S3 Raw (daily)\n2. **Ingesta Streaming**: API â†’ Kinesis â†’ Firehose â†’ S3\n3. **Procesamiento**: Glue Jobs (raw â†’ processed â†’ serving)\n4. **Consumo**: Athena (analysts), Redshift (BI), SageMaker (ML)\n\n## ğŸš€ Quick Start\n\n```bash\n# Clonar repo\ngit clone https://github.com/empresa/data-platform\ncd data-platform\n\n# Desplegar infraestructura\ncd terraform\nterraform init\nterraform apply\n\n# Ejecutar pipeline\naws stepfunctions start-execution \\\n  --state-machine-arn arn:aws:states:...:ETLPipeline\n```\n\n## ğŸ“Š Monitoreo\n\n- Dashboard: [CloudWatch Dashboard](https://console.aws.amazon.com/cloudwatch/...)\n- Alertas: SNS Topic `pipeline-alerts`\n- Logs: `/aws/glue/jobs/*`\n\n## ğŸ’° Costos\n\nEstimaciÃ³n mensual para 1TB/dÃ­a:\n- **MÃ­nimo**: ~$500 (desarrollo)\n- **ProducciÃ³n**: ~$2000-3000\n\n## ğŸ” Seguridad\n\n- Todos los datos encriptados con KMS\n- Lake Formation para governance\n- IAM roles con mÃ­nimo privilegio\n- VPC endpoints para servicios', testCode: '# Verificar que el README tiene todas las secciones', hints: [{ es: 'La buena documentaciÃ³n es tan importante como el cÃ³digo', en: 'Good documentation is as important as the code', pt: 'Boa documentaÃ§Ã£o Ã© tÃ£o importante quanto o cÃ³digo' }], xpReward: 60, tags: ['documentation', 'portfolio'] },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // EJERCICIOS NIVEL 1: LAMBDA
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  { id: 'aws-ex-36', phaseId: 'phaseLambda', title: { es: 'Lambda con S3 Trigger', en: 'Lambda with S3 Trigger', pt: 'Lambda com S3 Trigger' }, description: { es: 'Crea una Lambda que se ejecute cuando un archivo llega a S3.', en: 'Create a Lambda that runs when a file arrives in S3.', pt: 'Crie uma Lambda que executa quando um arquivo chega no S3.' }, difficulty: 'medium', type: 'python', services: ['Lambda', 'S3'], starterCode: 'import json\nimport boto3\n\ns3 = boto3.client("s3")\n\ndef lambda_handler(event, context):\n    # TODO: Extraer bucket y key del evento S3\n    # TODO: Leer el archivo\n    # TODO: Contar filas\n    # TODO: Retornar resultado\n    pass', solution: 'import json\nimport boto3\nimport io\n\ns3 = boto3.client("s3")\n\ndef lambda_handler(event, context):\n    record = event["Records"][0]\n    bucket = record["s3"]["bucket"]["name"]\n    key = record["s3"]["object"]["key"]\n    size = record["s3"]["object"].get("size", 0)\n    \n    print(f"Procesando: s3://{bucket}/{key} ({size} bytes)")\n    \n    response = s3.get_object(Bucket=bucket, Key=key)\n    content = response["Body"].read().decode("utf-8")\n    rows = len(content.strip().split("\\n")) - 1  # -1 for header\n    \n    result = {\n        "statusCode": 200,\n        "body": json.dumps({\n            "bucket": bucket,\n            "key": key,\n            "rows": rows\n        })\n    }\n    print(f"Resultado: {json.dumps(result)}")\n    return result', testCode: 'aws lambda invoke --function-name my-s3-trigger --payload file://test-event.json output.json && cat output.json', hints: [{ es: 'El evento S3 viene en event["Records"][0]["s3"]', en: 'The S3 event is in event["Records"][0]["s3"]', pt: 'O evento S3 vem em event["Records"][0]["s3"]' }], xpReward: 75, tags: ['lambda', 's3', 'trigger'] },
  { id: 'aws-ex-37', phaseId: 'phaseLambda', title: { es: 'Lambda CSV a Parquet', en: 'Lambda CSV to Parquet', pt: 'Lambda CSV para Parquet' }, description: { es: 'Convierte CSV a Parquet en Lambda con pandas y pyarrow.', en: 'Convert CSV to Parquet in Lambda with pandas and pyarrow.', pt: 'Converta CSV para Parquet no Lambda com pandas e pyarrow.' }, difficulty: 'hard', type: 'python', services: ['Lambda', 'S3'], starterCode: 'import pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport io, boto3\n\ns3 = boto3.client("s3")\n\ndef lambda_handler(event, context):\n    # TODO: Leer CSV de S3\n    # TODO: Limpiar datos\n    # TODO: Convertir a Parquet\n    # TODO: Subir a Silver\n    pass', solution: 'import pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport io, boto3, os\nfrom datetime import datetime\n\ns3 = boto3.client("s3")\n\ndef lambda_handler(event, context):\n    record = event["Records"][0]\n    bucket = record["s3"]["bucket"]["name"]\n    key = record["s3"]["object"]["key"]\n    \n    # Leer CSV\n    response = s3.get_object(Bucket=bucket, Key=key)\n    df = pd.read_csv(io.BytesIO(response["Body"].read()))\n    original = len(df)\n    \n    # Limpiar\n    df = df.drop_duplicates()\n    df = df.dropna(subset=df.columns[:3])\n    df["_processed_at"] = datetime.utcnow().isoformat()\n    \n    # Convertir a Parquet\n    table = pa.Table.from_pandas(df)\n    buffer = io.BytesIO()\n    pq.write_table(table, buffer, compression="snappy")\n    buffer.seek(0)\n    \n    # Subir a Silver\n    now = datetime.utcnow()\n    output_key = f"silver/data/year={now.year}/month={now.month:02d}/{key.split(\"/\")[-1].replace(\".csv\", \".parquet\")}"\n    s3.put_object(Bucket=bucket, Key=output_key, Body=buffer.getvalue())\n    \n    return {"rows_in": original, "rows_out": len(df), "output": output_key}', testCode: 'aws s3 ls s3://$BUCKET/silver/ --recursive', hints: [{ es: 'Usa compression="snappy" en Parquet para balance entre velocidad y tamaÃ±o', en: 'Use compression="snappy" in Parquet for speed/size balance', pt: 'Use compression="snappy" no Parquet para balanÃ§o velocidade/tamanho' }], xpReward: 100, tags: ['lambda', 'parquet', 'etl', 'pandas'] },
  { id: 'aws-ex-38', phaseId: 'phaseLambda', title: { es: 'Lambda con Secrets Manager', en: 'Lambda with Secrets Manager', pt: 'Lambda com Secrets Manager' }, description: { es: 'Lee credenciales de Secrets Manager con caching.', en: 'Read credentials from Secrets Manager with caching.', pt: 'Leia credenciais do Secrets Manager com caching.' }, difficulty: 'medium', type: 'python', services: ['Lambda', 'Secrets Manager'], starterCode: 'import boto3\nimport json\n\n# TODO: Implementar get_secret con caching\n# TODO: Lambda handler que lea un secret', solution: 'import boto3\nimport json\nimport os\n\nsecrets_client = boto3.client("secretsmanager")\n_cache = {}\n\ndef get_secret(secret_name):\n    """Lee secret con caching en variable global."""\n    if secret_name not in _cache:\n        response = secrets_client.get_secret_value(SecretId=secret_name)\n        _cache[secret_name] = json.loads(response["SecretString"])\n    return _cache[secret_name]\n\ndef lambda_handler(event, context):\n    secret_name = os.environ.get("SECRET_NAME", "dev/database/config")\n    secret = get_secret(secret_name)\n    \n    # Usar credenciales (nunca logear el password!)\n    print(f"Host: {secret[\"host\"]}")\n    print(f"Port: {secret[\"port\"]}")\n    print(f"User: {secret[\"username\"]}")\n    print(f"DB: {secret[\"dbname\"]}")\n    # secret["password"] disponible pero NO logear\n    \n    return {"status": "connected", "host": secret["host"]}', testCode: 'aws secretsmanager list-secrets', hints: [{ es: 'El cache en variable global se reutiliza en warm invocations', en: 'Cache in global variable is reused in warm invocations', pt: 'Cache em variÃ¡vel global Ã© reutilizado em warm invocations' }], xpReward: 70, tags: ['lambda', 'secrets-manager', 'security'] },
  { id: 'aws-ex-39', phaseId: 'phaseLambda', title: { es: 'Lambda con DynamoDB idempotencia', en: 'Lambda with DynamoDB idempotency', pt: 'Lambda com DynamoDB idempotÃªncia' }, description: { es: 'Implementa idempotencia para no procesar archivos duplicados.', en: 'Implement idempotency to not process duplicate files.', pt: 'Implemente idempotÃªncia para nÃ£o processar arquivos duplicados.' }, difficulty: 'hard', type: 'python', services: ['Lambda', 'DynamoDB'], starterCode: 'import boto3\nfrom datetime import datetime\n\n# TODO: Verificar si ya procesado\n# TODO: Registrar procesamiento\n# TODO: Skip si duplicado', solution: 'import boto3\nfrom datetime import datetime, timedelta\nimport json\n\ndynamodb = boto3.resource("dynamodb")\ntable = dynamodb.Table("pipeline-state")\n\ndef is_processed(file_key):\n    """Verifica si archivo ya fue procesado."""\n    response = table.get_item(Key={"file_key": file_key})\n    return "Item" in response and response["Item"]["status"] == "success"\n\ndef mark_processed(file_key, rows):\n    """Marca archivo como procesado con TTL de 90 dÃ­as."""\n    table.put_item(Item={\n        "file_key": file_key,\n        "status": "success",\n        "rows_processed": rows,\n        "processed_at": datetime.utcnow().isoformat(),\n        "ttl": int((datetime.utcnow() + timedelta(days=90)).timestamp())\n    })\n\ndef lambda_handler(event, context):\n    record = event["Records"][0]\n    bucket = record["s3"]["bucket"]["name"]\n    key = record["s3"]["object"]["key"]\n    file_key = f"{bucket}/{key}"\n    \n    if is_processed(file_key):\n        print(f"SKIP: {file_key} ya procesado")\n        return {"status": "skipped", "reason": "already_processed"}\n    \n    # Procesar...\n    rows = 1000  # resultado del procesamiento\n    mark_processed(file_key, rows)\n    return {"status": "processed", "rows": rows}', testCode: 'aws dynamodb scan --table-name pipeline-state --limit 5', hints: [{ es: 'Usa TTL para que DynamoDB borre registros viejos automÃ¡ticamente', en: 'Use TTL for DynamoDB to auto-delete old records', pt: 'Use TTL para DynamoDB deletar registros antigos automaticamente' }], xpReward: 85, tags: ['lambda', 'dynamodb', 'idempotency'] },
  { id: 'aws-ex-40', phaseId: 'phaseLambda', title: { es: 'Lambda Layer con pandas', en: 'Lambda Layer with pandas', pt: 'Lambda Layer com pandas' }, description: { es: 'Crea un Layer con pandas/pyarrow para reutilizar entre funciones.', en: 'Create a Layer with pandas/pyarrow to reuse between functions.', pt: 'Crie um Layer com pandas/pyarrow para reutilizar entre funÃ§Ãµes.' }, difficulty: 'medium', type: 'cli', services: ['Lambda'], starterCode: '# TODO: Crear layer con pandas+pyarrow\n# 1. Instalar en directorio layer/python/\n# 2. Zip\n# 3. Publicar\n# 4. Agregar a funciÃ³n', solution: '# Crear directorio\nmkdir -p layer/python\n\n# Instalar para Lambda (linux x86_64)\npip install pandas pyarrow \\\n  -t layer/python/ \\\n  --platform manylinux2014_x86_64 \\\n  --only-binary=:all:\n\n# Empaquetar\ncd layer && zip -r ../pandas-layer.zip python/ && cd ..\n\n# Publicar layer\nLAYER_ARN=$(aws lambda publish-layer-version \\\n  --layer-name pandas-pyarrow \\\n  --zip-file fileb://pandas-layer.zip \\\n  --compatible-runtimes python3.11 python3.12 \\\n  --description "pandas + pyarrow for ETL" \\\n  --query LayerVersionArn --output text)\n\necho "Layer ARN: $LAYER_ARN"\n\n# Agregar a funciÃ³n existente\naws lambda update-function-configuration \\\n  --function-name etl-bronze-to-silver \\\n  --layers $LAYER_ARN', testCode: 'aws lambda list-layers', hints: [{ es: 'Alternativa: usar el Layer oficial AWSSDKPandas de AWS', en: 'Alternative: use the official AWSSDKPandas Layer from AWS', pt: 'Alternativa: use o Layer oficial AWSSDKPandas da AWS' }], xpReward: 65, tags: ['lambda', 'layers', 'dependencies'] },

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // EJERCICIOS NIVEL 1: FARGATE
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  { id: 'aws-ex-41', phaseId: 'phaseFargate', title: { es: 'Dockerfile para ETL Python', en: 'Dockerfile for Python ETL', pt: 'Dockerfile para ETL Python' }, description: { es: 'Crea un Dockerfile multi-stage optimizado para ETL.', en: 'Create an optimized multi-stage Dockerfile for ETL.', pt: 'Crie um Dockerfile multi-stage otimizado para ETL.' }, difficulty: 'medium', type: 'cli', services: ['ECR', 'ECS'], starterCode: '# Dockerfile\n# TODO: Multi-stage build\n# TODO: Instalar pandas, pyarrow, boto3\n# TODO: Copiar script ETL\n# TODO: ENTRYPOINT', solution: '# --- Build stage ---\nFROM python:3.12-slim AS builder\n\nWORKDIR /build\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --target=/build/deps -r requirements.txt\n\n# --- Runtime stage ---\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Copiar dependencias del builder\nCOPY --from=builder /build/deps /usr/local/lib/python3.12/site-packages/\n\n# Copiar scripts ETL\nCOPY etl/ ./etl/\n\n# Variables de entorno\nENV PYTHONUNBUFFERED=1\nENV AWS_DEFAULT_REGION=us-east-1\n\n# Ejecutar ETL\nENTRYPOINT ["python", "etl/process.py"]\n\n# requirements.txt:\n# pandas==2.2.0\n# pyarrow==15.0.0\n# boto3==1.34.0\n\n# .dockerignore:\n# .git\n# __pycache__\n# *.pyc\n# tests/\n# docs/\n# .env', testCode: 'docker build -t etl-fargate . && docker images etl-fargate', hints: [{ es: 'Multi-stage build reduce la imagen de ~1.5GB a ~200MB', en: 'Multi-stage build reduces image from ~1.5GB to ~200MB', pt: 'Multi-stage build reduz a imagem de ~1.5GB para ~200MB' }], xpReward: 70, tags: ['docker', 'fargate', 'optimization'] },
  { id: 'aws-ex-42', phaseId: 'phaseFargate', title: { es: 'Push imagen a ECR', en: 'Push image to ECR', pt: 'Push imagem para ECR' }, description: { es: 'Crea repositorio ECR, build y push de imagen Docker.', en: 'Create ECR repository, build and push Docker image.', pt: 'Crie repositÃ³rio ECR, build e push de imagem Docker.' }, difficulty: 'medium', type: 'cli', services: ['ECR'], starterCode: '# TODO: Crear repo, login, build, tag, push', solution: 'ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nREGION=us-east-1\nREPO_NAME=etl-fargate\n\n# Crear repositorio\naws ecr create-repository --repository-name $REPO_NAME\n\n# Login\naws ecr get-login-password --region $REGION | \\\n  docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n\n# Build\ndocker build -t $REPO_NAME .\n\n# Tag\ndocker tag $REPO_NAME:latest $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPO_NAME:latest\n\n# Push\ndocker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPO_NAME:latest\n\n# Lifecycle policy (borrar imÃ¡genes > 30 dÃ­as)\naws ecr put-lifecycle-policy --repository-name $REPO_NAME --lifecycle-policy-text \'{\n  "rules": [{"rulePriority": 1, "selection": {"tagStatus": "any", "countType": "sinceImagePushed", "countUnit": "days", "countNumber": 30}, "action": {"type": "expire"}}]\n}\'', testCode: 'aws ecr describe-images --repository-name etl-fargate', hints: [{ es: 'El lifecycle policy limpia imÃ¡genes viejas para reducir costos de storage', en: 'Lifecycle policy cleans old images to reduce storage costs', pt: 'Lifecycle policy limpa imagens antigas para reduzir custos de storage' }], xpReward: 65, tags: ['ecr', 'docker', 'registry'] },
  { id: 'aws-ex-43', phaseId: 'phaseFargate', title: { es: 'Ejecutar tarea Fargate', en: 'Run Fargate task', pt: 'Executar tarefa Fargate' }, description: { es: 'Crea Task Definition y ejecuta una tarea Fargate.', en: 'Create Task Definition and run a Fargate task.', pt: 'Crie Task Definition e execute uma tarefa Fargate.' }, difficulty: 'hard', type: 'cli', services: ['ECS', 'Fargate'], starterCode: '# TODO: Crear cluster, task definition, ejecutar', solution: '# Crear cluster\naws ecs create-cluster --cluster-name etl-cluster\n\n# Registrar task definition\naws ecs register-task-definition --cli-input-json \'{\n  "family": "etl-heavy",\n  "networkMode": "awsvpc",\n  "requiresCompatibilities": ["FARGATE"],\n  "cpu": "1024",\n  "memory": "4096",\n  "executionRoleArn": "arn:aws:iam::ACCOUNT:role/ecsTaskExecutionRole",\n  "taskRoleArn": "arn:aws:iam::ACCOUNT:role/ecsTaskRole",\n  "containerDefinitions": [{\n    "name": "etl",\n    "image": "ACCOUNT.dkr.ecr.us-east-1.amazonaws.com/etl-fargate:latest",\n    "essential": true,\n    "environment": [\n      {"name": "INPUT_BUCKET", "value": "my-datalake"},\n      {"name": "INPUT_KEY", "value": "bronze/logs/big_file.csv"},\n      {"name": "OUTPUT_PREFIX", "value": "gold/logs/"}\n    ],\n    "logConfiguration": {\n      "logDriver": "awslogs",\n      "options": {\n        "awslogs-group": "/ecs/etl-heavy",\n        "awslogs-region": "us-east-1",\n        "awslogs-stream-prefix": "etl"\n      }\n    }\n  }]\n}\'\n\n# Ejecutar tarea\naws ecs run-task \\\n  --cluster etl-cluster \\\n  --task-definition etl-heavy \\\n  --launch-type FARGATE \\\n  --network-configuration \'{\n    "awsvpcConfiguration": {\n      "subnets": ["subnet-xxx"],\n      "assignPublicIp": "ENABLED"\n    }\n  }\'', testCode: 'aws ecs list-tasks --cluster etl-cluster', hints: [{ es: 'Usa assignPublicIp ENABLED si necesitas acceso a S3 sin VPC endpoint', en: 'Use assignPublicIp ENABLED if you need S3 access without VPC endpoint', pt: 'Use assignPublicIp ENABLED se precisar acesso ao S3 sem VPC endpoint' }], xpReward: 90, tags: ['fargate', 'ecs', 'task-definition'] },
  { id: 'aws-ex-44', phaseId: 'phaseFargate', title: { es: 'Fargate con Step Functions', en: 'Fargate with Step Functions', pt: 'Fargate com Step Functions' }, description: { es: 'Orquesta una tarea Fargate con Step Functions.', en: 'Orchestrate a Fargate task with Step Functions.', pt: 'Orquestre uma tarefa Fargate com Step Functions.' }, difficulty: 'hard', type: 'json', services: ['ECS', 'Step Functions'], starterCode: '// TODO: State machine que lance Fargate y espere', solution: '{\n  "Comment": "ETL Fargate Pipeline",\n  "StartAt": "RunFargateTask",\n  "States": {\n    "RunFargateTask": {\n      "Type": "Task",\n      "Resource": "arn:aws:states:::ecs:runTask.sync",\n      "Parameters": {\n        "LaunchType": "FARGATE",\n        "Cluster": "arn:aws:ecs:us-east-1:ACCOUNT:cluster/etl-cluster",\n        "TaskDefinition": "arn:aws:ecs:us-east-1:ACCOUNT:task-definition/etl-heavy",\n        "NetworkConfiguration": {\n          "AwsvpcConfiguration": {\n            "Subnets": ["subnet-xxx"],\n            "AssignPublicIp": "ENABLED"\n          }\n        },\n        "Overrides": {\n          "ContainerOverrides": [{\n            "Name": "etl",\n            "Environment": [\n              {"Name": "INPUT_KEY", "Value.$": "$.input_key"}\n            ]\n          }]\n        }\n      },\n      "Retry": [{"ErrorEquals": ["States.ALL"], "MaxAttempts": 2, "BackoffRate": 2}],\n      "Catch": [{"ErrorEquals": ["States.ALL"], "Next": "NotifyFailure"}],\n      "Next": "NotifySuccess"\n    },\n    "NotifySuccess": {\n      "Type": "Task",\n      "Resource": "arn:aws:states:::sns:publish",\n      "Parameters": {"TopicArn": "arn:aws:sns:...:alerts", "Subject": "Fargate ETL OK"},\n      "End": true\n    },\n    "NotifyFailure": {\n      "Type": "Task",\n      "Resource": "arn:aws:states:::sns:publish",\n      "Parameters": {"TopicArn": "arn:aws:sns:...:alerts", "Subject": "Fargate ETL FAILED"},\n      "End": true\n    }\n  }\n}', testCode: 'aws stepfunctions start-execution --state-machine-arn ...', hints: [{ es: 'Usa .sync para que Step Functions espere a que Fargate termine', en: 'Use .sync for Step Functions to wait for Fargate to finish', pt: 'Use .sync para Step Functions esperar o Fargate terminar' }], xpReward: 95, tags: ['fargate', 'stepfunctions', 'orchestration'] }
];

// Helper para obtener ejercicios por fase
export const getExercisesByPhase = (phaseId: string): AWSExercise[] => {
  return awsExercises.filter(ex => ex.phaseId === phaseId);
};

// Helper para obtener ejercicios por dificultad
export const getExercisesByDifficulty = (difficulty: 'easy' | 'medium' | 'hard'): AWSExercise[] => {
  return awsExercises.filter(ex => ex.difficulty === difficulty);
};

// EstadÃ­sticas
export const exerciseStats = {
  total: awsExercises.length,
  byDifficulty: {
    easy: awsExercises.filter(ex => ex.difficulty === 'easy').length,
    medium: awsExercises.filter(ex => ex.difficulty === 'medium').length,
    hard: awsExercises.filter(ex => ex.difficulty === 'hard').length
  },
  totalXP: awsExercises.reduce((sum, ex) => sum + ex.xpReward, 0)
};








