import { Project } from '../../../types/members';

export const p1_etl_python: Project = {
  id: 'p1-etl-python',
  level: 1,
  title: { es: 'ETL Simple con Python', en: 'Simple ETL with Python', pt: 'ETL Simples com Python' },
  description: {
    es: 'Tu primer pipeline de datos real: extraer de CSV, transformar con Pandas, y cargar resultados. Este es el patr√≥n que vas a usar en el 80% de tu trabajo como Data Engineer.',
    en: 'Your first real data pipeline: extract from CSV, transform with Pandas, and load results. This is the pattern you will use in 80% of your work as a Data Engineer.',
    pt: 'Seu primeiro pipeline de dados real: extrair de CSV, transformar com Pandas e carregar resultados. Este √© o padr√£o que voc√™ usar√° em 80% do seu trabalho como Data Engineer.'
  },
  difficulty: 'Principiante',
  duration: '2-3 horas',
  skills: [
    { es: 'Python' }, { es: 'Pandas' }, { es: 'CSV' }, { es: 'JSON' }, { es: 'Parquet' }, 
    { es: 'Data Cleaning', en: 'Data Cleaning', pt: 'Limpeza de Dados' }
  ],
  icon: 'üêç',
  color: 'emerald',
  datasetId: 'ecommerce',
  estimatedLines: 80,
  realWorldExample: {
    es: 'As√≠ es como Spotify procesa datos de reproducciones para generar reportes diarios',
    en: 'This is how Spotify processes playback data to generate daily reports',
    pt: '√â assim que o Spotify processa dados de reprodu√ß√µes para gerar relat√≥rios di√°rios'
  },
  usedBy: ['Spotify', 'Netflix', 'MercadoLibre', 'Rappi'],
  learningObjectives: [
    { es: 'Entender el patr√≥n ETL (Extract, Transform, Load)', en: 'Understand the ETL pattern (Extract, Transform, Load)', pt: 'Entender o padr√£o ETL (Extract, Transform, Load)' },
    { es: 'Manipular DataFrames con Pandas', en: 'Manipulate DataFrames with Pandas', pt: 'Manipular DataFrames com Pandas' },
    { es: 'Limpiar datos: nulos, duplicados, tipos', en: 'Clean data: nulls, duplicates, types', pt: 'Limpar dados: nulos, duplicados, tipos' },
    { es: 'Guardar en formatos optimizados (Parquet)', en: 'Save in optimized formats (Parquet)', pt: 'Salvar em formatos otimizados (Parquet)' },
    { es: 'Documentar c√≥digo profesionalmente', en: 'Document code professionally', pt: 'Documentar c√≥digo profissionalmente' },
  ],
  commonMistakes: [
    {
      mistake: { es: 'No revisar los datos antes de transformar', en: 'Not checking data before transforming', pt: 'N√£o verificar os dados antes de transformar' },
      why: { es: 'Pod√©s aplicar transformaciones incorrectas si no conoc√©s la estructura', en: 'You can apply incorrect transformations if you don\'t know the structure', pt: 'Voc√™ pode aplicar transforma√ß√µes incorretas se n√£o conhecer a estrutura' },
      solution: { es: 'Siempre us√° df.head(), df.info(), df.describe() antes de cualquier transformaci√≥n', en: 'Always use df.head(), df.info(), df.describe() before any transformation', pt: 'Sempre use df.head(), df.info(), df.describe() antes de qualquer transforma√ß√£o' },
      code: `# SIEMPRE empez√° con esto:
df.head()      # Ver primeras filas
df.info()      # Ver tipos y nulos
df.describe()  # Estad√≠sticas b√°sicas`
    },
    {
      mistake: { es: 'Olvidar manejar valores nulos', en: 'Forgetting to handle null values', pt: 'Esquecer de lidar com valores nulos' },
      why: { es: 'Los nulos pueden causar errores en c√°lculos o resultados incorrectos', en: 'Nulls can cause calculation errors or incorrect results', pt: 'Nulos podem causar erros em c√°lculos ou resultados incorretos' },
      solution: { es: 'Decid√≠ qu√© hacer con cada columna: eliminar, rellenar con promedio, o dejar', en: 'Decide what to do with each column: drop, fill with mean, or leave', pt: 'Decida o que fazer com cada coluna: remover, preencher com m√©dia ou deixar' },
      code: `# Opci√≥n 1: Eliminar filas con nulos
df = df.dropna()

# Opci√≥n 2: Rellenar con valor
df['columna'] = df['columna'].fillna(0)

# Opci√≥n 3: Rellenar con promedio
df['precio'] = df['precio'].fillna(df['precio'].mean())`
    },
    {
      mistake: { es: 'No documentar las decisiones de limpieza', en: 'Not documenting cleaning decisions', pt: 'N√£o documentar as decis√µes de limpeza' },
      why: { es: 'En 3 meses no vas a recordar por qu√© eliminaste esas filas', en: 'In 3 months you won\'t remember why you dropped those rows', pt: 'Em 3 meses voc√™ n√£o vai lembrar por que removeu essas linhas' },
      solution: { es: 'Agreg√° comentarios explicando el "por qu√©" de cada decisi√≥n', en: 'Add comments explaining the "why" of each decision', pt: 'Adicione coment√°rios explicando o "porqu√™" de cada decis√£o' },
      code: `# Eliminamos filas sin customer_id porque son √≥rdenes de prueba
# del equipo de QA (confirmado con el equipo de producto)
df = df.dropna(subset=['customer_id'])`
    },
    {
      mistake: { es: 'Usar CSV en vez de Parquet para datos grandes', en: 'Using CSV instead of Parquet for large data', pt: 'Usar CSV em vez de Parquet para dados grandes' },
      why: { es: 'CSV es 10x m√°s lento y ocupa 5x m√°s espacio', en: 'CSV is 10x slower and takes 5x more space', pt: 'CSV √© 10x mais lento e ocupa 5x mais espa√ßo' },
      solution: { es: 'Us√° Parquet para cualquier archivo > 100MB', en: 'Use Parquet for any file > 100MB', pt: 'Use Parquet para qualquer arquivo > 100MB' },
      code: `# ‚ùå Malo para datos grandes
df.to_csv('datos.csv')

# ‚úÖ Mucho mejor
df.to_parquet('datos.parquet')`
    },
  ],
  expectedOutputs: [
    {
      step: 3,
      description: { es: 'Output esperado al explorar los datos', en: 'Expected output when exploring data', pt: 'Output esperado ao explorar os dados' },
      example: `üìÇ Archivos encontrados en data/:
- ecommerce_orders.csv
- ecommerce_order_items.csv
- ecommerce_customers.csv
- ecommerce_products.csv
- ecommerce_categories.csv
... (14 archivos en total)

üìà Resumen:
Orders: 1000 filas, 8 columnas
Order Items: 2500 filas
Customers: 500 filas
Products: 200 filas`
    },
    {
      step: 7,
      description: { es: 'Output esperado de m√©tricas de negocio', en: 'Expected output of business metrics', pt: 'Output esperado de m√©tricas de neg√≥cio' },
      example: `üèÜ Top 5 clientes:
         total_gastado  cantidad_ordenes
customer_id                                
42              15420.50                12
156             12350.00                 8
89              11200.75                15
...

üì¶ Producto m√°s vendido: ID 23 (450 unidades)

üìà Ventas por mes:
    mes  total_ventas
2024-01      125000.00
2024-02      142000.00
2024-03      138500.00`
    },
    {
      step: 9,
      description: { es: 'Comparaci√≥n de tama√±os CSV vs Parquet', en: 'CSV vs Parquet size comparison', pt: 'Compara√ß√£o de tamanhos CSV vs Parquet' },
      example: `Tama√±o CSV: 2450.3 KB
Tama√±o Parquet: 312.1 KB
Parquet es 7.9x m√°s chico`
    }
  ],
  interviewStory: {
    hook: { es: "En mi √∫ltimo proyecto constru√≠ un pipeline ETL que procesaba datos de e-commerce y logr√© reducir el tiempo de generaci√≥n de reportes de 2 horas manuales a 3 minutos autom√°ticos.", en: "In my last project I built an ETL pipeline processing e-commerce data and managed to reduce reporting time from 2 manual hours to 3 automatic minutes.", pt: "No meu √∫ltimo projeto constru√≠ um pipeline ETL processando dados de e-commerce e consegui reduzir o tempo de gera√ß√£o de relat√≥rios de 2 horas manuais para 3 minutos autom√°ticos." },
    situation: { es: "Trabaj√© con un dataset de e-commerce que simulaba el sistema de una empresa real, con m√°s de 10 tablas relacionadas: √≥rdenes, productos, clientes, inventario. El problema era que el equipo de negocio tardaba horas en generar reportes manualmente en Excel.", en: "I worked with an e-commerce dataset simulating a real company system, with over 10 related tables: orders, products, customers, inventory. The problem was business team took hours to generate reports manually in Excel.", pt: "Trabalhei com um dataset de e-commerce simulando o sistema de uma empresa real, com mais de 10 tabelas relacionadas: pedidos, produtos, clientes, invent√°rio. O problema era que a equipe de neg√≥cio demorava horas para gerar relat√≥rios manualmente no Excel." },
    task: { es: "Mi objetivo era construir un pipeline automatizado que extrajera los datos, los limpiara, calculara m√©tricas de negocio, y los dejara listos para an√°lisis.", en: "My goal was to build an automated pipeline to extract data, clean it, calculate business metrics, and leave it ready for analysis.", pt: "Meu objetivo era construir um pipeline automatizado que extra√≠sse os dados, limpasse, calculasse m√©tricas de neg√≥cio e os deixasse prontos para an√°lise." },
    actions: [
      { es: "Dise√±√© la arquitectura del pipeline siguiendo el patr√≥n ETL cl√°sico", en: "Designed pipeline architecture following classic ETL pattern", pt: "Desenhei a arquitetura do pipeline seguindo o padr√£o ETL cl√°ssico" },
      { es: "Us√© Pandas para cargar y explorar los datos, identificando problemas de calidad", en: "Used Pandas to load and explore data, identifying quality issues", pt: "Usei Pandas para carregar e explorar os dados, identificando problemas de qualidade" },
      { es: "Implement√© limpieza de datos: manej√© 15% de valores nulos en precios usando el promedio por categor√≠a", en: "Implemented data cleaning: handled 15% null values in prices using category mean", pt: "Implementei limpeza de dados: tratei 15% de valores nulos em pre√ßos usando a m√©dia por categoria" },
      { es: "Elimin√© duplicados que representaban el 3% del dataset", en: "Removed duplicates representing 3% of the dataset", pt: "Removi duplicatas que representavam 3% do dataset" },
      { es: "Calcul√© m√©tricas clave: top clientes, productos m√°s vendidos, tendencias mensuales", en: "Calculated key metrics: top customers, best-selling products, monthly trends", pt: "Calculei m√©tricas chave: top clientes, produtos mais vendidos, tend√™ncias mensais" },
      { es: "Optimic√© el storage usando Parquet, reduciendo el tama√±o 8x vs CSV", en: "Optimized storage using Parquet, reducing size 8x vs CSV", pt: "Otimizei o storage usando Parquet, reduzindo o tamanho 8x vs CSV" }
    ],
    results: [
      { es: "Pipeline 100% automatizado que corre en 3 minutos", en: "100% automated pipeline running in 3 minutes", pt: "Pipeline 100% automatizado que roda em 3 minutos" },
      { es: "Reducci√≥n de 8x en tama√±o de archivos (de 2.4MB a 300KB)", en: "8x reduction in file size (from 2.4MB to 300KB)", pt: "Redu√ß√£o de 8x no tamanho de arquivos (de 2.4MB para 300KB)" },
      { es: "Identificamos que el 20% de los clientes generaban el 65% de las ventas", en: "Identified that 20% of customers generated 65% of sales", pt: "Identificamos que 20% dos clientes geravam 65% das vendas" },
      { es: "El equipo de negocio ahora tiene datos frescos cada ma√±ana", en: "Business team now has fresh data every morning", pt: "A equipe de neg√≥cio agora tem dados frescos toda manh√£" }
    ],
    learnings: [
      { es: "Aprend√≠ que explorar los datos ANTES de transformar es cr√≠tico - casi aplico transformaciones incorrectas por no revisar los tipos de datos", en: "Learned that exploring data BEFORE transforming is critical - almost applied incorrect transformations by not checking data types", pt: "Aprendi que explorar os dados ANTES de transformar √© cr√≠tico - quase apliquei transforma√ß√µes incorretas por n√£o verificar os tipos de dados" },
      { es: "Documentar las decisiones de limpieza es esencial - en un mes no recordar√≠a por qu√© elimin√© ciertas filas", en: "Documenting cleaning decisions is essential - in a month I wouldn't remember why I dropped certain rows", pt: "Documentar as decis√µes de limpeza √© essencial - em um m√™s n√£o lembraria por que removi certas linhas" },
      { es: "Parquet no es solo m√°s chico, tambi√©n es m√°s r√°pido de leer - esto importa cuando escal√°s", en: "Parquet is not just smaller, it's also faster to read - this matters when scaling", pt: "Parquet n√£o √© s√≥ menor, tamb√©m √© mais r√°pido de ler - isso importa quando escala" }
    ],
    possibleQuestions: [
      {
        question: { es: "¬øC√≥mo manejaste los valores nulos?", en: "How did you handle null values?", pt: "Como voc√™ tratou os valores nulos?" },
        answer: { es: "Depend√≠a de la columna. Para precios us√© el promedio por categor√≠a porque ten√≠a sentido de negocio. Para customer_id elimin√© las filas porque eran √≥rdenes de prueba. Siempre document√© el 'por qu√©' de cada decisi√≥n.", en: "It depended on the column. For prices I used category mean because it made business sense. For customer_id I dropped rows because they were test orders. Always documented the 'why' of each decision.", pt: "Dependia da coluna. Para pre√ßos usei a m√©dia por categoria porque fazia sentido de neg√≥cio. Para customer_id removi as linhas porque eram pedidos de teste. Sempre documentei o 'porqu√™' de cada decis√£o." }
      },
      {
        question: { es: "¬øPor qu√© elegiste Parquet sobre CSV?", en: "Why did you choose Parquet over CSV?", pt: "Por que escolheu Parquet em vez de CSV?" },
        answer: { es: "Tres razones: es 8x m√°s chico por la compresi√≥n columnar, es m√°s r√°pido de leer porque solo carga las columnas que necesit√°s, y preserva los tipos de datos - con CSV perd√©s esa informaci√≥n.", en: "Three reasons: it's 8x smaller due to columnar compression, faster to read because it only loads columns you need, and preserves data types - with CSV you lose that info.", pt: "Tr√™s raz√µes: √© 8x menor pela compress√£o colunar, √© mais r√°pido de ler porque s√≥ carrega as colunas que voc√™ precisa, e preserva os tipos de dados - com CSV voc√™ perde essa informa√ß√£o." }
      },
      {
        question: { es: "¬øC√≥mo lo escalar√≠as para m√°s datos?", en: "How would you scale it for more data?", pt: "Como escalaria para mais dados?" },
        answer: { es: "Para datasets m√°s grandes usar√≠a chunks en Pandas o migrar√≠a a PySpark. Tambi√©n agregar√≠a procesamiento incremental en vez de full refresh.", en: "For larger datasets I would use chunks in Pandas or migrate to PySpark. Also would add incremental processing instead of full refresh.", pt: "Para datasets maiores usaria chunks no Pandas ou migraria para PySpark. Tamb√©m adicionaria processamento incremental em vez de full refresh." }
      }
    ],
    closingStatement: { es: "Este proyecto me ense√±√≥ que un buen Data Engineer no solo mueve datos, sino que entiende el problema de negocio y entrega valor medible.", en: "This project taught me that a good Data Engineer not only moves data, but understands the business problem and delivers measurable value.", pt: "Este projeto me ensinou que um bom Data Engineer n√£o s√≥ move dados, mas entende o problema de neg√≥cio e entrega valor mensur√°vel." }
  },
  steps: [
    { 
      order: 1, 
      text: { es: 'üì• EXTRACT: Descarg√° el dataset de E-commerce desde la pesta√±a Datasets', en: 'üì• EXTRACT: Download E-commerce dataset from Datasets tab', pt: 'üì• EXTRACT: Baixe o dataset de E-commerce da aba Datasets' },
      explanation: { es: 'El dataset contiene 11 tablas relacionadas: categories, brands, suppliers, warehouses, products, inventory, customers, promotions, orders, order_items y reviews. Esto simula una base de datos real de e-commerce.', en: 'Dataset contains 11 related tables: categories, brands, suppliers, warehouses, products, inventory, customers, promotions, orders, order_items and reviews. This simulates a real e-commerce database.', pt: 'O dataset cont√©m 11 tabelas relacionadas: categories, brands, suppliers, warehouses, products, inventory, customers, promotions, orders, order_items e reviews. Isso simula um banco de dados real de e-commerce.' },
      tip: { es: 'Descarg√° todos los archivos en formato CSV. Vas a obtener 14 archivos (uno por tabla): ecommerce_orders.csv, ecommerce_customers.csv, etc. Guard√° todos en la carpeta data/.', en: 'Download all files in CSV format. You will get 14 files (one per table): ecommerce_orders.csv, ecommerce_customers.csv, etc. Save all in the data/ folder.', pt: 'Baixe todos os arquivos em formato CSV. Voc√™ vai obter 14 arquivos (um por tabela): ecommerce_orders.csv, ecommerce_customers.csv, etc. Salve todos na pasta data/.' },
      checkpoint: { es: '¬øTen√©s el archivo descargado en tu carpeta de proyecto?', en: 'Do you have the file downloaded in your project folder?', pt: 'Voc√™ tem o arquivo baixado na sua pasta de projeto?' },
      estimatedTime: '5min',
      difficulty: 'easy'
    },
    { 
      order: 2, 
      text: { es: 'üìÇ Cre√° la estructura del proyecto', en: 'üìÇ Create project structure', pt: 'üìÇ Crie a estrutura do projeto' },
      code: `# En tu terminal (Mac/Linux: Terminal, Windows: PowerShell o CMD)

# 1. Primero, abr√≠ la terminal y fijate d√≥nde est√°s:
pwd          # Mac/Linux: muestra tu ubicaci√≥n actual
cd           # Windows: muestra tu ubicaci√≥n actual

# 2. And√° a tu carpeta de proyectos (ejemplo):
cd Documents   # o donde quieras guardar el proyecto

# 3. Cre√° la carpeta del proyecto y entr√°:
mkdir mi-primer-etl
cd mi-primer-etl

# 4. Cre√° los archivos y carpetas:
# En Windows (PowerShell):
New-Item etl.py -ItemType File
mkdir data, output

# En Mac/Linux (Terminal):
touch etl.py
mkdir data output

# 5. Mov√© los CSVs descargados a la carpeta data/
# Pod√©s arrastrarlos con el mouse o usar:
# Mac/Linux: mv ~/Downloads/ecommerce_*.csv ./data/
# Windows: move %USERPROFILE%\\Downloads\\ecommerce_*.csv .\\data\\`,
      explanation: { es: 'Una estructura clara hace tu proyecto m√°s profesional y f√°cil de entender. Pod√©s usar cualquier terminal.', en: 'Clear structure makes your project more professional and easier to understand. You can use any terminal.', pt: 'Uma estrutura clara torna seu projeto mais profissional e f√°cil de entender. Voc√™ pode usar qualquer terminal.' },
      tip: { es: 'üí° ¬øPerdido con la terminal? Record√°: pwd = d√≥nde estoy, ls = qu√© hay aqu√≠, cd = moverme. Si no hiciste el Nivel 0, revis√° la fase "Tu Computadora y la Terminal" para entender rutas y comandos b√°sicos.', en: 'üí° Lost with the terminal? Remember: pwd = where am I, ls = what is here, cd = move. If you didn\'t do Level 0, check the "Your Computer and Terminal" phase to understand paths and basic commands.', pt: 'üí° Perdido com o terminal? Lembre-se: pwd = onde estou, ls = o que tem aqui, cd = mover. Se n√£o fez o N√≠vel 0, confira a fase "Seu Computador e o Terminal" para entender caminhos e comandos b√°sicos.' },
      estimatedTime: '5min',
      difficulty: 'easy',
      expectedOutput: `mi-primer-etl/
‚îú‚îÄ‚îÄ etl.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ ecommerce_orders.csv
‚îÇ   ‚îú‚îÄ‚îÄ ecommerce_customers.csv
‚îÇ   ‚îú‚îÄ‚îÄ ecommerce_products.csv
‚îÇ   ‚îî‚îÄ‚îÄ ... (14 archivos CSV)
‚îî‚îÄ‚îÄ output/`
    },
    { 
      order: 3, 
      text: { es: 'üìñ Carg√° y explor√° los datos', en: 'üìñ Load and explore data', pt: 'üìñ Carregue e explore os dados' },
      code: `import pandas as pd
import glob
import os

# Verificar que existen los archivos CSV descargados
archivos = glob.glob('data/ecommerce_*.csv')
if not archivos:
    print("‚ùå No se encontraron los archivos. Asegurate de descargarlos en la carpeta data/")
    print("   Deber√≠as tener: ecommerce_orders.csv, ecommerce_customers.csv, etc.")
else:
    print(f"üìÇ Archivos encontrados: {len(archivos)}")
    for f in sorted(archivos):
        print(f"  - {os.path.basename(f)}")

# Cargar los CSVs principales
df_orders = pd.read_csv('data/ecommerce_orders.csv')
df_order_items = pd.read_csv('data/ecommerce_order_items.csv')
df_customers = pd.read_csv('data/ecommerce_customers.csv')
df_products = pd.read_csv('data/ecommerce_products.csv')

# Explorar
print(f"\\nüìà Resumen:")
print(f"Orders: {len(df_orders)} filas, {len(df_orders.columns)} columnas")
print(f"Order Items: {len(df_order_items)} filas")
print(f"Customers: {len(df_customers)} filas")
print(f"Products: {len(df_products)} filas")

print("\\nüîç Primeras filas de orders:")
print(df_orders.head())
print("\\nüìã Info de orders:")
print(df_orders.info())`,
      explanation: { es: 'Siempre explor√° los datos antes de transformar. df.info() te muestra tipos y nulos. Los archivos CSV tienen el formato ecommerce_TABLA.csv (ej: ecommerce_orders.csv).', en: 'Always explore data before transforming. df.info() shows types and nulls. CSV files follow the format ecommerce_TABLE.csv (ex: ecommerce_orders.csv).', pt: 'Sempre explore os dados antes de transformar. df.info() mostra tipos e nulos. Os arquivos CSV t√™m o formato ecommerce_TABELA.csv (ex: ecommerce_orders.csv).' },
      tip: { es: 'üí° ¬øError "archivo no encontrado"? Asegurate de ejecutar el c√≥digo DESDE la carpeta mi-primer-etl (us√° cd mi-primer-etl). La ruta "data/archivo.json" es RELATIVA - significa "desde donde estoy, entr√° a data/". Si no entend√©s rutas, revis√° la fase Terminal en el Nivel 0.', en: 'üí° "File not found" error? Make sure to run the code FROM the mi-primer-etl folder (use cd mi-primer-etl). The path "data/file.json" is RELATIVE - it means "from where I am, enter data/". If you don\'t understand paths, check the Terminal phase in Level 0.', pt: 'üí° Erro "arquivo n√£o encontrado"? Certifique-se de executar o c√≥digo A PARTIR da pasta mi-primer-etl (use cd mi-primer-etl). O caminho "data/arquivo.json" √© RELATIVO - significa "de onde estou, entre em data/". Se n√£o entende caminhos, confira a fase Terminal no N√≠vel 0.' },
      checkpoint: { es: '¬øPod√©s ver cu√°ntas filas tiene cada tabla? ¬øHay columnas con valores nulos?', en: 'Can you see how many rows each table has? Are there columns with null values?', pt: 'Consegue ver quantas linhas tem cada tabela? Existem colunas com valores nulos?' },
      estimatedTime: '15min',
      difficulty: 'easy'
    },
    { 
      order: 4, 
      text: { es: 'üîç TRANSFORM - Identific√° y manej√° nulos', en: 'üîç TRANSFORM - Identify and handle nulls', pt: 'üîç TRANSFORM - Identifique e trate nulos' },
      estimatedTime: '20min',
      difficulty: 'medium',
      challenge: { es: 'Antes de ver el c√≥digo, intent√°: ¬øC√≥mo ver√≠as cu√°ntos nulos hay por columna? ¬øQu√© har√≠as con ellos?', en: 'Before seeing code, try: How would you see how many nulls per column? What would you do with them?', pt: 'Antes de ver o c√≥digo, tente: Como veria quantos nulos h√° por coluna? O que faria com eles?' },
      explanation: { es: 'No hay una regla universal para manejar nulos. Depende del contexto. Lo importante es DOCUMENTAR tu decisi√≥n.', en: 'No universal rule for handling nulls. Context matters. Important thing is to DOCUMENT your decision.', pt: 'N√£o h√° regra universal para lidar com nulos. Depende do contexto. O importante √© DOCUMENTAR sua decis√£o.' },
      warning: { es: 'Nunca elimines nulos sin entender por qu√© est√°n ah√≠. A veces un nulo tiene significado (ej: cliente sin tel√©fono).', en: 'Never drop nulls without understanding why they are there. Sometimes null has meaning (ex: customer without phone).', pt: 'Nunca remova nulos sem entender por que est√£o l√°. √Äs vezes um nulo tem significado (ex: cliente sem telefone).' },
      hints: [
        {
          level: 1,
          title: { es: 'üí° Pista 1', en: 'üí° Hint 1', pt: 'üí° Dica 1' },
          content: { es: 'Para ver nulos us√° .isnull().sum() sobre el DataFrame. Te da un conteo por columna.', en: 'To see nulls use .isnull().sum() on DataFrame. Gives you a count per column.', pt: 'Para ver nulos use .isnull().sum() no DataFrame. Te d√° uma contagem por coluna.' }
        },
        {
          level: 2,
          title: { es: 'üí° Pista 2', en: 'üí° Hint 2', pt: 'üí° Dica 2' },
          content: { es: 'Para eliminar filas con nulos us√° .dropna(). Pod√©s especificar columnas con subset=[...].\nPara rellenar nulos us√° .fillna(valor).', en: 'To drop rows with nulls use .dropna(). You can specify columns with subset=[...].\nTo fill nulls use .fillna(value).', pt: 'Para remover linhas com nulos use .dropna(). Pode especificar colunas com subset=[...].\nPara preencher nulos use .fillna(valor).' },
          code: `# Eliminar donde hay nulos en columnas cr√≠ticas
df_clean = df.dropna(subset=['columna_importante'])

# Rellenar nulos con 0
df['columna'] = df['columna'].fillna(0)`
        },
        {
          level: 3,
          title: { es: '‚úÖ Soluci√≥n Completa', en: '‚úÖ Full Solution', pt: '‚úÖ Solu√ß√£o Completa' },
          content: { es: 'Ac√° est√° el c√≥digo completo para manejar nulos:', en: 'Here is the full code to handle nulls:', pt: 'Aqui est√° o c√≥digo completo para lidar com nulos:' },
          code: `# Ver nulos por columna
print("Nulos por columna:")
print(df_orders.isnull().sum())

# Decisi√≥n: ¬øeliminar o rellenar?
# Si son pocos (<5%), podemos eliminar
# Si son muchos, mejor rellenar con un valor por defecto

# Ejemplo: eliminar filas con nulos en campos cr√≠ticos
df_orders_clean = df_orders.dropna(subset=['customer_id', 'product_id', 'total'])

# Ejemplo: rellenar con 0 en campos num√©ricos opcionales
df_orders_clean['discount'] = df_orders_clean['discount'].fillna(0)

print(f"Filas antes: {len(df_orders)}, despu√©s: {len(df_orders_clean)}")`
        }
      ]
    },
    { 
      order: 5, 
      text: { es: 'üîç TRANSFORM - Elimin√° duplicados', en: 'üîç TRANSFORM - Remove duplicates', pt: 'üîç TRANSFORM - Remova duplicatas' },
      challenge: { es: 'Intent√°: ¬øC√≥mo detectar√≠as duplicados? ¬øQu√© pasa si un order_id aparece dos veces?', en: 'Try: How would you detect duplicates? What if an order_id appears twice?', pt: 'Tente: Como detectaria duplicatas? O que acontece se um order_id aparece duas vezes?' },
      explanation: { es: 'Los duplicados pueden venir de errores en la fuente o de cargas repetidas. Siempre verific√°.', en: 'Duplicates can come from source errors or repeated loads. Always verify.', pt: 'As duplicatas podem vir de erros na fonte ou de cargas repetidas. Sempre verifique.' },
      tip: { es: 'Us√° duplicated(subset=[...]) para buscar duplicados en columnas espec√≠ficas, no en toda la fila.', en: 'Use duplicated(subset=[...]) to check duplicates in specific columns, not entire row.', pt: 'Use duplicated(subset=[...]) para buscar duplicatas em colunas espec√≠ficas, n√£o em toda a linha.' },
      hints: [
        {
          level: 1,
          title: { es: 'üí° Pista 1', en: 'üí° Hint 1', pt: 'üí° Dica 1' },
          content: { es: 'Para ver duplicados us√° .duplicated().sum(). Te dice cu√°ntas filas son duplicadas.', en: 'To see duplicates use .duplicated().sum(). Tells you how many rows are duplicated.', pt: 'Para ver duplicatas use .duplicated().sum(). Diz quantas linhas s√£o duplicadas.' }
        },
        {
          level: 2,
          title: { es: 'üí° Pista 2', en: 'üí° Hint 2', pt: 'üí° Dica 2' },
          content: { es: 'Para eliminar duplicados us√° .drop_duplicates(). Pod√©s especificar qu√© columnas considerar con subset=[...].\nEl par√°metro keep puede ser "first", "last", o False.', en: 'To drop duplicates use .drop_duplicates(). You can specify columns with subset=[...].\nkeep parameter can be "first", "last", or False.', pt: 'Para remover duplicatas use .drop_duplicates(). Pode especificar quais colunas considerar com subset=[...].\nO par√¢metro keep pode ser "first", "last", ou False.' },
          code: `# Eliminar duplicados exactos
df = df.drop_duplicates()

# Eliminar duplicados por columna espec√≠fica, quedarse con el √∫ltimo
df = df.drop_duplicates(subset=['id'], keep='last')`
        },
        {
          level: 3,
          title: { es: '‚úÖ Soluci√≥n Completa', en: '‚úÖ Full Solution', pt: '‚úÖ Solu√ß√£o Completa' },
          content: { es: 'Ac√° est√° el c√≥digo completo:', en: 'Here is the full code:', pt: 'Aqui est√° o c√≥digo completo:' },
          code: `# Ver duplicados
duplicados = df_orders_clean.duplicated().sum()
print(f"Duplicados encontrados: {duplicados}")

# Ver duplicados por columna espec√≠fica (ej: order_id deber√≠a ser √∫nico)
duplicados_id = df_orders_clean.duplicated(subset=['order_id']).sum()
print(f"Order IDs duplicados: {duplicados_id}")

# Eliminar duplicados
df_orders_clean = df_orders_clean.drop_duplicates()

# Si hay IDs duplicados, quedarse con el m√°s reciente
df_orders_clean = df_orders_clean.sort_values('order_date').drop_duplicates(
    subset=['order_id'], 
    keep='last'
)`
        }
      ]
    },
    { 
      order: 6, 
      text: { es: 'üîç TRANSFORM - Correg√≠ tipos de datos', en: 'üîç TRANSFORM - Fix data types', pt: 'üîç TRANSFORM - Corrija tipos de dados' },
      code: `# Ver tipos actuales
print(df_orders_clean.dtypes)

# Convertir fechas
df_orders_clean['order_date'] = pd.to_datetime(df_orders_clean['order_date'])

# Asegurar que los n√∫meros sean num√©ricos
df_orders_clean['total'] = pd.to_numeric(df_orders_clean['total'], errors='coerce')
df_orders_clean['quantity'] = pd.to_numeric(df_orders_clean['quantity'], errors='coerce')

# Verificar
print("\\nTipos despu√©s de conversi√≥n:")
print(df_orders_clean.dtypes)`,
      explanation: { es: 'Pandas a veces infiere tipos incorrectamente. Fechas como strings, n√∫meros como objetos. Siempre verific√° y correg√≠.', en: 'Pandas sometimes infers types incorrectly. Dates as strings, numbers as objects. Always verify and fix.', pt: 'Pandas √†s vezes infere tipos incorretamente. Datas como strings, n√∫meros como objetos. Sempre verifique e corrija.' },
      warning: { es: 'errors="coerce" convierte valores inv√°lidos a NaN. Despu√©s ten√©s que manejar esos NaN.', en: 'errors="coerce" converts invalid values to NaN. Then you have to handle those NaNs.', pt: 'errors="coerce" converte valores inv√°lidos em NaN. Depois tem que tratar esses NaN.' }
    },
    { 
      order: 7, 
      text: { es: 'üìä TRANSFORM - Respond√© preguntas de negocio', en: 'üìä TRANSFORM - Answer business questions', pt: 'üìä TRANSFORM - Responda perguntas de neg√≥cio' },
      explanation: { es: `En este paso vas a responder preguntas reales que har√≠a un gerente de e-commerce. Us√° groupby + agg sobre estas columnas:
      
**Columnas clave para agrupar:**
- \`customer_id\`: Para an√°lisis por cliente
- \`product_id\`: Para an√°lisis por producto  
- \`order_date\` (convertida a mes): Para tendencias temporales
- \`status\`: Para an√°lisis de estados de √≥rdenes

**Columnas para agregar:**
- \`total_amount\`: Sumar para ingresos totales
- \`quantity\`: Sumar para unidades vendidas
- \`order_id\`: Contar para n√∫mero de √≥rdenes`, en: `In this step you answer real questions an e-commerce manager would ask. Use groupby + agg on these columns:
      
**Key grouping columns:**
- \`customer_id\`: For customer analysis
- \`product_id\`: For product analysis  
- \`order_date\` (converted to month): For temporal trends
- \`status\`: For order status analysis

**Aggregation columns:**
- \`total_amount\`: Sum for total revenue
- \`quantity\`: Sum for units sold
- \`order_id\`: Count for number of orders`, pt: `Neste passo vai responder perguntas reais que um gerente de e-commerce faria. Use groupby + agg sobre estas colunas:
      
**Colunas chave para agrupar:**
- \`customer_id\`: Para an√°lise por cliente
- \`product_id\`: Para an√°lise por produto  
- \`order_date\` (convertida a m√™s): Para tend√™ncias temporais
- \`status\`: Para an√°lise de status de pedidos

**Colunas para agregar:**
- \`total_amount\`: Somar para receitas totais
- \`quantity\`: Somar para unidades vendidas
- \`order_id\`: Contar para n√∫mero de pedidos` },
      challenge: { es: `Respond√© estas 3 preguntas de negocio:
1. ¬øCu√°les son los 5 clientes que m√°s gastaron?
2. ¬øCu√°l es el producto m√°s vendido (por cantidad)?
3. ¬øC√≥mo evolucionaron las ventas mes a mes?`, en: `Answer these 3 business questions:
1. Who are the top 5 spenders?
2. What is the best-selling product (by quantity)?
3. How did sales evolve month by month?`, pt: `Responda estas 3 perguntas de neg√≥cio:
1. Quais s√£o os 5 clientes que mais gastaram?
2. Qual √© o produto mais vendido (por quantidade)?
3. Como evolu√≠ram as vendas m√™s a m√™s?` },
      code: `# PREGUNTA 1: Top 5 clientes por gasto total
# Agrupamos por customer_id y sumamos total_amount
ventas_cliente = df_orders_clean.groupby('customer_id').agg({
  'total_amount': 'sum',
  'order_id': 'count'
}).rename(columns={'total_amount': 'total_gastado', 'order_id': 'cantidad_ordenes'})
ventas_cliente = ventas_cliente.sort_values('total_gastado', ascending=False)
print("üèÜ Top 5 clientes:")
print(ventas_cliente.head())

# PREGUNTA 2: Producto m√°s vendido
# Primero unimos orders con order_items para tener quantity
# Agrupamos por product_id y sumamos quantity
productos_vendidos = df_order_items.groupby('product_id')['quantity'].sum().sort_values(ascending=False)
print(f"\\nüì¶ Producto m√°s vendido: ID {productos_vendidos.idxmax()} ({productos_vendidos.max()} unidades)")

# PREGUNTA 3: Evoluci√≥n mensual de ventas
# Agrupamos por mes y sumamos total_amount
df_orders_clean['mes'] = df_orders_clean['order_date'].dt.to_period('M')
ventas_mes = df_orders_clean.groupby('mes')['total_amount'].sum().reset_index()
ventas_mes.columns = ['mes', 'total_ventas']
print("\\nüìà Ventas por mes:")
print(ventas_mes)`,
      tip: { es: 'Siempre renombr√° las columnas despu√©s de agregar para que sean descriptivas. Us√° sort_values() para ordenar resultados.', en: 'Always rename columns after aggregating so they are descriptive. Use sort_values() to order results.', pt: 'Sempre renomeie as colunas depois de agregar para que sejam descritivas. Use sort_values() para ordenar resultados.' }
    },
    { 
      order: 8, 
      text: { es: 'üíæ LOAD - Guard√° en CSV', en: 'üíæ LOAD - Save to CSV', pt: 'üíæ LOAD - Salve em CSV' },
      code: `# Crear carpeta output si no existe
import os
os.makedirs('output', exist_ok=True)

# Guardar m√©tricas en CSV
ventas_cliente.to_csv('output/ventas_por_cliente.csv', index=False)
ventas_mes.to_csv('output/ventas_por_mes.csv', index=False)

# Guardar datos limpios
df_orders_clean.to_csv('output/orders_clean.csv', index=False)

print("‚úÖ Archivos CSV guardados en output/")`,
      explanation: { es: 'CSV es el formato m√°s universal. Cualquiera puede abrirlo en Excel.', en: 'CSV is the most universal format. Anyone can open it in Excel.', pt: 'CSV √© o formato mais universal. Qualquer um pode abrir no Excel.' },
      tip: { es: 'Us√° index=False para no guardar el √≠ndice de Pandas como columna.', en: 'Use index=False to avoid saving Pandas index as column.', pt: 'Use index=False para n√£o salvar o √≠ndice do Pandas como coluna.' }
    },
    { 
      order: 9, 
      text: { es: 'üíæ LOAD - Guard√° en Parquet (formato profesional)', en: 'üíæ LOAD - Save to Parquet (professional format)', pt: 'üíæ LOAD - Salve em Parquet (formato profissional)' },
      code: `# Instalar pyarrow si no lo ten√©s: pip install pyarrow

# Guardar en Parquet
df_orders_clean.to_parquet('output/orders_clean.parquet', index=False)

# Comparar tama√±os
csv_size = os.path.getsize('output/orders_clean.csv') / 1024
parquet_size = os.path.getsize('output/orders_clean.parquet') / 1024

print(f"Tama√±o CSV: {csv_size:.1f} KB")
print(f"Tama√±o Parquet: {parquet_size:.1f} KB")
print(f"Parquet es {csv_size/parquet_size:.1f}x m√°s chico")`,
      explanation: { es: 'Parquet es columnar y comprimido. Para datasets grandes, es 10x m√°s r√°pido de leer que CSV.', en: 'Parquet is columnar and compressed. For large datasets, it\'s 10x faster to read than CSV.', pt: 'Parquet √© colunar e comprimido. Para datasets grandes, √© 10x mais r√°pido de ler que CSV.' },
      tip: { es: 'En producci√≥n, siempre us√° Parquet. CSV solo para compartir con no-t√©cnicos.', en: 'In production, always use Parquet. CSV only for sharing with non-techies.', pt: 'Em produ√ß√£o, sempre use Parquet. CSV s√≥ para compartilhar com n√£o-t√©cnicos.' }
    },
    { 
      order: 10, 
      text: { es: 'üìù Document√° tu trabajo', en: 'üìù Document your work', pt: 'üìù Documente seu trabalho' },
      code: `# Crear README.md

readme_content = """
# Mi Primer ETL con Python

## Descripci√≥n
Pipeline ETL que procesa datos de e-commerce para generar m√©tricas de ventas.

## C√≥mo correr
\`\`\`bash
pip install pandas pyarrow
python etl.py
\`\`\`

## Decisiones de limpieza
- **Nulos**: Elimin√© filas sin customer_id, product_id o total (campos cr√≠ticos)
- **Duplicados**: Elimin√© duplicados por order_id, qued√°ndome con el m√°s reciente
- **Tipos**: Convert√≠ order_date a datetime, total y quantity a num√©rico

## Output
- \`ventas_por_cliente.csv\`: Total gastado y cantidad de √≥rdenes por cliente
- \`ventas_por_mes.csv\`: Ventas totales por mes
- \`orders_clean.parquet\`: Dataset limpio en formato optimizado

## Autor
[Tu nombre] - [Fecha]
"""

with open('README.md', 'w') as f:
    f.write(readme_content)

print("‚úÖ README.md creado")`,
      explanation: { es: 'Un buen README es tu carta de presentaci√≥n. En entrevistas, van a mirar tu GitHub.', en: 'A good README is your cover letter. In interviews, they will look at your GitHub.', pt: 'Um bom README √© sua carta de apresenta√ß√£o. Em entrevistas, v√£o olhar seu GitHub.' },
      checkpoint: { es: '¬øTu README explica qu√© hace el proyecto, c√≥mo correrlo, y qu√© decisiones tomaste?', en: 'Does your README explain what the project does, how to run it, and what decisions you took?', pt: 'Seu README explica o que faz o projeto, como rodar e que decis√µes tomou?' }
    },
    { 
      order: 11, 
      text: { es: 'üöÄ Sub√≠ a GitHub', en: 'üöÄ Upload to GitHub', pt: 'üöÄ Suba para o GitHub' },
      code: `# En terminal:
git init
git add .
git commit -m "Mi primer ETL con Python"

# Crear repo en GitHub y conectar
git remote add origin https://github.com/TU_USUARIO/mi-primer-etl.git
git push -u origin main`,
      explanation: { es: 'Tu portfolio en GitHub es fundamental para conseguir trabajo. Cada proyecto cuenta.', en: 'Your GitHub portfolio is fundamental to get a job. Every project counts.', pt: 'Seu portfolio no GitHub √© fundamental para conseguir emprego. Cada projeto conta.' },
      tip: { es: 'Agreg√° un .gitignore para no subir los datos (pueden ser grandes o sensibles).', en: 'Add a .gitignore to not upload data (can be large or sensitive).', pt: 'Adicione um .gitignore para n√£o subir os dados (podem ser grandes ou sens√≠veis).' }
    },
  ],
  deliverable: { es: 'Repositorio en GitHub con: etl.py, requirements.txt, README.md, carpeta output/ con resultados', en: 'GitHub repository with: etl.py, requirements.txt, README.md, output/ folder with results', pt: 'Reposit√≥rio no GitHub com: etl.py, requirements.txt, README.md, pasta output/ com resultados' },
  evaluation: [
    { es: '¬øEl script corre sin errores con python etl.py?', en: 'Does the script run without errors with python etl.py?', pt: 'O script roda sem erros com python etl.py?' },
    { es: '¬øDocumentaste c√≥mo manejaste los nulos y por qu√©?', en: 'Did you document how you handled nulls and why?', pt: 'Documentou como tratou os nulos e por qu√™?' },
    { es: '¬øLas m√©tricas calculadas son correctas? (verific√° manualmente 2-3)', en: 'Are calculated metrics correct? (manually verify 2-3)', pt: 'As m√©tricas calculadas est√£o corretas? (verifique manualmente 2-3)' },
    { es: '¬øEl c√≥digo tiene comentarios explicando cada paso?', en: 'Does the code have comments explaining each step?', pt: 'O c√≥digo tem coment√°rios explicando cada passo?' },
    { es: '¬øEl README explica c√≥mo correr el proyecto?', en: 'Does the README explain how to run the project?', pt: 'O README explica como rodar o projeto?' },
    { es: '¬øGuardaste en Parquet adem√°s de CSV?', en: 'Did you save in Parquet besides CSV?', pt: 'Salvou em Parquet al√©m de CSV?' },
  ],
  codeExample: `# etl.py - Pipeline ETL Completo
import pandas as pd
import os
from datetime import datetime

def extract(data_dir: str = 'data') -> dict:
    """Extrae datos de los archivos CSV."""
    print("üì• EXTRACT: Cargando datos...")
    
    tables = {}
    csv_files = {
        'orders': 'ecommerce_orders.csv',
        'order_items': 'ecommerce_order_items.csv',
        'customers': 'ecommerce_customers.csv',
        'products': 'ecommerce_products.csv',
        'categories': 'ecommerce_categories.csv',
    }
    
    for table_name, filename in csv_files.items():
        filepath = os.path.join(data_dir, filename)
        if os.path.exists(filepath):
            tables[table_name] = pd.read_csv(filepath)
            print(f"   {table_name}: {len(tables[table_name])} filas")
        else:
            print(f"   ‚ö†Ô∏è {filename} no encontrado")
    
    return tables

def transform(tables: dict) -> pd.DataFrame:
    """Limpia y transforma los datos."""
    print("\\nüîÑ TRANSFORM: Limpiando datos...")
    df = tables['orders'].copy()
    
    # 1. Manejar nulos
    antes = len(df)
    df = df.dropna(subset=['customer_id', 'total_amount'])
    print(f"   Filas eliminadas por nulos: {antes - len(df)}")
    
    # 2. Eliminar duplicados
    antes = len(df)
    df = df.drop_duplicates(subset=['order_id'], keep='last')
    print(f"   Duplicados eliminados: {antes - len(df)}")
    
    # 3. Corregir tipos
    df['order_date'] = pd.to_datetime(df['order_date'])
    df['total_amount'] = pd.to_numeric(df['total_amount'], errors='coerce')
    
    # 4. Agregar campos calculados
    df['order_month'] = df['order_date'].dt.to_period('M').astype(str)
    df['is_high_value'] = df['total_amount'] > 100
    
    print(f"   Filas finales: {len(df)}")
    return df

def load(df: pd.DataFrame, output_dir: str = 'output'):
    """Guarda los resultados."""
    print(f"\\nüíæ LOAD: Guardando en {output_dir}/...")
    os.makedirs(output_dir, exist_ok=True)
    
    # Datos limpios
    df.to_csv(f'{output_dir}/orders_clean.csv', index=False)
    df.to_parquet(f'{output_dir}/orders_clean.parquet', index=False)
    
    # M√©tricas
    ventas_cliente = df.groupby('customer_id')['total_amount'].sum().reset_index()
    ventas_cliente.to_csv(f'{output_dir}/ventas_por_cliente.csv', index=False)
    
    ventas_mes = df.groupby('order_month')['total_amount'].sum().reset_index()
    ventas_mes.to_csv(f'{output_dir}/ventas_por_mes.csv', index=False)
    
    print("   ‚úÖ Archivos guardados")

def main():
    print("=" * 50)
    print("ETL Pipeline - E-commerce Data")
    print("=" * 50)
    
    tables = extract('data')
    df_clean = transform(tables)
    load(df_clean)
    
    print("\\n‚úÖ ETL completado exitosamente!")

if __name__ == "__main__":
    main()`,
  theory: { es: `## ¬øQu√© es ETL?

**ETL** significa Extract, Transform, Load. Es el patr√≥n fundamental de Data Engineering:

### 1. Extract (Extraer)
Obtener datos de una fuente:
- Archivos (CSV, JSON, Parquet)
- APIs REST
- Bases de datos (PostgreSQL, MySQL)
- Servicios cloud (S3, GCS)

### 2. Transform (Transformar)
Limpiar y preparar los datos:
- Manejar valores nulos
- Eliminar duplicados
- Corregir tipos de datos
- Calcular m√©tricas
- Enriquecer con datos adicionales

### 3. Load (Cargar)
Guardar en el destino:
- Data Warehouse (Snowflake, BigQuery)
- Data Lake (S3, GCS)
- Archivos (Parquet, CSV)

## ¬øPor qu√© Pandas?

Pandas es la librer√≠a est√°ndar para manipulaci√≥n de datos en Python:
- **DataFrames**: Tablas en memoria, f√°ciles de manipular
- **Operaciones vectorizadas**: R√°pidas sin loops expl√≠citos
- **Integraci√≥n**: Lee/escribe SQL, Parquet, JSON, Excel, CSV

## Formatos de Archivo

| Formato | Pros | Contras | Usar cuando |
|---------|------|---------|-------------|
| CSV | Universal, legible | Lento, sin tipos | Compartir con no-t√©cnicos |
| JSON | Flexible, estructuras anidadas | Verboso | APIs, configs |
| Parquet | R√°pido, comprimido, tipado | Binario | Producci√≥n, analytics |

**Regla pr√°ctica**: Us√° Parquet para guardar datos procesados. Es 10x m√°s r√°pido que CSV.`, en: `## What is ETL?

**ETL** stands for Extract, Transform, Load. It is the fundamental pattern of Data Engineering:

### 1. Extract
Get data from a source:
- Files (CSV, JSON, Parquet)
- REST APIs
- Databases (PostgreSQL, MySQL)
- Cloud services (S3, GCS)

### 2. Transform
Clean and prepare data:
- Handle null values
- Remove duplicates
- Fix data types
- Calculate metrics
- Enrich with additional data

### 3. Load
Save to destination:
- Data Warehouse (Snowflake, BigQuery)
- Data Lake (S3, GCS)
- Files (Parquet, CSV)

## Why Pandas?

Pandas is the standard library for data manipulation in Python:
- **DataFrames**: In-memory tables, easy to manipulate
- **Vectorized operations**: Fast without explicit loops
- **Integration**: Reads/writes SQL, Parquet, JSON, Excel, CSV

## File Formats

| Format | Pros | Cons | Use when |
|--------|------|------|----------|
| CSV | Universal, readable | Slow, untyped | Share with non-techies |
| JSON | Flexible, nested structures | Verbose | APIs, configs |
| Parquet | Fast, compressed, typed | Binary | Production, analytics |

**Rule of thumb**: Use Parquet to save processed data. It's 10x faster than CSV.`, pt: `## O que √© ETL?

**ETL** significa Extract, Transform, Load. √â o padr√£o fundamental de Data Engineering:

### 1. Extract (Extrair)
Obter dados de uma fonte:
- Arquivos (CSV, JSON, Parquet)
- APIs REST
- Bancos de dados (PostgreSQL, MySQL)
- Servi√ßos cloud (S3, GCS)

### 2. Transform (Transformar)
Limpar e preparar os dados:
- Lidar com valores nulos
- Remover duplicatas
- Corrigir tipos de dados
- Calcular m√©tricas
- Enriquecer com dados adicionais

### 3. Load (Carregar)
Salvar no destino:
- Data Warehouse (Snowflake, BigQuery)
- Data Lake (S3, GCS)
- Arquivos (Parquet, CSV)

## Por que Pandas?

Pandas √© a biblioteca padr√£o para manipula√ß√£o de dados em Python:
- **DataFrames**: Tabelas em mem√≥ria, f√°ceis de manipular
- **Opera√ß√µes vetorizadas**: R√°pidas sem loops expl√≠citos
- **Integra√ß√£o**: L√™/escreve SQL, Parquet, JSON, Excel, CSV

## Formatos de Arquivo

| Formato | Pros | Contras | Usar quando |
|---------|------|---------|-------------|
| CSV | Universal, leg√≠vel | Lento, sem tipos | Compartilhar com n√£o-t√©cnicos |
| JSON | Flex√≠vel, estruturas aninhadas | Verboso | APIs, configs |
| Parquet | R√°pido, comprimido, tipado | Bin√°rio | Produ√ß√£o, analytics |

**Regra pr√°tica**: Use Parquet para salvar dados processados. √â 10x mais r√°pido que CSV.` },
  nextSteps: [
    { es: 'Hac√© el proyecto "An√°lisis SQL de Logs" para practicar SQL', en: 'Do "SQL Logs Analysis" project to practice SQL', pt: 'Fa√ßa o projeto "An√°lise SQL de Logs" para praticar SQL' },
    { es: 'Agreg√° m√°s m√©tricas: ticket promedio, clientes recurrentes', en: 'Add more metrics: average ticket, recurring customers', pt: 'Adicione mais m√©tricas: ticket m√©dio, clientes recorrentes' },
    { es: 'Conect√° con una base de datos real en vez de archivos', en: 'Connect to a real database instead of files', pt: 'Conecte com um banco de dados real em vez de arquivos' },
  ],
  resources: [
    { title: { es: 'Pandas en 10 minutos', en: 'Pandas in 10 minutes', pt: 'Pandas em 10 minutos' }, url: 'https://pandas.pydata.org/docs/user_guide/10min.html', type: 'docs' },
    { title: { es: 'ETL vs ELT', en: 'ETL vs ELT', pt: 'ETL vs ELT' }, url: 'https://www.fivetran.com/blog/etl-vs-elt', type: 'article' },
  ],
};


