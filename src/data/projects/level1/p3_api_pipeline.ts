import { Project } from '../../../types/members';

export const p3_api_pipeline: Project = {
  id: 'p3-api-pipeline',
  level: 1,
  title: { es: 'Pipeline con API REST', en: 'REST API Pipeline', pt: 'Pipeline com API REST' },
  description: {
    es: 'Constru√≠ un pipeline que consume una API REST, transforma los datos, y los guarda en formato optimizado. Este patr√≥n es el m√°s com√∫n en Data Engineering moderno.',
    en: 'Build a pipeline that consumes a REST API, transforms the data, and saves it in an optimized format. This is the most common pattern in modern Data Engineering.',
    pt: 'Construa um pipeline que consome uma API REST, transforma os dados e os salva em formato otimizado. Este √© o padr√£o mais comum em Data Engineering moderno.'
  },
  difficulty: 'Intermedio',
  duration: '3-4 horas',
  skills: [
    { es: 'Python' }, { es: 'APIs REST' }, { es: 'requests' }, 
    { es: 'Error Handling', en: 'Error Handling', pt: 'Tratamento de Erros' }, 
    { es: 'Logging' }, { es: 'Parquet' }
  ],
  icon: 'üåê',
  color: 'purple',
  datasetId: 'ecommerce',
  estimatedLines: 120,
  realWorldExample: {
    es: 'As√≠ es como Airbnb consume datos de APIs externas para enriquecer sus listings',
    en: 'This is how Airbnb consumes data from external APIs to enrich their listings',
    pt: '√â assim que o Airbnb consome dados de APIs externas para enriquecer seus an√∫ncios'
  },
  usedBy: ['Airbnb', 'Stripe', 'Twilio', 'Plaid'],
  learningObjectives: [
    { es: 'Consumir APIs REST con Python', en: 'Consume REST APIs with Python', pt: 'Consumir APIs REST com Python' },
    { es: 'Manejar errores de red (timeouts, reintentos)', en: 'Handle network errors (timeouts, retries)', pt: 'Lidar com erros de rede (timeouts, re tentativas)' },
    { es: 'Implementar logging profesional', en: 'Implement professional logging', pt: 'Implementar logging profissional' },
    { es: 'Usar variables de entorno para secrets', en: 'Use environment variables for secrets', pt: 'Usar vari√°veis de ambiente para segredos' },
    { es: 'Guardar datos particionados', en: 'Save partitioned data', pt: 'Salvar dados particionados' },
  ],
  commonMistakes: [
    {
      mistake: { es: 'Hardcodear tokens en el c√≥digo', en: 'Hardcoding tokens in code', pt: 'Hardcodar tokens no c√≥digo' },
      why: { es: 'Si sub√≠s el c√≥digo a GitHub, cualquiera puede ver tu token', en: 'If you push code to GitHub, anyone can see your token', pt: 'Se subir o c√≥digo para o GitHub, qualquer um pode ver seu token' },
      solution: { es: 'Us√° variables de entorno o archivos .env', en: 'Use environment variables or .env files', pt: 'Use vari√°veis de ambiente ou arquivos .env' },
      code: `# ‚ùå NUNCA hagas esto
API_TOKEN = "sk_live_abc123..."

# ‚úÖ Hac√© esto
import os
API_TOKEN = os.environ.get('API_TOKEN')`
    },
    {
      mistake: { es: 'No manejar errores de conexi√≥n', en: 'Not handling connection errors', pt: 'N√£o lidar com erros de conex√£o' },
      why: { es: 'Las APIs fallan: timeouts, rate limits, errores 500', en: 'APIs fail: timeouts, rate limits, 500 errors', pt: 'APIs falham: timeouts, rate limits, erros 500' },
      solution: { es: 'Siempre us√° try/except y reintentos', en: 'Always use try/except and retries', pt: 'Sempre use try/except e re tentativas' },
      code: `import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

session = requests.Session()
retries = Retry(total=3, backoff_factor=1)
session.mount('https://', HTTPAdapter(max_retries=retries))`
    },
    {
      mistake: { es: 'No implementar reintentos', en: 'Not implementing retries', pt: 'N√£o implementar re tentativas' },
      why: { es: 'Un error temporal no deber√≠a fallar todo el pipeline', en: 'A temporary error shouldn\'t fail the whole pipeline', pt: 'Um erro tempor√°rio n√£o deveria falhar todo o pipeline' },
      solution: { es: 'Us√° exponential backoff: 1s, 2s, 4s...', en: 'Use exponential backoff: 1s, 2s, 4s...', pt: 'Use exponential backoff: 1s, 2s, 4s...' },
    },
    {
      mistake: { es: 'Olvidar timeouts en requests', en: 'Forgetting timeouts in requests', pt: 'Esquecer timeouts em requests' },
      why: { es: 'Sin timeout, tu script puede quedarse colgado para siempre', en: 'Without timeout, your script can hang forever', pt: 'Sem timeout, seu script pode ficar travado para sempre' },
      solution: { es: 'Siempre us√° timeout=(connect, read)', en: 'Always use timeout=(connect, read)', pt: 'Sempre use timeout=(connect, read)' },
      code: `response = requests.get(url, timeout=(5, 30))  # 5s connect, 30s read`
    },
  ],
  expectedOutputs: [
    {
      step: 4,
      description: { es: 'Response exitosa de la API', en: 'Successful API Response', pt: 'Resposta de sucesso da API' },
      example: `2024-01-15 10:30:45 INFO - Fetching page 1...
2024-01-15 10:30:46 INFO - Got 100 records
2024-01-15 10:30:46 INFO - Fetching page 2...
2024-01-15 10:30:47 INFO - Got 100 records
...
2024-01-15 10:30:52 INFO - Total: 500 records fetched`
    },
    {
      step: 7,
      description: { es: 'Archivos guardados particionados', en: 'Partitioned saved files', pt: 'Arquivos salvos particionados' },
      example: `output/
‚îú‚îÄ‚îÄ 2024-01-15/
‚îÇ   ‚îî‚îÄ‚îÄ data.parquet (150 KB)
‚îú‚îÄ‚îÄ 2024-01-16/
‚îÇ   ‚îî‚îÄ‚îÄ data.parquet (145 KB)
‚îî‚îÄ‚îÄ 2024-01-17/
    ‚îî‚îÄ‚îÄ data.parquet (160 KB)`
    },
  ],
  interviewStory: {
    hook: { es: "Constru√≠ un pipeline que consume datos de APIs externas con manejo robusto de errores, y logr√© 99.9% de uptime procesando 50,000 requests diarios.", en: "Built a pipeline consuming external API data with robust error handling, achieving 99.9% uptime processing 50,000 daily requests.", pt: "Constru√≠ um pipeline que consome dados de APIs externas com tratamento robusto de erros, e consegui 99.9% de uptime processando 50.000 requests di√°rios." },
    situation: { es: "Necesitaba integrar datos de una API externa para enriquecer nuestro dataset de e-commerce. El problema era que la API ten√≠a rate limits, fallaba ocasionalmente, y necesit√°bamos datos frescos cada d√≠a.", en: "Needed to integrate external API data to enrich our e-commerce dataset. The API had rate limits, failed occasionally, and we needed fresh data daily.", pt: "Precisava integrar dados de uma API externa para enriquecer nosso dataset de e-commerce. O problema era que a API tinha rate limits, falhava ocasionalmente, e precis√°vamos de dados frescos a cada dia." },
    task: { es: "Construir un pipeline robusto que consumiera la API, manejara todos los casos de error, y guardara los datos de forma particionada para f√°cil acceso.", en: "Build a robust pipeline to consume the API, handle all error cases, and save data partitioned for easy access.", pt: "Construir um pipeline robusto que consumisse a API, tratasse todos os casos de erro e salvasse os dados de forma particionada para f√°cil acesso." },
    actions: [
      { es: "Implement√© autenticaci√≥n segura usando variables de entorno - nunca hardcode√© tokens", en: "Implemented secure auth using env vars - never hardcoded tokens", pt: "Implementei autentica√ß√£o segura usando vari√°veis de ambiente - nunca hardcodei tokens" },
      { es: "Us√© la librer√≠a requests con retry autom√°tico y exponential backoff", en: "Used requests library with automatic retry and exponential backoff", pt: "Usei a biblioteca requests com retry autom√°tico e exponential backoff" },
      { es: "Agregu√© logging profesional para debuggear problemas en producci√≥n", en: "Added professional logging to debug production issues", pt: "Adicionei logging profissional para debuggar problemas em produ√ß√£o" },
      { es: "Implement√© timeouts para evitar que el script se colgara", en: "Implemented timeouts to prevent script hanging", pt: "Implementei timeouts para evitar que o script travasse" },
      { es: "Particion√© los datos por fecha para queries eficientes", en: "Partitioned data by date for efficient queries", pt: "Particionei os dados por data para queries eficientes" }
    ],
    results: [
      { es: "Pipeline con 99.9% de uptime - solo 1 falla en 3 meses", en: "Pipeline with 99.9% uptime - only 1 failure in 3 months", pt: "Pipeline com 99.9% de uptime - s√≥ 1 falha em 3 meses" },
      { es: "Procesamos 50,000 requests diarios sin problemas de rate limit", en: "Processed 50,000 daily requests without rate limit issues", pt: "Processamos 50.000 requests di√°rios sem problemas de rate limit" },
      { es: "Tiempo de recuperaci√≥n de errores: autom√°tico en <1 minuto", en: "Error recovery time: automatic in <1 minute", pt: "Tempo de recupera√ß√£o de erros: autom√°tico em <1 minuto" },
      { es: "Datos disponibles para el equipo de analytics cada ma√±ana a las 6am", en: "Data available for analytics team every morning at 6am", pt: "Dados dispon√≠veis para a equipe de analytics toda manh√£ √†s 6am" }
    ],
    learnings: [
      { es: "El manejo de errores es el 80% del c√≥digo de producci√≥n - el happy path es solo el 20%", en: "Error handling is 80% of production code - happy path is only 20%", pt: "O tratamento de erros √© 80% do c√≥digo de produ√ß√£o - o happy path √© s√≥ 20%" },
      { es: "Exponential backoff es esencial - sin √©l satur√°s la API cuando hay problemas", en: "Exponential backoff is essential - without it you saturate the API when there are issues", pt: "Exponential backoff √© essencial - sem ele voc√™ satura a API quando h√° problemas" },
      { es: "Logging estructurado (JSON) hace debugging 10x m√°s f√°cil que print statements", en: "Structured logging (JSON) makes debugging 10x easier than print statements", pt: "Logging estruturado (JSON) torna o debugging 10x mais f√°cil que print statements" }
    ],
    possibleQuestions: [
      {
        question: { es: "¬øC√≥mo manejaste los rate limits?", en: "How did you handle rate limits?", pt: "Como lidou com os rate limits?" },
        answer: { es: "Implement√© exponential backoff con jitter: si recib√≠a 429, esperaba 1s, luego 2s, luego 4s, con un random de ¬±20% para evitar thundering herd. Tambi√©n distribu√≠ las requests a lo largo del d√≠a.", en: "Implemented exponential backoff with jitter: if 429 received, wait 1s, then 2s, then 4s, with ¬±20% random to avoid thundering herd. Also distributed requests throughout the day.", pt: "Implementei exponential backoff com jitter: se recebia 429, esperava 1s, depois 2s, depois 4s, com um random de ¬±20% para evitar thundering herd. Tamb√©m distribu√≠ as requests ao longo do dia." }
      },
      {
        question: { es: "¬øQu√© pasa si la API est√° ca√≠da por horas?", en: "What if API is down for hours?", pt: "O que acontece se a API cair por horas?" },
        answer: { es: "Tengo un circuit breaker: despu√©s de 5 fallos consecutivos, el pipeline se pausa y env√≠a una alerta. Cuando la API vuelve, retoma desde donde qued√≥ gracias a checkpoints.", en: "I have a circuit breaker: after 5 consecutive failures, pipeline pauses and sends alert. When API returns, resumes from checkpoint.", pt: "Tenho um circuit breaker: depois de 5 falhas consecutivas, o pipeline pausa e envia um alerta. Quando a API volta, retoma de onde parou gra√ßas a checkpoints." }
      },
      {
        question: { es: "¬øC√≥mo asegur√°s que los tokens no se filtren?", en: "How do you ensure tokens don't leak?", pt: "Como garante que os tokens n√£o vazem?" },
        answer: { es: "Tres capas: 1) Variables de entorno, nunca en c√≥digo, 2) .gitignore para .env, 3) Secrets scanning en CI/CD que bloquea commits con tokens.", en: "Three layers: 1) Env vars, never in code, 2) .gitignore for .env, 3) Secrets scanning in CI/CD blocking commits with tokens.", pt: "Tr√™s camadas: 1) Vari√°veis de ambiente, nunca no c√≥digo, 2) .gitignore para .env, 3) Secrets scanning em CI/CD que bloqueia commits com tokens." }
      }
    ],
    closingStatement: { es: "Este proyecto me ense√±√≥ que integrar con APIs externas requiere pensar en todo lo que puede fallar - y va a fallar.", en: "This project taught me integrating with external APIs requires thinking about everything that can fail - and it will fail.", pt: "Este projeto me ensinou que integrar com APIs externas requer pensar em tudo o que pode falhar - e vai falhar." }
  },
  prerequisites: ['p1-etl-python'],
  steps: [
    { 
      order: 1, 
      text: { es: 'üîë Gener√° tu API Token', en: 'üîë Generate your API Token', pt: 'üîë Gere seu Token de API' },
      explanation: { es: 'And√° a la secci√≥n "API Token" en el Dashboard de la plataforma (esquina superior derecha de /members, click en tu avatar ‚Üí Dashboard). Gener√° un token y guardalo de forma segura - no lo vas a poder ver de nuevo.', en: 'Go to "API Token" section in Dashboard (top right of /members, click avatar ‚Üí Dashboard). Generate token and save securely - you won\'t see it again.', pt: 'V√° na se√ß√£o "API Token" no Dashboard da plataforma (canto superior direito de /members, clique no seu avatar ‚Üí Dashboard). Gere um token e guarde de forma segura - n√£o vai poder ver de novo.' },
      tip: { es: 'El token se genera desde el Dashboard de la plataforma. Guardalo en un archivo .env antes de continuar.', en: 'Token is generated from Dashboard. Save in .env file before continuing.', pt: 'O token √© gerado no Dashboard. Guarde em um arquivo .env antes de continuar.' },
      warning: { es: 'NUNCA pongas tokens en el c√≥digo. Siempre us√° variables de entorno.', en: 'NEVER put tokens in code. Always use env vars.', pt: 'NUNCA coloque tokens no c√≥digo. Sempre use vari√°veis de ambiente.' },
      checkpoint: { es: '¬øTen√©s tu token generado y guardado en .env?', en: 'Have you generated and saved your token in .env?', pt: 'Tem seu token gerado e guardado no .env?' }
    },
    { 
      order: 2, 
      text: { es: 'üìÇ Cre√° la estructura del proyecto', en: 'üìÇ Create project structure', pt: 'üìÇ Crie a estrutura do projeto' },
      code: `mkdir api-pipeline
cd api-pipeline

# Crear archivos
touch main.py config.py .env .gitignore requirements.txt
mkdir output

# .gitignore - MUY IMPORTANTE
echo ".env
output/
__pycache__/
*.pyc
.DS_Store" > .gitignore`,
      explanation: { es: 'El .gitignore evita que subas secrets o datos a GitHub.', en: '.gitignore prevents uploading secrets or data to GitHub.', pt: 'O .gitignore evita que suba segredos ou dados para o GitHub.' },
      tip: { es: 'Siempre cre√° el .gitignore ANTES de hacer git init.', en: 'Always create .gitignore BEFORE git init.', pt: 'Sempre crie o .gitignore ANTES de fazer git init.' }
    },
    { 
      order: 3, 
      text: { es: 'üì¶ Instal√° dependencias', en: 'üì¶ Install dependencies', pt: 'üì¶ Instale depend√™ncias' },
      code: `# requirements.txt
requests>=2.28.0
pandas>=1.5.0
pyarrow>=10.0.0
python-dotenv>=0.21.0

# Instalar
pip install -r requirements.txt`,
      explanation: { es: 'Siempre us√° requirements.txt con versiones para reproducibilidad.', en: 'Always use requirements.txt with versions for reproducibility.', pt: 'Sempre use requirements.txt com vers√µes para reprodutibilidade.' }
    },
    { 
      order: 4, 
      text: { es: 'üîß Configur√° variables de entorno', en: 'üîß Configure environment variables', pt: 'üîß Configure vari√°veis de ambiente' },
      code: `# .env (NUNCA subir a git)
API_TOKEN=tu_token_aqui
API_BASE_URL=https://iansaura.com/api

# config.py
import os
from dotenv import load_dotenv

load_dotenv()

API_TOKEN = os.getenv('API_TOKEN')
API_BASE_URL = os.getenv('API_BASE_URL', 'https://iansaura.com/api')

if not API_TOKEN:
    raise ValueError("API_TOKEN no configurado. Cre√° un archivo .env")`,
      explanation: { es: 'python-dotenv carga variables del archivo .env autom√°ticamente.', en: 'python-dotenv loads .env variables automatically.', pt: 'python-dotenv carrega vari√°veis do arquivo .env automaticamente.' },
      tip: { es: 'El segundo par√°metro de getenv es el valor por defecto si no existe.', en: 'Second parameter of getenv is default value if not exists.', pt: 'O segundo par√¢metro de getenv √© o valor padr√£o se n√£o existir.' }
    },
    { 
      order: 5, 
      text: { es: 'üì• EXTRACT: Implement√° la llamada a la API', en: 'üì• EXTRACT: Implement API call', pt: 'üì• EXTRACT: Implemente a chamada √† API' },
      code: `# main.py
import requests
import logging
from config import API_TOKEN, API_BASE_URL

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def fetch_data(dataset_type: str = 'ecommerce', rows: int = 1000) -> dict:
    """Obtiene datos de la API."""
    url = f"{API_BASE_URL}/datasets.php"
    params = {
        'type': dataset_type,
        'rows': rows,
        'token': API_TOKEN
    }
    
    logger.info(f"Fetching {rows} rows of {dataset_type} data...")
    
    response = requests.get(url, params=params, timeout=30)
    response.raise_for_status()  # Lanza excepci√≥n si hay error HTTP
    
    data = response.json()
    logger.info(f"Received {len(data.get('tables', {}).get('orders', []))} orders")
    
    return data`,
      explanation: { es: 'raise_for_status() lanza una excepci√≥n si el status code es 4xx o 5xx.', en: 'raise_for_status() raises exception if status code is 4xx or 5xx.', pt: 'raise_for_status() lan√ßa exce√ß√£o se o status code for 4xx ou 5xx.' },
      warning: { es: 'Siempre us√° timeout. Sin timeout, tu script puede quedarse colgado indefinidamente.', en: 'Always use timeout. Without it, script can hang indefinitely.', pt: 'Sempre use timeout. Sem timeout, seu script pode ficar travado indefinidamente.' }
    },
    { 
      order: 6, 
      text: { es: '‚ö†Ô∏è Implement√° manejo de errores', en: '‚ö†Ô∏è Implement error handling', pt: '‚ö†Ô∏è Implemente tratamento de erros' },
      code: `import time
from requests.exceptions import RequestException, Timeout, HTTPError

def fetch_data_with_retry(
    dataset_type: str = 'ecommerce', 
    rows: int = 1000,
    max_retries: int = 3,
    backoff_factor: float = 2.0
) -> dict:
    """Obtiene datos con reintentos autom√°ticos."""
    
    for attempt in range(max_retries):
        try:
            return fetch_data(dataset_type, rows)
            
        except Timeout:
            logger.warning(f"Timeout en intento {attempt + 1}/{max_retries}")
            
        except HTTPError as e:
            if e.response.status_code >= 500:
                logger.warning(f"Error del servidor: {e}")
            else:
                # Errores 4xx no se reintentan
                logger.error(f"Error del cliente: {e}")
                raise
                
        except RequestException as e:
            logger.warning(f"Error de conexi√≥n: {e}")
        
        if attempt < max_retries - 1:
            wait_time = backoff_factor ** attempt
            logger.info(f"Reintentando en {wait_time} segundos...")
            time.sleep(wait_time)
    
    raise Exception(f"Fall√≥ despu√©s de {max_retries} intentos")`,
      explanation: { es: 'Exponential backoff: cada reintento espera m√°s tiempo (2s, 4s, 8s). Evita sobrecargar el servidor.', en: 'Exponential backoff: each retry waits longer (2s, 4s, 8s). Avoids overloading server.', pt: 'Exponential backoff: cada re tentativa espera mais tempo (2s, 4s, 8s). Evita sobrecarregar o servidor.' },
      tip: { es: 'Los errores 4xx (cliente) no se reintentan porque el problema est√° en tu request, no en el servidor.', en: '4xx errors (client) are not retried because problem is in your request, not server.', pt: 'Os erros 4xx (cliente) n√£o s√£o re tentados porque o problema est√° na sua request, n√£o no servidor.' }
    },
    { 
      order: 7, 
      text: { es: 'üìä TRANSFORM: Proces√° los datos', en: 'üìä TRANSFORM: Process data', pt: 'üìä TRANSFORM: Processe os dados' },
      code: `import pandas as pd

def transform_data(raw_data: dict) -> pd.DataFrame:
    """Transforma y enriquece los datos."""
    logger.info("Transformando datos...")
    
    # Extraer tabla de orders
    orders = raw_data.get('tables', {}).get('orders', [])
    df = pd.DataFrame(orders)
    
    if df.empty:
        logger.warning("No hay datos para procesar")
        return df
    
    # Convertir tipos
    df['order_date'] = pd.to_datetime(df['order_date'])
    df['total'] = pd.to_numeric(df['total'], errors='coerce')
    
    # Agregar campos calculados
    df['order_month'] = df['order_date'].dt.to_period('M').astype(str)
    df['order_year'] = df['order_date'].dt.year
    df['is_high_value'] = df['total'] > 100
    df['day_of_week'] = df['order_date'].dt.day_name()
    
    # Validaciones
    invalid_totals = df['total'].isna().sum()
    if invalid_totals > 0:
        logger.warning(f"{invalid_totals} √≥rdenes con total inv√°lido")
    
    logger.info(f"Transformadas {len(df)} √≥rdenes")
    return df`,
      explanation: { es: 'Siempre valid√° los datos despu√©s de transformar. Logue√° anomal√≠as.', en: 'Always validate data after transforming. Log anomalies.', pt: 'Sempre valide os dados depois de transformar. Logue anomalias.' },
      checkpoint: { es: '¬øTu funci√≥n maneja el caso de datos vac√≠os?', en: 'Does your function handle empty data case?', pt: 'Sua fun√ß√£o lida com o caso de dados vazios?' }
    },
    { 
      order: 8, 
      text: { es: 'üíæ LOAD: Guard√° particionado', en: 'üíæ LOAD: Save partitioned', pt: 'üíæ LOAD: Salve particionado' },
      code: `import os

def save_data(df: pd.DataFrame, output_dir: str = 'output'):
    """Guarda datos particionados por mes."""
    logger.info(f"Guardando datos en {output_dir}/...")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Guardar particionado por mes
    df.to_parquet(
        f'{output_dir}/orders',
        partition_cols=['order_year', 'order_month'],
        index=False
    )
    
    # Tambi√©n guardar un archivo consolidado
    df.to_parquet(f'{output_dir}/orders_all.parquet', index=False)
    
    # Estad√≠sticas
    logger.info(f"Guardadas {len(df)} √≥rdenes")
    logger.info(f"Particiones: {df['order_month'].nunique()} meses")`,
      explanation: { es: 'Particionar por fecha hace que las queries sean m√°s r√°pidas: solo lee los meses que necesit√°s.', en: 'Partitioning by date makes queries faster: only reads months you need.', pt: 'Particionar por data faz as queries serem mais r√°pidas: s√≥ l√™ os meses que voc√™ precisa.' },
      tip: { es: 'La estructura ser√°: output/orders/order_year=2024/order_month=2024-01/data.parquet', en: 'Structure will be: output/orders/order_year=2024/order_month=2024-01/data.parquet', pt: 'A estrutura ser√°: output/orders/order_year=2024/order_month=2024-01/data.parquet' }
    },
    { 
      order: 9, 
      text: { es: 'üöÄ Arm√° el pipeline completo', en: 'üöÄ Assemble full pipeline', pt: 'üöÄ Monte o pipeline completo' },
      code: `def main():
    """Pipeline principal."""
    logger.info("=" * 50)
    logger.info("API Pipeline - Iniciando")
    logger.info("=" * 50)
    
    try:
        # Extract
        raw_data = fetch_data_with_retry(rows=5000)
        
        # Transform
        df = transform_data(raw_data)
        
        if df.empty:
            logger.error("No hay datos para guardar")
            return
        
        # Load
        save_data(df)
        
        logger.info("=" * 50)
        logger.info("Pipeline completado exitosamente!")
        logger.info("=" * 50)
        
    except Exception as e:
        logger.error(f"Pipeline fall√≥: {e}")
        raise

if __name__ == "__main__":
    main()`,
      explanation: { es: 'El try/except en main() asegura que cualquier error se loguee antes de fallar.', en: 'try/except in main() ensures any error is logged before failing.', pt: 'O try/except em main() garante que qualquer erro seja logado antes de falhar.' },
      checkpoint: { es: '¬øTu pipeline corre sin errores con python main.py?', en: 'Does your pipeline run without errors with python main.py?', pt: 'Seu pipeline roda sem erros com python main.py?' }
    },
    { 
      order: 10, 
      text: { es: 'üìù Cre√° el README', en: 'üìù Create README', pt: 'üìù Crie o README' },
      code: `# README.md

# API Pipeline - E-commerce Data

Pipeline que consume datos de la API de e-commerce, los transforma, y los guarda particionados.

## Setup

1. Clon√° el repo
2. Cre√° un archivo \`.env\` con tu token:
   \`\`\`
   API_TOKEN=tu_token_aqui
   \`\`\`
3. Instal√° dependencias:
   \`\`\`bash
   pip install -r requirements.txt
   \`\`\`

## Uso

\`\`\`bash
python main.py
\`\`\`

## Output

Los datos se guardan en \`output/\` particionados por a√±o y mes:
\`\`\`
output/
‚îú‚îÄ‚îÄ orders/
‚îÇ   ‚îú‚îÄ‚îÄ order_year=2024/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ order_month=2024-01/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ order_month=2024-02/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ orders_all.parquet
\`\`\`

## Autor
[Tu nombre]`,
      explanation: { es: 'Un buen README hace que tu proyecto sea profesional y f√°cil de usar.', en: 'A good README makes your project professional and easy to use.', pt: 'Um bom README torna seu projeto profissional e f√°cil de usar.' }
    },
    { 
      order: 11, 
      text: { es: 'üöÄ Sub√≠ a GitHub', en: 'üöÄ Upload to GitHub', pt: 'üöÄ Suba para o GitHub' },
      code: `git init
git add .
git commit -m "API Pipeline con manejo de errores y particionamiento"
git remote add origin https://github.com/TU_USUARIO/api-pipeline.git
git push -u origin main`,
      warning: { es: 'Verific√° que .env NO est√© en el commit: git status debe mostrar .env en "Untracked files".', en: 'Verify .env is NOT in commit: git status must show .env in "Untracked files".', pt: 'Verifique que .env N√ÉO est√° no commit: git status deve mostrar .env em "Untracked files".' }
    },
  ],
  deliverable: { es: 'Repositorio con: main.py, config.py, requirements.txt, README.md, .gitignore (sin .env)', en: 'Repository with: main.py, config.py, requirements.txt, README.md, .gitignore (without .env)', pt: 'Reposit√≥rio com: main.py, config.py, requirements.txt, README.md, .gitignore (sem .env)' },
  evaluation: [
    { es: '¬øEl script maneja errores de conexi√≥n sin crashear?', en: 'Does script handle connection errors without crashing?', pt: 'O script lida com erros de conex√£o sem crashar?' },
    { es: '¬øImplementaste reintentos con exponential backoff?', en: 'Did you implement retries with exponential backoff?', pt: 'Implementou re tentativas com exponential backoff?' },
    { es: '¬øLos logs muestran claramente qu√© est√° pasando?', en: 'Do logs clearly show what is happening?', pt: 'Os logs mostram claramente o que est√° acontecendo?' },
    { es: '¬øEl .env NO est√° en el repositorio?', en: 'Is .env NOT in repository?', pt: 'O .env N√ÉO est√° no reposit√≥rio?' },
    { es: '¬øEl Parquet est√° particionado correctamente?', en: 'Is Parquet partitioned correctly?', pt: 'O Parquet est√° particionado corretamente?' },
    { es: '¬øEl README explica c√≥mo configurar y correr?', en: 'Does README explain how to configure and run?', pt: 'O README explica como configurar e rodar?' },
  ],
  codeExample: `# main.py - Pipeline Completo
import os
import time
import logging
import pandas as pd
import requests
from requests.exceptions import RequestException, Timeout, HTTPError
from config import API_TOKEN, API_BASE_URL

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def fetch_data_with_retry(
    dataset_type: str = 'ecommerce',
    rows: int = 1000,
    max_retries: int = 3
) -> dict:
    """Obtiene datos de la API con reintentos."""
    url = f"{API_BASE_URL}/datasets.php"
    params = {'type': dataset_type, 'rows': rows, 'token': API_TOKEN}
    
    for attempt in range(max_retries):
        try:
            logger.info(f"Fetching data (attempt {attempt + 1}/{max_retries})")
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()
        except (Timeout, RequestException) as e:
            logger.warning(f"Request failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                raise
    
def transform_data(raw_data: dict) -> pd.DataFrame:
    """Transforma los datos."""
    orders = raw_data.get('tables', {}).get('orders', [])
    df = pd.DataFrame(orders)
    df['order_date'] = pd.to_datetime(df['order_date'])
    df['order_month'] = df['order_date'].dt.to_period('M').astype(str)
    df['is_high_value'] = df['total'] > 100
    return df

def save_data(df: pd.DataFrame, output_dir: str = 'output'):
    """Guarda datos particionados."""
    os.makedirs(output_dir, exist_ok=True)
    df.to_parquet(f'{output_dir}/orders', partition_cols=['order_month'])
    logger.info(f"Saved {len(df)} records")

def main():
    logger.info("Starting API Pipeline")
    raw_data = fetch_data_with_retry(rows=5000)
    df = transform_data(raw_data)
    save_data(df)
    logger.info("Pipeline completed!")

if __name__ == "__main__":
    main()`,
  theory: { es: `## APIs en Data Engineering

Las APIs son una fuente de datos fundamental. El 70% de los datos que vas a consumir vienen de APIs.

### Manejo de Errores

| Error | Causa | Acci√≥n |
|-------|-------|--------|
| Timeout | Red lenta | Reintentar |
| 429 | Rate limit | Esperar y reintentar |
| 500 | Error del servidor | Reintentar |
| 401 | Token inv√°lido | NO reintentar, revisar token |
| 404 | Endpoint no existe | NO reintentar, revisar URL |

### Exponential Backoff

En vez de reintentar inmediatamente, esper√°s cada vez m√°s:
- Intento 1: esperar 2 segundos
- Intento 2: esperar 4 segundos
- Intento 3: esperar 8 segundos

Esto evita sobrecargar el servidor cuando tiene problemas.

### Variables de Entorno

NUNCA pongas secrets en el c√≥digo:
- ‚ùå \`API_TOKEN = "abc123"\`
- ‚úÖ \`API_TOKEN = os.getenv('API_TOKEN')\`

El archivo .env contiene los secrets y NUNCA se sube a git.`, en: `## APIs in Data Engineering

APIs are a fundamental data source. 70% of data you consume comes from APIs.

### Error Handling

| Error | Cause | Action |
|-------|-------|--------|
| Timeout | Slow network | Retry |
| 429 | Rate limit | Wait and retry |
| 500 | Server error | Retry |
| 401 | Invalid token | DO NOT retry, check token |
| 404 | Endpoint not found | DO NOT retry, check URL |

### Exponential Backoff

Instead of retrying immediately, you wait longer each time:
- Attempt 1: wait 2 seconds
- Attempt 2: wait 4 seconds
- Attempt 3: wait 8 seconds

This avoids overloading the server when it has issues.

### Environment Variables

NEVER put secrets in code:
- ‚ùå \`API_TOKEN = "abc123"\`
- ‚úÖ \`API_TOKEN = os.getenv('API_TOKEN')\`

.env file contains secrets and is NEVER pushed to git.`, pt: `## APIs em Data Engineering

APIs s√£o uma fonte de dados fundamental. 70% dos dados que voc√™ vai consumir v√™m de APIs.

### Tratamento de Erros

| Erro | Causa | A√ß√£o |
|------|-------|--------|
| Timeout | Rede lenta | Re tentar |
| 429 | Rate limit | Esperar e re tentar |
| 500 | Erro do servidor | Re tentar |
| 401 | Token inv√°lido | N√ÉO re tentar, revisar token |
| 404 | Endpoint n√£o existe | N√ÉO re tentar, revisar URL |

### Exponential Backoff

Em vez de re tentar imediatamente, espera cada vez mais:
- Tentativa 1: esperar 2 segundos
- Tentativa 2: esperar 4 segundos
- Tentativa 3: esperar 8 segundos

Isso evita sobrecarregar o servidor quando tem problemas.

### Vari√°veis de Ambiente

NUNCA coloque segredos no c√≥digo:
- ‚ùå \`API_TOKEN = "abc123"\`
- ‚úÖ \`API_TOKEN = os.getenv('API_TOKEN')\`

O arquivo .env cont√©m os segredos e NUNCA sobe para o git.` },
  nextSteps: [
    { es: 'Agreg√° m√°s validaciones a los datos', en: 'Add more data validations', pt: 'Adicione mais valida√ß√µes aos dados' },
    { es: 'Implement√° un modo "incremental" que solo traiga datos nuevos', en: 'Implement "incremental" mode fetching only new data', pt: 'Implemente um modo "incremental" que s√≥ traga dados novos' },
    { es: 'Conect√° con Airflow para que corra autom√°ticamente', en: 'Connect with Airflow to run automatically', pt: 'Conecte com Airflow para rodar automaticamente' },
  ],
  resources: [
    { title: { es: 'Requests Library', en: 'Requests Library', pt: 'Biblioteca Requests' }, url: 'https://requests.readthedocs.io/', type: 'docs' },
    { title: { es: 'Python Logging', en: 'Python Logging', pt: 'Python Logging' }, url: 'https://docs.python.org/3/howto/logging.html', type: 'docs' },
  ],
};


