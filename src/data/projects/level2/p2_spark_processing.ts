import { Project } from '../../../types/members';

export const p2_spark_processing: Project = {
  id: 'p2-spark-processing',
  level: 2,
  title: { es: 'Procesamiento con PySpark', en: 'Processing with PySpark', pt: 'Processamento com PySpark' },
  description: {
    es: 'Procesar grandes vol√∫menes de datos con Spark. Cuando Pandas no alcanza, Spark es la respuesta.',
    en: 'Process large data volumes with Spark. When Pandas is not enough, Spark is the answer.',
    pt: 'Processar grandes volumes de dados com Spark. Quando Pandas n√£o √© suficiente, Spark √© a resposta.'
  },
  difficulty: 'Avanzado',
  duration: '5-6 horas',
  skills: [{ es: 'Python' }, { es: 'PySpark' }, { es: 'Big Data' }, { es: 'Distributed Computing', en: 'Distributed Computing', pt: 'Computa√ß√£o Distribu√≠da' }],
  icon: '‚ö°',
  color: 'orange',
  datasetId: 'iot',
  prerequisites: ['p1-etl-python', 'p1-extra-window-functions'],
  estimatedLines: 150,
  realWorldExample: {
    es: 'As√≠ procesa Tesla millones de lecturas de sensores de sus veh√≠culos por segundo',
    en: 'This is how Tesla processes millions of sensor readings from its vehicles per second',
    pt: 'Assim a Tesla processa milh√µes de leituras de sensores de seus ve√≠culos por segundo'
  },
  usedBy: ['Netflix', 'Uber', 'Apple', 'Meta', 'LinkedIn'],
  learningObjectives: [
    { es: 'Entender Spark architecture (Driver, Executors)', en: 'Understand Spark architecture (Driver, Executors)', pt: 'Entender arquitetura Spark (Driver, Executors)' },
    { es: 'Transformations vs Actions', en: 'Transformations vs Actions', pt: 'Transformations vs Actions' },
    { es: 'Optimizar para evitar shuffles', en: 'Optimize to avoid shuffles', pt: 'Otimizar para evitar shuffles' },
    { es: 'Usar Window Functions en Spark', en: 'Use Window Functions in Spark', pt: 'Usar Window Functions no Spark' },
    { es: 'Particionar datos eficientemente', en: 'Partition data efficiently', pt: 'Particionar dados eficientemente' },
  ],
  expectedOutputs: [
    {
      step: 4,
      description: { es: 'Agregaci√≥n de sensores con Spark', en: 'Sensor aggregation with Spark', pt: 'Agrega√ß√£o de sensores com Spark' },
      example: `+------------+------+----------+----------+--------------+
| machine_id | hour | avg_temp | max_temp | reading_count|
+------------+------+----------+----------+--------------+
| M001       | 8    | 45.2     | 52.1     | 3600         |
| M001       | 9    | 46.8     | 55.3     | 3600         |
| M002       | 8    | 38.5     | 41.2     | 3600         |
+------------+------+----------+----------+--------------+
Time: 2.3 seconds (vs 45s en Pandas para 10M lecturas)`
    },
  ],
  interviewStory: {
    hook: { es: "Migr√© un pipeline de Pandas a PySpark y reduje el tiempo de procesamiento de 4 horas a 8 minutos para 50GB de datos.", en: "Migrated a pipeline from Pandas to PySpark and reduced processing time from 4 hours to 8 minutes for 50GB of data.", pt: "Migrei um pipeline de Pandas para PySpark e reduzi o tempo de processamento de 4 horas para 8 minutos para 50GB de dados." },
    situation: { es: "El pipeline de analytics corr√≠a con Pandas pero empez√≥ a fallar cuando el dataset creci√≥ a 50GB. Se quedaba sin memoria y tardaba horas.", en: "Analytics pipeline ran with Pandas but started failing when dataset grew to 50GB. Ran out of memory and took hours.", pt: "O pipeline de analytics rodava com Pandas mas come√ßou a falhar quando o dataset cresceu para 50GB. Ficava sem mem√≥ria e demorava horas." },
    task: { es: "Migrar el pipeline a PySpark para procesar datos a escala sin cambiar la l√≥gica de negocio.", en: "Migrate pipeline to PySpark to process data at scale without changing business logic.", pt: "Migrar o pipeline para PySpark para processar dados em escala sem mudar a l√≥gica de neg√≥cio." },
    actions: [
      { es: "Analic√© el c√≥digo Pandas existente e identifiqu√© operaciones equivalentes en Spark", en: "Analyzed existing Pandas code and identified equivalent operations in Spark", pt: "Analisei o c√≥digo Pandas existente e identifiquei opera√ß√µes equivalentes no Spark" },
      { es: "Configur√© SparkSession con particiones √≥ptimas para el tama√±o del cluster", en: "Configured SparkSession with optimal partitions for cluster size", pt: "Configurei SparkSession com parti√ß√µes √≥timas para o tamanho do cluster" },
      { es: "Reescrib√≠ transformaciones usando DataFrame API (m√°s legible que RDDs)", en: "Rewrote transformations using DataFrame API (more readable than RDDs)", pt: "Reescrevi transforma√ß√µes usando DataFrame API (mais leg√≠vel que RDDs)" },
      { es: "Implement√© broadcast joins para tablas peque√±as", en: "Implemented broadcast joins for small tables", pt: "Implementei broadcast joins para tabelas pequenas" },
      { es: "Optimic√© con repartition y coalesce para mejor paralelismo", en: "Optimized with repartition and coalesce for better parallelism", pt: "Otimizei com repartition e coalesce para melhor paralelismo" }
    ],
    results: [
      { es: "Tiempo de procesamiento: de 4 horas a 8 minutos (30x m√°s r√°pido)", en: "Processing time: from 4 hours to 8 minutes (30x faster)", pt: "Tempo de processamento: de 4 horas para 8 minutos (30x mais r√°pido)" },
      { es: "Puede procesar 50GB+ sin problemas de memoria", en: "Can process 50GB+ without memory issues", pt: "Pode processar 50GB+ sem problemas de mem√≥ria" },
      { es: "Costo de infraestructura: mismo, solo usamos mejor el cluster existente", en: "Infrastructure cost: same, just used existing cluster better", pt: "Custo de infraestrutura: mesmo, s√≥ usamos melhor o cluster existente" },
      { es: "El pipeline ahora escala linealmente con m√°s datos", en: "Pipeline now scales linearly with more data", pt: "O pipeline agora escala linearmente com mais dados" }
    ],
    learnings: [
      { es: "Spark no es 'Pandas distribuido' - hay que pensar diferente (lazy evaluation)", en: "Spark is not 'distributed Pandas' - must think differently (lazy evaluation)", pt: "Spark n√£o √© 'Pandas distribu√≠do' - tem que pensar diferente (lazy evaluation)" },
      { es: "El particionamiento es cr√≠tico - mal particionado = peor que Pandas", en: "Partitioning is critical - bad partitioning = worse than Pandas", pt: "O particionamento √© cr√≠tico - mal particionado = pior que Pandas" },
      { es: "Broadcast joins son un game changer para tablas de lookup", en: "Broadcast joins are a game changer for lookup tables", pt: "Broadcast joins s√£o um game changer para tabelas de lookup" }
    ],
    possibleQuestions: [
      {
        question: { es: "¬øCu√°ndo usar√≠as Spark vs Pandas?", en: "When would you use Spark vs Pandas?", pt: "Quando usaria Spark vs Pandas?" },
        answer: { es: "Pandas: datos que caben en memoria (<10GB). Spark: datos grandes o cuando necesit√°s paralelismo. El overhead de Spark no vale para datos chicos.", en: "Pandas: data fitting in memory (<10GB). Spark: large data or when parallelism is needed. Spark overhead isn't worth it for small data.", pt: "Pandas: dados que cabem na mem√≥ria (<10GB). Spark: dados grandes ou quando precisa de paralelismo. O overhead do Spark n√£o vale para dados pequenos." }
      },
      {
        question: { es: "¬øC√≥mo optimizaste el job de Spark?", en: "How did you optimize the Spark job?", pt: "Como otimizou o job do Spark?" },
        answer: { es: "1) Broadcast joins para dims peque√±as, 2) Repartition por columnas de join, 3) Cache de DataFrames reutilizados, 4) Evit√© UDFs cuando hab√≠a funciones nativas.", en: "1) Broadcast joins for small dims, 2) Repartition by join columns, 3) Cache reused DataFrames, 4) Avoided UDFs when native functions existed.", pt: "1) Broadcast joins para dims pequenas, 2) Repartition por colunas de join, 3) Cache de DataFrames reutilizados, 4) Evitei UDFs quando havia fun√ß√µes nativas." }
      },
      {
        question: { es: "¬øQu√© errores comunes ves en Spark?", en: "What common mistakes do you see in Spark?", pt: "Quais erros comuns v√™ no Spark?" },
        answer: { es: "1) Collect() en datasets grandes (trae todo al driver), 2) Demasiadas particiones peque√±as, 3) Shuffle innecesario por no usar broadcast, 4) UDFs lentos cuando hay alternativas nativas.", en: "1) Collect() on large datasets (brings all to driver), 2) Too many small partitions, 3) Unnecessary shuffle by not using broadcast, 4) Slow UDFs when native alternatives exist.", pt: "1) Collect() em datasets grandes (traz tudo para o driver), 2) Demasiadas parti√ß√µes pequenas, 3) Shuffle desnecess√°rio por n√£o usar broadcast, 4) UDFs lentos quando h√° alternativas nativas." }
      }
    ],
    closingStatement: { es: "Spark me ense√±√≥ que escalar no es solo agregar m√°quinas - es pensar diferente sobre c√≥mo fluyen los datos.", en: "Spark taught me that scaling is not just adding machines - it's thinking differently about how data flows.", pt: "Spark me ensinou que escalar n√£o √© s√≥ adicionar m√°quinas - √© pensar diferente sobre como fluem os dados." }
  },
  steps: [
    { 
      order: 1, 
      text: { es: 'ü§î ¬øQu√© es Spark y cu√°ndo lo necesito?', en: 'ü§î What is Spark and when do I need it?', pt: 'ü§î O que √© Spark e quando preciso dele?' },
      explanation: { es: `**Apache Spark** es un motor de procesamiento de datos distribuido. Pensalo as√≠:

| Herramienta | L√≠mite pr√°ctico | Cu√°ndo usar |
|-------------|-----------------|-------------|
| **Pandas** | ~5-10 GB | Datos que caben en memoria de tu laptop |
| **DuckDB** | ~50-100 GB | Analytics local, m√°s eficiente que Pandas |
| **Spark** | Terabytes+ | Datos que no caben en una sola m√°quina |

### ¬øPor qu√© Spark?
- **Distribuido**: Divide el trabajo entre m√∫ltiples m√°quinas
- **In-memory**: Procesa en memoria (mucho m√°s r√°pido que disco)
- **Lazy evaluation**: Optimiza el plan antes de ejecutar

### La sintaxis es muy similar a Pandas:
\`\`\`python
# Pandas
df.groupby('category')['total'].sum()

# Spark
df.groupBy('category').sum('total')
\`\`\`

### ¬øPuedo practicar sin un cluster?
¬°S√≠! PySpark corre localmente en tu laptop. Us√°s la misma API que en un cluster de 100 m√°quinas.`, en: `**Apache Spark** is a distributed data processing engine. Think of it like this:

| Tool | Practical Limit | When to use |
|------|-----------------|-------------|
| **Pandas** | ~5-10 GB | Data fitting in your laptop memory |
| **DuckDB** | ~50-100 GB | Local analytics, more efficient than Pandas |
| **Spark** | Terabytes+ | Data that doesn't fit in a single machine |

### Why Spark?
- **Distributed**: Divides work among multiple machines
- **In-memory**: Processes in memory (much faster than disk)
- **Lazy evaluation**: Optimizes plan before executing

### Syntax is very similar to Pandas:
\`\`\`python
# Pandas
df.groupby('category')['total'].sum()

# Spark
df.groupBy('category').sum('total')
\`\`\`

### Can I practice without a cluster?
Yes! PySpark runs locally on your laptop. You use the same API as in a 100-machine cluster.`, pt: `**Apache Spark** √© um motor de processamento de dados distribu√≠do. Pense assim:

| Ferramenta | Limite pr√°tico | Quando usar |
|------------|----------------|-------------|
| **Pandas** | ~5-10 GB | Dados que cabem na mem√≥ria do seu laptop |
| **DuckDB** | ~50-100 GB | Analytics local, mais eficiente que Pandas |
| **Spark** | Terabytes+ | Dados que n√£o cabem numa √∫nica m√°quina |

### Por que Spark?
- **Distribu√≠do**: Divide o trabalho entre m√∫ltiplas m√°quinas
- **In-memory**: Processa em mem√≥ria (muito mais r√°pido que disco)
- **Lazy evaluation**: Otimiza o plano antes de executar

### A sintaxe √© muito similar ao Pandas:
\`\`\`python
# Pandas
df.groupby('category')['total'].sum()

# Spark
df.groupBy('category').sum('total')
\`\`\`

### Posso praticar sem um cluster?
Sim! PySpark roda localmente no seu laptop. Usa a mesma API que num cluster de 100 m√°quinas.` },
      tip: { es: 'Para este proyecto usamos Spark local. El c√≥digo es id√©ntico al que usar√≠as en producci√≥n.', en: 'For this project we use local Spark. Code is identical to production.', pt: 'Para este projeto usamos Spark local. O c√≥digo √© id√™ntico ao que usaria em produ√ß√£o.' },
      checkpoint: { es: '¬øEntend√©s cu√°ndo usar Spark vs Pandas?', en: 'Do you understand when to use Spark vs Pandas?', pt: 'Entende quando usar Spark vs Pandas?' }
    },
    { 
      order: 2, 
      text: { es: 'üì¶ Instal√° PySpark', en: 'üì¶ Install PySpark', pt: 'üì¶ Instale PySpark' },
      code: `# Instalar PySpark (incluye Spark local)
pip install pyspark

# Verificar instalaci√≥n
python -c "from pyspark.sql import SparkSession; print('‚úÖ PySpark instalado')"`,
      explanation: { es: `**Nota**: PySpark incluye una versi√≥n de Spark que corre localmente. No necesit√°s instalar nada m√°s.

En producci√≥n, Spark correr√≠a en un cluster (Databricks, EMR, Dataproc), pero la API es exactamente la misma.`, en: `**Note**: PySpark includes a Spark version running locally. You don't need to install anything else.

In production, Spark would run on a cluster (Databricks, EMR, Dataproc), but API is exactly the same.`, pt: `**Nota**: PySpark inclui uma vers√£o do Spark que roda localmente. N√£o precisa instalar nada mais.

Em produ√ß√£o, Spark rodaria num cluster (Databricks, EMR, Dataproc), mas a API √© exatamente a mesma.` },
      checkpoint: { es: '¬øfrom pyspark.sql import SparkSession funciona?', en: 'Does from pyspark.sql import SparkSession work?', pt: 'from pyspark.sql import SparkSession funciona?' }
    },
    { 
      order: 3, 
      text: { es: 'üöÄ Cre√° SparkSession', en: 'üöÄ Create SparkSession', pt: 'üöÄ Crie SparkSession' },
      code: `from pyspark.sql import SparkSession

spark = SparkSession.builder \\
    .appName("EcommerceAnalytics") \\
    .config("spark.sql.shuffle.partitions", "4") \\
    .getOrCreate()

print(f"Spark version: {spark.version}")`,
      explanation: { es: 'SparkSession es el punto de entrada a Spark. Configura partitions seg√∫n tu m√°quina.', en: 'SparkSession is the entry point to Spark. Configures partitions according to your machine.', pt: 'SparkSession √© o ponto de entrada para o Spark. Configura partitions segundo sua m√°quina.' }
    },
    { 
      order: 4, 
      text: { es: 'üì• Carg√° datos', en: 'üì• Load data', pt: 'üì• Carregue dados' },
      code: `# Cargar datos de sensores IoT (ideal para Spark por el volumen)
# Descarg√° el dataset IoT desde la pesta√±a Datasets
df = spark.read.option("header", True).option("inferSchema", True).csv("data/iot_sensor_readings.csv")

# Cargar tambi√©n info de m√°quinas para enriquecer
machines = spark.read.option("header", True).option("inferSchema", True).csv("data/iot_machines.csv")

# Ver schema
df.printSchema()

# Ver primeras filas
df.show(5)

# Contar filas (las lecturas de sensores suelen ser millones)
print(f"Total lecturas: {df.count()}")`,
      explanation: { es: 'Los datos de IoT son ideales para Spark: millones de lecturas de sensores que necesitan procesamiento distribuido.', en: 'IoT data is ideal for Spark: millions of sensor readings that need distributed processing.', pt: 'Dados de IoT s√£o ideais para Spark: milh√µes de leituras de sensores que precisam de processamento distribu√≠do.' }
    },
    { 
      order: 5, 
      text: { es: 'üîÑ Transformaciones b√°sicas', en: 'üîÑ Basic transformations', pt: 'üîÑ Transforma√ß√µes b√°sicas' },
      code: `from pyspark.sql.functions import col, to_timestamp, hour, dayofweek

# Transformaciones (lazy - no se ejecutan todav√≠a)
df_clean = df \\
    .withColumn("reading_time", to_timestamp(col("timestamp"))) \\
    .withColumn("reading_hour", hour(col("reading_time"))) \\
    .withColumn("day_of_week", dayofweek(col("reading_time"))) \\
    .filter(col("temperature").isNotNull())

# Esto S√ç ejecuta (action)
df_clean.show(5)`,
      explanation: { es: 'Las transformaciones son "lazy": Spark las acumula y optimiza antes de ejecutar.', en: 'Transformations are "lazy": Spark accumulates and optimizes them before executing.', pt: 'As transforma√ß√µes s√£o "lazy": Spark as acumula e otimiza antes de executar.' },
      tip: { es: 'Solo cuando llam√°s una "action" (show, count, write) Spark ejecuta el plan.', en: 'Only when you call an "action" (show, count, write) does Spark execute the plan.', pt: 'S√≥ quando chama uma "action" (show, count, write) o Spark executa o plano.' }
    },
    { 
      order: 6, 
      text: { es: 'üìä Agregaciones', en: 'üìä Aggregations', pt: 'üìä Agrega√ß√µes' },
      code: `from pyspark.sql.functions import sum, avg, count, max, min

# Estad√≠sticas de sensores por m√°quina y hora
sensor_stats = df_clean \\
    .groupBy("machine_id", "reading_hour") \\
    .agg(
        avg("temperature").alias("avg_temp"),
        max("temperature").alias("max_temp"),
        avg("vibration").alias("avg_vibration"),
        count("*").alias("reading_count")
    ) \\
    .orderBy("machine_id", "reading_hour")

sensor_stats.show()`,
      explanation: { es: 'groupBy + agg es el patr√≥n para agregaciones. Siempre us√° alias() para nombrar columnas.', en: 'groupBy + agg is the pattern for aggregations. Always use alias() to name columns.', pt: 'groupBy + agg √© o padr√£o para agrega√ß√µes. Sempre use alias() para nomear colunas.' }
    },
    { 
      order: 7, 
      text: { es: 'ü™ü Window Functions', en: 'ü™ü Window Functions', pt: 'ü™ü Window Functions' },
      code: `from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, lag

# Definir ventana por m√°quina ordenada por tiempo
window = Window.partitionBy("machine_id").orderBy("reading_time")

# Detectar cambios de temperatura (√∫til para mantenimiento predictivo)
df_with_window = df_clean \\
    .withColumn("reading_rank", row_number().over(window)) \\
    .withColumn("prev_temperature", lag("temperature").over(window)) \\
    .withColumn("temp_change", col("temperature") - col("prev_temperature"))

df_with_window.show(10)`,
      explanation: { es: 'Window Functions en Spark funcionan igual que en SQL.', en: 'Window Functions in Spark work just like in SQL.', pt: 'Window Functions no Spark funcionam igual ao SQL.' }
    },
    { 
      order: 8, 
      text: { es: 'üíæ Guard√° particionado', en: 'üíæ Save partitioned', pt: 'üíæ Salve particionado' },
      code: `# Guardar particionado por m√°quina y d√≠a
df_clean.write \\
    .partitionBy("machine_id", "day_of_week") \\
    .mode("overwrite") \\
    .parquet("output/sensor_data_partitioned")

# Verificar estructura
import os
for root, dirs, files in os.walk("output/sensor_data_partitioned"):
    print(root)`,
      explanation: { es: 'Particionar mejora performance: las queries solo leen las particiones necesarias.', en: 'Partitioning improves performance: queries only read necessary partitions.', pt: 'Particionar melhora performance: as queries s√≥ leem as parti√ß√µes necess√°rias.' }
    },
    { 
      order: 9, 
      text: { es: 'üìä Compar√° con Pandas', en: 'üìä Compare with Pandas', pt: 'üìä Compare com Pandas' },
      code: `import time
import pandas as pd

# Medir tiempo con Pandas
start = time.time()
df_pandas = pd.read_csv("data/iot_sensor_readings.csv")
df_pandas['timestamp'] = pd.to_datetime(df_pandas['timestamp'])
df_pandas['hour'] = df_pandas['timestamp'].dt.hour
result_pandas = df_pandas.groupby(['machine_id', 'hour'])['temperature'].mean()
pandas_time = time.time() - start

# Medir tiempo con Spark
start = time.time()
result_spark = df_clean.groupBy("machine_id", "reading_hour").avg("temperature").collect()
spark_time = time.time() - start

print(f"Pandas: {pandas_time:.2f}s")
print(f"Spark: {spark_time:.2f}s")
print("üí° Con millones de lecturas de sensores, Spark gana por mucho!")`,
      explanation: { es: 'Para datasets peque√±os, Pandas es m√°s r√°pido. Spark brilla con millones de filas.', en: 'For small datasets, Pandas is faster. Spark shines with millions of rows.', pt: 'Para datasets pequenos, Pandas √© mais r√°pido. Spark brilha com milh√µes de linhas.' },
      tip: { es: 'Spark tiene overhead de inicializaci√≥n. Vale la pena para >1GB de datos.', en: 'Spark has initialization overhead. Worth it for >1GB data.', pt: 'Spark tem overhead de inicializa√ß√£o. Vale a pena para >1GB de dados.' }
    },
  ],
  deliverable: { es: 'Notebook con c√≥digo Spark + an√°lisis de performance', en: 'Notebook with Spark code + performance analysis', pt: 'Notebook com c√≥digo Spark + an√°lise de performance' },
  evaluation: [
    { es: '¬øEntend√©s la diferencia entre transformations y actions?', en: 'Do you understand difference between transformations and actions?', pt: 'Entende a diferen√ßa entre transformations e actions?' },
    { es: '¬øUsaste particionamiento?', en: 'Did you use partitioning?', pt: 'Usou particionamento?' },
    { es: '¬øOptimizaste para evitar shuffles?', en: 'Did you optimize to avoid shuffles?', pt: 'Otimizou para evitar shuffles?' },
    { es: '¬øComparaste performance con Pandas?', en: 'Did you compare performance with Pandas?', pt: 'Comparou performance com Pandas?' },
  ],
  theory: { es: `## Spark Architecture

**Driver**: Coordina el trabajo, crea el plan de ejecuci√≥n
**Executors**: Ejecutan las tareas en paralelo
**Cluster Manager**: Asigna recursos (YARN, Kubernetes, Standalone)

## Transformations vs Actions

| Transformations (Lazy) | Actions (Execute) |
|------------------------|-------------------|
| filter, select, withColumn | show, count, collect |
| groupBy, join | write, save |
| No ejecutan inmediatamente | Ejecutan el plan |

## Optimizaci√≥n

1. **Evitar shuffles**: groupBy y join causan shuffles (costosos)
2. **Particionar bien**: Usar columnas de filtro com√∫n
3. **Broadcast joins**: Para tablas peque√±as
4. **Caching**: df.cache() para reusar DataFrames`, en: `## Spark Architecture

**Driver**: Coordinates work, creates execution plan
**Executors**: Execute tasks in parallel
**Cluster Manager**: Assigns resources (YARN, Kubernetes, Standalone)

## Transformations vs Actions

| Transformations (Lazy) | Actions (Execute) |
|------------------------|-------------------|
| filter, select, withColumn | show, count, collect |
| groupBy, join | write, save |
| Do not execute immediately | Execute the plan |

## Optimization

1. **Avoid shuffles**: groupBy and join cause shuffles (expensive)
2. **Partition well**: Use common filter columns
3. **Broadcast joins**: For small tables
4. **Caching**: df.cache() to reuse DataFrames`, pt: `## Arquitetura Spark

**Driver**: Coordena o trabalho, cria o plano de execu√ß√£o
**Executors**: Executam as tarefas em paralelo
**Cluster Manager**: Atribui recursos (YARN, Kubernetes, Standalone)

## Transformations vs Actions

| Transformations (Lazy) | Actions (Execute) |
|------------------------|-------------------|
| filter, select, withColumn | show, count, collect |
| groupBy, join | write, save |
| N√£o executam imediatamente | Executam o plano |

## Otimiza√ß√£o

1. **Evitar shuffles**: groupBy e join causam shuffles (custosos)
2. **Particionar bem**: Usar colunas de filtro comum
3. **Broadcast joins**: Para tabelas pequenas
4. **Caching**: df.cache() para reusar DataFrames` },
};


