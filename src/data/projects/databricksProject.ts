import { Project } from '../../types/members';

export const databricksProject: Project = {
  id: 'p2-databricks-intro',
  level: 2,
  title: {
    es: 'Data Engineering con Databricks',
    pt: 'Data Engineering com Databricks'
  },
  description: {
    es: 'Aprend√© a usar Databricks, la plataforma de lakehouse m√°s popular. Vas a crear notebooks, procesar datos con Spark, y usar Delta Lake.',
    pt: 'Aprenda a usar o Databricks, a plataforma de lakehouse mais popular. Voc√™ vai criar notebooks, processar dados com Spark e usar Delta Lake.'
  },
  difficulty: 'Intermedio',
  duration: '4-5 horas',
  skills: [
    { es: 'Databricks', pt: 'Databricks' },
    { es: 'PySpark', pt: 'PySpark' },
    { es: 'Delta Lake', pt: 'Delta Lake' },
    { es: 'Notebooks', pt: 'Notebooks' },
    { es: 'SQL', pt: 'SQL' }
  ],
  icon: 'üß±',
  color: 'orange',
  datasetId: 'streaming',
  estimatedLines: 100,
  realWorldExample: {
    es: 'As√≠ analiza Comcast millones de eventos de visualizaci√≥n de su plataforma de streaming',
    pt: 'Assim a Comcast analisa milh√µes de eventos de visualiza√ß√£o de sua plataforma de streaming'
  },
  usedBy: ['Shell', 'Comcast', 'HSBC', 'CVS Health', 'Regeneron'],
  learningObjectives: [
    { es: 'Entender la arquitectura Lakehouse', pt: 'Entender a arquitetura Lakehouse' },
    { es: 'Crear y organizar notebooks', pt: 'Criar e organizar notebooks' },
    { es: 'Procesar datos con PySpark', pt: 'Processar dados com PySpark' },
    { es: 'Usar Delta Lake para tablas ACID', pt: 'Usar Delta Lake para tabelas ACID' },
    { es: 'Optimizar con Z-Ordering y VACUUM', pt: 'Otimizar com Z-Ordering e VACUUM' },
  ],
  commonMistakes: [
    {
      mistake: { es: 'Dejar clusters prendidos sin usar', pt: 'Deixar clusters ligados sem uso' },
      why: { es: 'Databricks cobra por DBU (Databricks Units) por hora', pt: 'Databricks cobra por DBU (Databricks Units) por hora' },
      solution: { es: 'Configurar auto-termination en 10-30 minutos', pt: 'Configurar auto-termination em 10-30 minutos' },
      code: `# En configuraci√≥n del cluster:
Auto Termination: 30 minutes`
    },
    {
      mistake: { es: 'No usar Delta Lake', pt: 'N√£o usar Delta Lake' },
      why: { es: 'Parquet simple no tiene ACID, time travel, ni schema evolution', pt: 'Parquet simples n√£o tem ACID, time travel nem schema evolution' },
      solution: { es: 'Siempre us√° Delta: df.write.format("delta").save(path)', pt: 'Sempre use Delta: df.write.format("delta").save(path)' },
      code: `# ‚ùå Parquet simple
df.write.parquet("/data/orders")

# ‚úÖ Delta Lake
df.write.format("delta").save("/data/orders")`
    },
    {
      mistake: { es: 'No optimizar tablas grandes', pt: 'N√£o otimizar tabelas grandes' },
      why: { es: 'Queries lentas y costos altos por escaneo', pt: 'Queries lentas e custos altos por escaneamento' },
      solution: { es: 'Us√° OPTIMIZE y Z-ORDER en columnas de filtro', pt: 'Use OPTIMIZE e Z-ORDER em colunas de filtro' },
      code: `OPTIMIZE watch_history ZORDER BY (started_at, profile_id)`
    },
  ],
  expectedOutputs: [
    {
      step: 4,
      description: { es: 'DataFrame de visualizaciones en Spark', pt: 'DataFrame de visualiza√ß√µes no Spark' },
      example: `+----------+------------+------------+------------------+-----------+
| watch_id | profile_id | content_id | started_at       | completed |
+----------+------------+------------+------------------+-----------+
| 1        | 42         | 156        | 2024-01-15 20:30 | true      |
| 2        | 42         | 203        | 2024-01-15 22:15 | false     |
+----------+------------+------------+------------------+-----------+
Rows: 100000, Partitions: 8`
    },
    {
      step: 6,
      description: { es: 'Tabla Delta creada', pt: 'Tabela Delta criada' },
      example: `DESCRIBE HISTORY watch_history
+-------+---------------------+----------+
|version| timestamp           | operation|
+-------+---------------------+----------+
| 0     | 2024-01-15 10:30:00 | WRITE    |
| 1     | 2024-01-15 11:00:00 | MERGE    |
+-------+---------------------+----------+`
    },
  ],
  steps: [
    {
      order: 1,
      text: { es: 'üìã Cre√° tu cuenta de Databricks Community Edition', pt: 'üìã Crie sua conta do Databricks Community Edition' },
      explanation: { es: 'Databricks Community Edition es gratis y suficiente para aprender. And√° a databricks.com/try-databricks y eleg√≠ "Community Edition".', pt: 'Databricks Community Edition √© gr√°tis e suficiente para aprender. V√° para databricks.com/try-databricks e escolha "Community Edition".' },
      tip: { es: 'Community Edition tiene un cluster limitado pero suficiente para proyectos de aprendizaje.', pt: 'Community Edition tem um cluster limitado mas suficiente para projetos de aprendizado.' },
      checkpoint: { es: '¬øPod√©s acceder al workspace de Databricks?', pt: 'Consegue acessar o workspace do Databricks?' },
      estimatedTime: '10min',
      difficulty: 'easy',
    },
    {
      order: 2,
      text: { es: 'üìÅ Cre√° la estructura de notebooks', pt: 'üìÅ Crie a estrutura de notebooks' },
      code: `# En el Workspace, cre√° esta estructura:
/Users/tu_email/
‚îú‚îÄ‚îÄ 01_setup/
‚îÇ   ‚îî‚îÄ‚îÄ 00_config.py
‚îú‚îÄ‚îÄ 02_ingestion/
‚îÇ   ‚îî‚îÄ‚îÄ 01_load_raw_data.py
‚îú‚îÄ‚îÄ 03_transformation/
‚îÇ   ‚îî‚îÄ‚îÄ 01_clean_data.py
‚îî‚îÄ‚îÄ 04_analytics/
    ‚îî‚îÄ‚îÄ 01_metrics.sql`,
      explanation: { es: 'Organiz√° notebooks por etapa del pipeline. Us√° n√∫meros para ordenar la ejecuci√≥n.', pt: 'Organize notebooks por etapa do pipeline. Use n√∫meros para ordenar a execu√ß√£o.' },
      estimatedTime: '10min',
      difficulty: 'easy',
    },
    {
      order: 3,
      text: { es: '‚ö° Cre√° y configur√° un cluster', pt: '‚ö° Crie e configure um cluster' },
      code: `# Configuraci√≥n recomendada para Community Edition:
Cluster Mode: Single Node
Databricks Runtime: 14.0 LTS (o la √∫ltima LTS)
Node Type: Standard (el disponible)
Auto Termination: 30 minutes

# En Compute > Create Cluster`,
      explanation: { es: 'LTS = Long Term Support. Siempre eleg√≠ versiones LTS para estabilidad.', pt: 'LTS = Long Term Support. Sempre escolha vers√µes LTS para estabilidade.' },
      warning: { es: 'En Community Edition solo pod√©s tener 1 cluster activo.', pt: 'No Community Edition voc√™ s√≥ pode ter 1 cluster ativo.' },
      estimatedTime: '5min',
      difficulty: 'easy',
    },
    {
      order: 4,
      text: { es: 'üì• Carg√° datos con PySpark', pt: 'üì• Carregue dados com PySpark' },
      code: `# Notebook: 02_ingestion/01_load_raw_data.py

# Subir el JSON del dataset a DBFS (Databricks File System)
# En el sidebar: Data > DBFS > Upload

# Leer datos de plataforma de streaming desde DBFS
# Sub√≠ los archivos CSV/JSON a DBFS primero (Data > DBFS > Upload)

# En Databricks, spark ya est√° disponible
# spark = SparkSession.builder.getOrCreate()  # No necesario

# Cargar historial de visualizaciones (tabla principal)
watch_history = spark.read.option("header", True).option("inferSchema", True).csv("/FileStore/streaming_watch_history.csv")

# Cargar contenido y usuarios
content = spark.read.option("header", True).option("inferSchema", True).csv("/FileStore/streaming_content.csv")
users = spark.read.option("header", True).option("inferSchema", True).csv("/FileStore/streaming_users.csv")

# Ver estructura
watch_history.printSchema()

# Ver datos
watch_history.show(5)
print(f"Total visualizaciones: {watch_history.count()}")`,
      explanation: { es: 'DBFS es el filesystem distribuido de Databricks. /FileStore/ es para archivos subidos manualmente.', pt: 'DBFS √© o filesystem distribu√≠do do Databricks. /FileStore/ √© para arquivos carregados manualmente.' },
      checkpoint: { es: '¬øPod√©s ver las primeras filas del DataFrame?', pt: 'Consegue ver as primeiras linhas do DataFrame?' },
      estimatedTime: '20min',
      difficulty: 'medium',
    },
    {
      order: 5,
      text: { es: 'üîÑ Transform√° con PySpark', pt: 'üîÑ Transforme com PySpark' },
      code: `# Notebook: 03_transformation/01_clean_data.py

from pyspark.sql.functions import col, to_timestamp, when, lit, current_timestamp

# Leer datos raw desde CSV
watch_history = spark.read.option("header", True).option("inferSchema", True).csv("/FileStore/streaming_watch_history.csv")

# Transformaciones
watch_clean = watch_history \\
    .withColumn("order_date", to_date(col("order_date"))) \\
    .withColumn("total", col("total").cast("double")) \\
    .withColumn("is_high_value", when(col("total") > 100, True).otherwise(False)) \\
    .withColumn("processed_at", current_timestamp()) \\
    .dropna(subset=["order_id", "customer_id"]) \\
    .dropDuplicates(["order_id"])

# Ver resultado
orders_clean.show(5)
print(f"Rows after cleaning: {orders_clean.count()}")

# Cache para reusar
orders_clean.cache()`,
      explanation: { es: 'PySpark es similar a Pandas pero distribuido. Us√° .cache() para DataFrames que vas a reusar.', pt: 'PySpark √© similar a Pandas mas distribu√≠do. Use .cache() para DataFrames que vai reutilizar.' },
      tip: { es: 'Las transformaciones son lazy - no se ejecutan hasta que hac√©s una acci√≥n (show, count, write).', pt: 'As transforma√ß√µes s√£o lazy - n√£o s√£o executadas at√© que voc√™ fa√ßa uma a√ß√£o (show, count, write).' },
      estimatedTime: '20min',
      difficulty: 'medium',
    },
    {
      order: 6,
      text: { es: 'üíæ Guard√° en Delta Lake', pt: 'üíæ Salve no Delta Lake' },
      code: `# Guardar como tabla Delta
orders_clean.write \\
    .format("delta") \\
    .mode("overwrite") \\
    .saveAsTable("default.orders_clean")

# Verificar
spark.sql("DESCRIBE EXTENDED default.orders_clean").show(truncate=False)

# Ver historial de cambios (Time Travel)
spark.sql("DESCRIBE HISTORY default.orders_clean").show()

# Leer versi√≥n anterior
# orders_v0 = spark.read.format("delta").option("versionAsOf", 0).table("default.orders_clean")`,
      explanation: { es: 'Delta Lake agrega ACID, time travel, y schema evolution sobre Parquet.', pt: 'Delta Lake adiciona ACID, time travel e schema evolution sobre Parquet.' },
      checkpoint: { es: '¬øPod√©s ver la tabla en el Data Explorer?', pt: 'Consegue ver a tabela no Data Explorer?' },
      estimatedTime: '15min',
      difficulty: 'medium',
    },
    {
      order: 7,
      text: { es: 'üìä Queries SQL en notebooks', pt: 'üìä Queries SQL em notebooks' },
      code: `-- Notebook: 04_analytics/01_metrics.sql
-- Cambiar el lenguaje del notebook a SQL

-- Ventas por mes
SELECT 
    date_trunc('month', order_date) AS mes,
    COUNT(*) AS total_ordenes,
    SUM(total) AS ingresos,
    AVG(total) AS ticket_promedio
FROM default.orders_clean
GROUP BY 1
ORDER BY 1;

-- Top clientes
SELECT 
    customer_id,
    COUNT(*) AS ordenes,
    SUM(total) AS total_gastado
FROM default.orders_clean
GROUP BY 1
ORDER BY 3 DESC
LIMIT 10;`,
      explanation: { es: 'Pod√©s mezclar notebooks Python y SQL. SQL es m√°s f√°cil para an√°lisis exploratorio.', pt: 'Pode misturar notebooks Python e SQL. SQL √© mais f√°cil para an√°lise explorat√≥ria.' },
      estimatedTime: '15min',
      difficulty: 'easy',
    },
    {
      order: 8,
      text: { es: '‚ö° Optimiz√° tablas Delta', pt: '‚ö° Otimize tabelas Delta' },
      code: `-- Optimizar tabla (compactar archivos peque√±os)
OPTIMIZE default.orders_clean;

-- Z-Order para queries frecuentes por fecha
OPTIMIZE default.orders_clean ZORDER BY (order_date);

-- Limpiar versiones antiguas (liberar storage)
VACUUM default.orders_clean RETAIN 168 HOURS;

-- Ver m√©tricas de la tabla
DESCRIBE DETAIL default.orders_clean;`,
      explanation: { es: 'OPTIMIZE compacta archivos peque√±os. Z-ORDER ordena datos para queries m√°s r√°pidas. VACUUM limpia versiones antiguas.', pt: 'OPTIMIZE compacta arquivos pequenos. Z-ORDER ordena dados para queries mais r√°pidas. VACUUM limpa vers√µes antigas.' },
      warning: { es: 'VACUUM elimina el time travel para versiones anteriores al per√≠odo de retenci√≥n.', pt: 'VACUUM elimina o time travel para vers√µes anteriores ao per√≠odo de reten√ß√£o.' },
      estimatedTime: '10min',
      difficulty: 'medium',
    },
    {
      order: 9,
      text: { es: 'üìù Document√° y organiz√°', pt: 'üìù Documente e organize' },
      code: `# Buenas pr√°cticas:
# 1. Agreg√° markdown al inicio de cada notebook explicando qu√© hace
# 2. Us√° widgets para par√°metros

# Crear widget para fecha
dbutils.widgets.text("start_date", "2024-01-01", "Fecha Inicio")
start_date = dbutils.widgets.get("start_date")

# Usarlo en queries
df = spark.sql(f"SELECT * FROM orders WHERE order_date >= '{start_date}'")

# 3. Crear un notebook de orquestaci√≥n
# dbutils.notebook.run("./02_ingestion/01_load_raw_data", 300)
# dbutils.notebook.run("./03_transformation/01_clean_data", 300)`,
      explanation: { es: 'Widgets permiten parametrizar notebooks. dbutils.notebook.run permite orquestar notebooks.', pt: 'Widgets permitem parametrizar notebooks. dbutils.notebook.run permite orquestrar notebooks.' },
      checkpoint: { es: '¬øTus notebooks tienen documentaci√≥n clara?', pt: 'Seus notebooks t√™m documenta√ß√£o clara?' },
      estimatedTime: '15min',
      difficulty: 'easy',
    },
  ],
  deliverable: { es: 'Workspace de Databricks con notebooks organizados, tabla Delta optimizada, y queries de analytics', pt: 'Workspace do Databricks com notebooks organizados, tabela Delta otimizada e queries de analytics' },
  evaluation: [
    { es: '¬øCreaste la estructura de notebooks correctamente?', pt: 'Criou a estrutura de notebooks corretamente?' },
    { es: '¬øCargaste y transformaste datos con PySpark?', pt: 'Carregou e transformou dados com PySpark?' },
    { es: '¬øGuardaste en formato Delta Lake?', pt: 'Salvou em formato Delta Lake?' },
    { es: '¬øOptimizaste la tabla con OPTIMIZE y Z-ORDER?', pt: 'Otimizou a tabela com OPTIMIZE e Z-ORDER?' },
    { es: '¬øConfiguraste auto-termination en el cluster?', pt: 'Configurou auto-termination no cluster?' },
  ],
  theory: {
    es: `## Arquitectura Lakehouse

Databricks invent√≥ el concepto de **Lakehouse**: combina lo mejor de Data Lakes y Data Warehouses.

### Data Lake vs Data Warehouse vs Lakehouse

| Aspecto | Data Lake | Data Warehouse | Lakehouse |
|---------|-----------|----------------|-----------|
| Storage | Barato (S3) | Caro | Barato (S3) |
| Schema | Schema-on-read | Schema-on-write | Flexible |
| ACID | ‚ùå | ‚úÖ | ‚úÖ (Delta) |
| Performance | Variable | Alta | Alta |
| ML/AI | ‚úÖ | ‚ùå | ‚úÖ |

## Delta Lake

Delta Lake es un formato de storage que agrega:

- **ACID transactions**: No m√°s datos corruptos
- **Time Travel**: Volver a versiones anteriores
- **Schema Evolution**: Cambiar schema sin reescribir
- **Audit History**: Ver qui√©n cambi√≥ qu√©

## Comandos Esenciales

\`\`\`sql
-- Crear tabla Delta
CREATE TABLE orders USING DELTA AS SELECT ...

-- Time Travel
SELECT * FROM orders VERSION AS OF 5
SELECT * FROM orders TIMESTAMP AS OF '2024-01-01'

-- Merge (Upsert)
MERGE INTO target USING source ON condition
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT

-- Optimizar
OPTIMIZE table ZORDER BY (columns)
VACUUM table RETAIN 168 HOURS
\`\`\`

## Costos

Databricks cobra por DBU (Databricks Unit):
- 1 DBU ‚âà $0.15-0.75/hora (depende del tier)
- Cluster peque√±o: ~2 DBU/hora = ~$0.30-1.50/hora
- **Siempre usar auto-termination**`,
    pt: `## Arquitetura Lakehouse

Databricks inventou o conceito de **Lakehouse**: combina o melhor de Data Lakes e Data Warehouses.

### Data Lake vs Data Warehouse vs Lakehouse

| Aspecto | Data Lake | Data Warehouse | Lakehouse |
|---------|-----------|----------------|-----------|
| Storage | Barato (S3) | Caro | Barato (S3) |
| Schema | Schema-on-read | Schema-on-write | Flex√≠vel |
| ACID | ‚ùå | ‚úÖ | ‚úÖ (Delta) |
| Performance | Vari√°vel | Alta | Alta |
| ML/AI | ‚úÖ | ‚ùå | ‚úÖ |

## Delta Lake

Delta Lake √© um formato de storage que adiciona:

- **Transa√ß√µes ACID**: Sem mais dados corrompidos
- **Time Travel**: Voltar a vers√µes anteriores
- **Schema Evolution**: Mudar schema sem reescrever
- **Hist√≥rico de Auditoria**: Ver quem mudou o qu√™

## Comandos Essenciais

\`\`\`sql
-- Criar tabela Delta
CREATE TABLE orders USING DELTA AS SELECT ...

-- Time Travel
SELECT * FROM orders VERSION AS OF 5
SELECT * FROM orders TIMESTAMP AS OF '2024-01-01'

-- Merge (Upsert)
MERGE INTO target USING source ON condition
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT

-- Otimizar
OPTIMIZE table ZORDER BY (columns)
VACUUM table RETAIN 168 HOURS
\`\`\`

## Custos

Databricks cobra por DBU (Databricks Unit):
- 1 DBU ‚âà $0.15-0.75/hora (depende do tier)
- Cluster pequeno: ~2 DBU/hora = ~$0.30-1.50/hora
- **Sempre usar auto-termination**`
  },
};
