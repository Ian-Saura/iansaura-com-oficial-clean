import { Project } from '../../../types/members';

export const p3PerformanceTuning: Project = {
  id: 'p3-performance-tuning',
  level: 3,
  title: {
    es: 'Performance Tuning de Pipeline',
    pt: 'Performance Tuning de Pipeline'
  },
  description: {
    es: 'Optimiz√° un pipeline lento hasta hacerlo 10x m√°s r√°pido. Esto te diferencia como Senior en entrevistas y en el trabajo.',
    pt: 'Otimize um pipeline lento at√© torn√°-lo 10x mais r√°pido. Isso te diferencia como S√™nior em entrevistas e no trabalho.'
  },
  difficulty: 'Expert',
  duration: '4-5 horas',
  skills: [
    { es: 'Performance', pt: 'Performance' },
    { es: 'Profiling', pt: 'Profiling' },
    { es: 'Optimization', pt: 'Otimiza√ß√£o' },
    { es: 'Python', pt: 'Python' },
    { es: 'SQL', pt: 'SQL' }
  ],
  icon: '‚ö°',
  color: 'emerald',
  datasetId: 'logs',
  prerequisites: ['p2-spark-processing', 'p2-sql-optimization'],
  estimatedLines: 150,
  realWorldExample: {
    es: 'As√≠ optimiza Datadog el procesamiento de miles de millones de logs por d√≠a',
    pt: 'Assim o Datadog otimiza o processamento de bilh√µes de logs por dia'
  },
  usedBy: ['Shopify', 'Stripe', 'Square', 'Plaid'],
  expectedOutputs: [
    {
      step: 5,
      description: { es: 'Comparaci√≥n antes/despu√©s', pt: 'Compara√ß√£o antes/depois' },
      example: `üìä Performance Report
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ANTES (slow_pipeline):
  - Tiempo: 45.2 segundos
  - Memoria: 2.1 GB peak
  - CPU: 25% (single thread)

DESPU√âS (fast_pipeline):
  - Tiempo: 3.8 segundos
  - Memoria: 450 MB peak
  - CPU: 85% (multi-thread)

‚ö° Mejora: 11.9x m√°s r√°pido
üíæ Mejora: 4.7x menos memoria`
    },
  ],
  learningObjectives: [
    { es: 'Identificar cuellos de botella con profiling', pt: 'Identificar gargalos com profiling' },
    { es: 'Optimizar c√≥digo Python', pt: 'Otimizar c√≥digo Python' },
    { es: 'Optimizar queries SQL', pt: 'Otimizar queries SQL' },
    { es: 'Usar paralelismo efectivamente', pt: 'Usar paralelismo efetivamente' },
    { es: 'Medir y documentar mejoras', pt: 'Medir e documentar melhorias' },
  ],
  interviewStory: {
    hook: {
      es: "Optimic√© un pipeline que tardaba 4 horas a solo 8 minutos - una mejora de 30x que ahorr√≥ $50K/a√±o en compute.",
      pt: "Otimizei um pipeline que demorava 4 horas para apenas 8 minutos - uma melhoria de 30x que economizou $50K/ano em computa√ß√£o."
    },
    situation: {
      es: "El pipeline de analytics tardaba 4 horas y bloqueaba otros jobs. El costo de compute era alt√≠simo y los reportes llegaban tarde.",
      pt: "O pipeline de analytics demorava 4 horas e bloqueava outros jobs. O custo de computa√ß√£o era alt√≠ssimo e os relat√≥rios chegavam tarde."
    },
    task: {
      es: "Identificar los cuellos de botella y optimizar el pipeline sin cambiar los resultados.",
      pt: "Identificar os gargalos e otimizar o pipeline sem mudar os resultados."
    },
    actions: [
      { es: "Perfil√© el c√≥digo con cProfile y line_profiler para encontrar hotspots", pt: "Perfiei o c√≥digo com cProfile e line_profiler para encontrar hotspots" },
      { es: "Reemplac√© loops de Python con operaciones vectorizadas de Pandas", pt: "Substitu√≠ loops de Python por opera√ß√µes vetorizadas de Pandas" },
      { es: "Cambi√© de CSV a Parquet para I/O 10x m√°s r√°pido", pt: "Mudei de CSV para Parquet para I/O 10x mais r√°pido" },
      { es: "Implement√© procesamiento en chunks para no cargar todo en memoria", pt: "Implementei processamento em chunks para n√£o carregar tudo na mem√≥ria" },
      { es: "Paralelic√© operaciones independientes con multiprocessing", pt: "Paralelizei opera√ß√µes independentes com multiprocessing" }
    ],
    results: [
      { es: "Tiempo de ejecuci√≥n: de 4 horas a 8 minutos (30x m√°s r√°pido)", pt: "Tempo de execu√ß√£o: de 4 horas para 8 minutos (30x mais r√°pido)" },
      { es: "Uso de memoria: de 16GB a 2GB", pt: "Uso de mem√≥ria: de 16GB para 2GB" },
      { es: "Costo de compute: -$50K/a√±o", pt: "Custo de computa√ß√£o: -$50K/ano" },
      { es: "Pipeline ahora corre 3 veces al d√≠a en vez de 1", pt: "Pipeline agora roda 3 vezes ao dia em vez de 1" }
    ],
    learnings: [
      { es: "Siempre perfilar antes de optimizar - la intuici√≥n suele estar mal", pt: "Sempre fazer profiling antes de otimizar - a intui√ß√£o costuma estar errada" },
      { es: "Los loops de Python son el enemigo #1 de performance", pt: "Os loops de Python s√£o o inimigo #1 da performance" },
      { es: "Parquet no es solo m√°s chico, es dram√°ticamente m√°s r√°pido", pt: "Parquet n√£o √© s√≥ menor, √© dramaticamente mais r√°pido" }
    ],
    possibleQuestions: [
      {
        question: { es: "¬øC√≥mo identific√°s qu√© optimizar?", pt: "Como voc√™ identifica o que otimizar?" },
        answer: { es: "Profiling primero, siempre. Uso cProfile para overview, line_profiler para detalle. Optimizo el 20% del c√≥digo que toma el 80% del tiempo.", pt: "Profiling primeiro, sempre. Uso cProfile para overview, line_profiler para detalhe. Otimizo os 20% do c√≥digo que toma 80% do tempo." }
      },
      {
        question: { es: "¬øPor qu√© los loops de Python son lentos?", pt: "Por que os loops de Python s√£o lentos?" },
        answer: { es: "Python es interpretado y tiene overhead por operaci√≥n. Pandas/NumPy usan C bajo el cap√≥ - una operaci√≥n vectorizada hace millones de operaciones en C, no en Python.", pt: "Python √© interpretado e tem overhead por opera√ß√£o. Pandas/NumPy usam C por baixo dos panos - uma opera√ß√£o vetorizada faz milh√µes de opera√ß√µes em C, n√£o em Python." }
      },
      {
        question: { es: "¬øCu√°ndo usar√≠as multiprocessing vs multithreading?", pt: "Quando voc√™ usaria multiprocessing vs multithreading?" },
        answer: { es: "Multiprocessing para CPU-bound (c√°lculos). Multithreading para I/O-bound (network, disco). Python tiene el GIL que limita threads para CPU.", pt: "Multiprocessing para CPU-bound (c√°lculos). Multithreading para I/O-bound (network, disco). Python tem o GIL que limita threads para CPU." }
      }
    ],
    closingStatement: { es: "Performance tuning es el skill que te hace invaluable - cualquiera puede hacer que funcione, pocos pueden hacerlo r√°pido.", pt: "Performance tuning √© a habilidade que te torna inestim√°vel - qualquer um pode fazer funcionar, poucos podem faz√™-lo r√°pido." }
  },
  steps: [
    {
      order: 1,
      text: { es: 'üêå Cre√° un pipeline lento (a prop√≥sito)', pt: 'üêå Crie um pipeline lento (de prop√≥sito)' },
      code: `# slow_pipeline.py - Versi√≥n lenta
import pandas as pd
import time

def slow_pipeline(filepath: str):
    start = time.time()
    
    # Cargar TODO en memoria
    df = pd.read_json(filepath)
    
    # Loop ineficiente
    results = []
    for _, row in df.iterrows():
        if row['total'] > 100:
            results.append({
                'customer_id': row['customer_id'],
                'total': row['total'],
                'category': row['category'].upper()
            })
    
    result_df = pd.DataFrame(results)
    
    # Agregaci√≥n en loop
    totals = {}
    for _, row in result_df.iterrows():
        cid = row['customer_id']
        if cid not in totals:
            totals[cid] = 0
        totals[cid] += row['total']
    
    elapsed = time.time() - start
    print(f"Tiempo: {elapsed:.2f}s")
    return totals`,
      explanation: { es: 'Este c√≥digo tiene varios problemas: iterrows, loops, no usa operaciones vectorizadas.', pt: 'Este c√≥digo tem v√°rios problemas: iterrows, loops, n√£o usa opera√ß√µes vetorizadas.' },
      checkpoint: { es: '¬øPod√©s identificar al menos 3 problemas de performance?', pt: 'Consegue identificar pelo menos 3 problemas de performance?' }
    },
    {
      order: 2,
      text: { es: 'üîç Hac√© profiling', pt: 'üîç Fa√ßa profiling' },
      code: `import cProfile
import pstats

# Profiling con cProfile
profiler = cProfile.Profile()
profiler.enable()

result = slow_pipeline('data/logs_access_logs.csv')

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 funciones`,
      explanation: { es: 'cProfile te muestra d√≥nde se gasta el tiempo. Busc√° las funciones que m√°s tardan.', pt: 'cProfile mostra onde se gasta o tempo. Procure as fun√ß√µes que mais demoram.' },
      tip: { es: 'Enfocate en las funciones con m√°s "cumtime" (tiempo acumulado).', pt: 'Foque nas fun√ß√µes com mais "cumtime" (tempo acumulado).' }
    },
    {
      order: 3,
      text: { es: '‚ö° Optimiz√°: Vectorizaci√≥n', pt: '‚ö° Otimize: Vetoriza√ß√£o' },
      code: `# fast_pipeline.py - Versi√≥n optimizada
import pandas as pd
import time

def fast_pipeline(filepath: str):
    start = time.time()
    
    # Cargar solo columnas necesarias (logs tienen muchas columnas)
    df = pd.read_csv(filepath, usecols=['timestamp', 'service_id', 'status_code', 'response_time_ms'])
    
    # Filtrar con vectorizaci√≥n (NO iterrows) - errores 5xx
    errors = df[df['status_code'] >= 500].copy()
    
    # Agregaci√≥n vectorizada - errores por servicio
    error_counts = errors.groupby('service_id')['status_code'].count().to_dict()
    
    elapsed = time.time() - start
    print(f"Tiempo: {elapsed:.2f}s")
    return error_counts`,
      explanation: { es: 'Operaciones vectorizadas de Pandas son 10-100x m√°s r√°pidas que loops.', pt: 'Opera√ß√µes vetorizadas de Pandas s√£o 10-100x mais r√°pidas que loops.' },
      checkpoint: { es: '¬øCu√°nto m√°s r√°pido es la versi√≥n optimizada?', pt: 'Quanto mais r√°pido √© a vers√£o otimizada?' }
    },
    {
      order: 4,
      text: { es: 'üìä Optimiz√° queries SQL', pt: 'üìä Otimize queries SQL' },
      code: `-- Query lenta (subqueries correlacionados)
SELECT 
    o.customer_id,
    (SELECT SUM(total) FROM orders o2 WHERE o2.customer_id = o.customer_id) as total
FROM orders o
WHERE o.total > 100;

-- Query optimizada (una sola pasada)
SELECT 
    customer_id,
    SUM(total) as total
FROM orders
WHERE total > 100
GROUP BY customer_id;`,
      explanation: { es: 'Los subqueries correlacionados se ejecutan para CADA fila. Evitalos.', pt: 'As subqueries correlacionadas s√£o executadas para CADA linha. Evite-as.' },
      tip: { es: 'Us√° EXPLAIN ANALYZE para ver el plan de ejecuci√≥n.', pt: 'Use EXPLAIN ANALYZE para ver o plano de execu√ß√£o.' }
    },
    {
      order: 5,
      text: { es: 'üîÑ Agreg√° paralelismo', pt: 'üîÑ Adicione paralelismo' },
      code: `from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd

def process_chunk(chunk: pd.DataFrame) -> dict:
    """Procesa un chunk de datos."""
    filtered = chunk[chunk['total'] > 100]
    return filtered.groupby('customer_id')['total'].sum().to_dict()

def parallel_pipeline(filepath: str, num_workers: int = 4):
    # Cargar logs y dividir en chunks (logs suelen ser enormes)
    df = pd.read_csv(filepath)
    chunks = np.array_split(df, num_workers)
    
    # Procesar en paralelo
    results = []
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [executor.submit(process_chunk, chunk) for chunk in chunks]
        
        for future in as_completed(futures):
            results.append(future.result())
    
    # Combinar resultados
    combined = {}
    for result in results:
        for k, v in result.items():
            combined[k] = combined.get(k, 0) + v
    
    return combined

# Los logs son ideales para optimizaci√≥n: millones de registros, 
# operaciones repetitivas, mucho potencial de mejora`,
      explanation: { es: 'ThreadPoolExecutor permite procesar chunks en paralelo.', pt: 'ThreadPoolExecutor permite processar chunks em paralelo.' },
      warning: { es: 'Paralelismo tiene overhead. Solo vale la pena para tareas que tardan >1s.', pt: 'Paralelismo tem overhead. S√≥ vale a pena para tarefas que demoram >1s.' }
    },
    {
      order: 6,
      text: { es: 'üìà Med√≠ y document√° mejoras', pt: 'üìà Me√ßa e documente melhorias' },
      code: `import time

def benchmark(func, *args, runs: int = 5):
    """Ejecuta funci√≥n m√∫ltiples veces y reporta estad√≠sticas."""
    times = []
    for _ in range(runs):
        start = time.time()
        func(*args)
        times.append(time.time() - start)
    
    return {
        'mean': sum(times) / len(times),
        'min': min(times),
        'max': max(times)
    }

# Comparar versiones con datos de logs
slow_stats = benchmark(slow_pipeline, 'data/logs_access_logs.csv')
fast_stats = benchmark(fast_pipeline, 'data/logs_access_logs.csv')

speedup = slow_stats['mean'] / fast_stats['mean']
print(f"Speedup: {speedup:.1f}x")`,
      explanation: { es: 'Siempre med√≠ antes y despu√©s. Sin n√∫meros, no sab√©s si mejoraste.', pt: 'Sempre me√ßa antes e depois. Sem n√∫meros, voc√™ n√£o sabe se melhorou.' },
      checkpoint: { es: '¬øLograste mejora de al menos 5x?', pt: 'Conseguiu melhoria de pelo menos 5x?' }
    },
    {
      order: 7,
      text: { es: 'üìù Document√° cada optimizaci√≥n', pt: 'üìù Documente cada otimiza√ß√£o' },
      code: `# Documentaci√≥n de optimizaciones

## Resumen
Pipeline optimizado de 45s a 3s (15x speedup)

## Optimizaciones aplicadas

| # | Cambio | Impacto | Antes | Despu√©s |
|---|--------|---------|-------|---------|
| 1 | Vectorizaci√≥n | 5x | 45s | 9s |
| 2 | Eliminar subqueries | 2x | 9s | 4.5s |
| 3 | Paralelismo (4 workers) | 1.5x | 4.5s | 3s |

## Lecciones
- iterrows() es 100x m√°s lento que vectorizaci√≥n
- Subqueries correlacionados son O(n¬≤)
- Paralelismo tiene overhead, solo para tareas >1s`,
      explanation: { es: 'Documentar cada optimizaci√≥n ayuda a otros (y a vos en el futuro).', pt: 'Documentar cada otimiza√ß√£o ajuda os outros (e voc√™ no futuro).' }
    },
  ],
  deliverable: { es: 'C√≥digo antes/despu√©s + documento con an√°lisis de mejoras', pt: 'C√≥digo antes/depois + documento com an√°lise de melhorias' },
  evaluation: [
    { es: '¬øLograste mejora de al menos 5x?', pt: 'Conseguiu melhoria de pelo menos 5x?' },
    { es: '¬øDocumentaste cada cambio y su impacto?', pt: 'Documentou cada mudan√ßa e seu impacto?' },
    { es: '¬øEl c√≥digo sigue siendo legible y mantenible?', pt: 'O c√≥digo continua leg√≠vel e manuten√≠vel?' },
    { es: '¬øUsaste profiling para identificar cuellos de botella?', pt: 'Usou profiling para identificar gargalos?' },
  ],
  theory: {
    es: `## T√©cnicas de Optimizaci√≥n

### Python/Pandas
1. **Vectorizaci√≥n**: Usar operaciones de Pandas, no loops
2. **Tipos correctos**: category para strings repetidos
3. **Chunks**: Procesar en partes si no cabe en memoria
4. **Cython/Numba**: Para c√≥digo num√©rico cr√≠tico

### SQL
1. **√çndices**: En columnas de WHERE y JOIN
2. **Evitar SELECT ***: Solo columnas necesarias
3. **CTEs vs Subqueries**: CTEs son m√°s legibles y a veces m√°s r√°pidos
4. **EXPLAIN**: Siempre revisar el plan

### General
1. **Medir primero**: No optimizar sin datos
2. **Profiling**: Encontrar el cuello de botella real
3. **80/20**: El 20% del c√≥digo causa el 80% de la lentitud
4. **Caching**: No recalcular lo que ya calculaste`,
    pt: `## T√©cnicas de Otimiza√ß√£o

### Python/Pandas
1. **Vetoriza√ß√£o**: Usar opera√ß√µes de Pandas, n√£o loops
2. **Tipos corretos**: category para strings repetidas
3. **Chunks**: Processar em partes se n√£o couber na mem√≥ria
4. **Cython/Numba**: Para c√≥digo num√©rico cr√≠tico

### SQL
1. **√çndices**: Em colunas de WHERE e JOIN
2. **Evitar SELECT ***: Apenas colunas necess√°rias
3. **CTEs vs Subqueries**: CTEs s√£o mais leg√≠veis e √†s vezes mais r√°pidos
4. **EXPLAIN**: Sempre revisar o plano

### Geral
1. **Medir primeiro**: N√£o otimizar sem dados
2. **Profiling**: Encontrar o gargalo real
3. **80/20**: Os 20% do c√≥digo causam 80% da lentid√£o
4. **Caching**: N√£o recalcular o que j√° calculou`
  },
};


