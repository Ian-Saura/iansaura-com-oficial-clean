import { Project } from '../../../types/members';

export const p3DataLakeDesign: Project = {
  id: 'p3-data-lake-design',
  level: 3,
  title: {
    es: 'DiseÃ±o de Data Lake con Medallion Architecture',
    pt: 'Design de Data Lake com Medallion Architecture'
  },
  description: {
    es: 'DiseÃ±Ã¡ un Data Lake con zonas Bronze/Silver/Gold. La arquitectura que usan Databricks, Netflix, y la mayorÃ­a de empresas modernas.',
    pt: 'Projete um Data Lake com zonas Bronze/Silver/Gold. A arquitetura usada por Databricks, Netflix e a maioria das empresas modernas.'
  },
  difficulty: 'Expert',
  duration: '5-6 horas',
  skills: [
    { es: 'Data Lake', pt: 'Data Lake' },
    { es: 'Medallion Architecture', pt: 'Medallion Architecture' },
    { es: 'Delta Lake', pt: 'Delta Lake' },
    { es: 'S3', pt: 'S3' },
    { es: 'Spark', pt: 'Spark' }
  ],
  icon: 'ğŸ…',
  color: 'blue',
  datasetId: 'ecommerce',
  prerequisites: ['p5-aws-pipeline', 'p2-spark-processing'],
  estimatedLines: 200,
  realWorldExample: {
    es: 'AsÃ­ estructura Databricks su propio Data Lake interno',
    pt: 'Assim a Databricks estrutura seu prÃ³prio Data Lake interno'
  },
  usedBy: ['Databricks', 'Netflix', 'Comcast', 'Shell'],
  expectedOutputs: [
    {
      step: 4,
      description: { es: 'Estructura de Data Lake', pt: 'Estrutura de Data Lake' },
      example: `s3://my-data-lake/
â”œâ”€â”€ bronze/          (raw, append-only)
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ _delta_log/
â”‚   â””â”€â”€ customers/
â”œâ”€â”€ silver/          (cleaned, deduplicated)
â”‚   â”œâ”€â”€ orders_clean/
â”‚   â””â”€â”€ customers_clean/
â””â”€â”€ gold/            (business-ready)
    â”œâ”€â”€ daily_sales/
    â””â”€â”€ customer_360/

Delta tables: 5
Total size: 2.3 GB
Time travel: 30 days`
    },
  ],
  learningObjectives: [
    { es: 'DiseÃ±ar estructura de Data Lake', pt: 'Projetar estrutura de Data Lake' },
    { es: 'Implementar Bronze/Silver/Gold', pt: 'Implementar Bronze/Silver/Gold' },
    { es: 'Usar Delta Lake para ACID', pt: 'Usar Delta Lake para ACID' },
    { es: 'Manejar schema evolution', pt: 'Gerenciar schema evolution' },
    { es: 'Implementar time travel', pt: 'Implementar time travel' },
  ],
  interviewStory: {
    hook: {
      es: "DiseÃ±Ã© un Data Lake con Medallion Architecture que procesa 1TB diario y permite queries sobre 3 aÃ±os de historia en segundos.",
      pt: "Projetei um Data Lake com Medallion Architecture que processa 1TB diÃ¡rio e permite queries sobre 3 anos de histÃ³ria em segundos."
    },
    situation: {
      es: "Los datos estaban en silos: cada equipo tenÃ­a su copia, formatos diferentes, sin historial. Nadie confiaba en los nÃºmeros.",
      pt: "Os dados estavam em silos: cada equipe tinha sua cÃ³pia, formatos diferentes, sem histÃ³rico. NinguÃ©m confiava nos nÃºmeros."
    },
    task: {
      es: "DiseÃ±ar un Data Lake centralizado con zonas claras, calidad garantizada, y capacidad de time travel.",
      pt: "Projetar um Data Lake centralizado com zonas claras, qualidade garantida e capacidade de time travel."
    },
    actions: [
      { es: "DiseÃ±Ã© Bronze/Silver/Gold con responsabilidades claras para cada zona", pt: "Projetei Bronze/Silver/Gold com responsabilidades claras para cada zona" },
      { es: "ImplementÃ© ingesta incremental a Bronze preservando datos originales", pt: "Implementei ingestÃ£o incremental para Bronze preservando dados originais" },
      { es: "CreÃ© pipelines de limpieza Bronzeâ†’Silver con validaciones", pt: "Criei pipelines de limpeza Bronzeâ†’Silver com validaÃ§Ãµes" },
      { es: "ConstruÃ­ modelos de negocio en Gold optimizados para queries", pt: "ConstruÃ­ modelos de negÃ³cio em Gold otimizados para queries" },
      { es: "UsÃ© Delta Lake para ACID, time travel y schema evolution", pt: "Usei Delta Lake para ACID, time travel e schema evolution" }
    ],
    results: [
      { es: "Una sola fuente de verdad para toda la empresa", pt: "Uma Ãºnica fonte de verdade para toda a empresa" },
      { es: "Queries sobre 3 aÃ±os de historia en <10 segundos", pt: "Queries sobre 3 anos de histÃ³ria em <10 segundos" },
      { es: "Time travel: podemos ver cÃ³mo estaban los datos hace 30 dÃ­as", pt: "Time travel: podemos ver como estavam os dados hÃ¡ 30 dias" },
      { es: "Schema evolution sin romper pipelines existentes", pt: "Schema evolution sem quebrar pipelines existentes" }
    ],
    learnings: [
      { es: "Bronze es sagrado - nunca modificar, solo append", pt: "Bronze Ã© sagrado - nunca modificar, apenas append" },
      { es: "Delta Lake cambia el juego - ACID en un Data Lake era imposible antes", pt: "Delta Lake muda o jogo - ACID em um Data Lake era impossÃ­vel antes" },
      { es: "La separaciÃ³n de zonas es organizacional, no solo tÃ©cnica", pt: "A separaÃ§Ã£o de zonas Ã© organizacional, nÃ£o apenas tÃ©cnica" }
    ],
    possibleQuestions: [
      {
        question: { es: "Â¿Por quÃ© 3 zonas y no 2?", pt: "Por que 3 zonas e nÃ£o 2?" },
        answer: { es: "Bronze: datos crudos para auditorÃ­a. Silver: datos limpios reutilizables. Gold: modelos de negocio especÃ­ficos. Cada zona tiene diferente audiencia y SLA.", pt: "Bronze: dados crus para auditoria. Silver: dados limpos reutilizÃ¡veis. Gold: modelos de negÃ³cio especÃ­ficos. Cada zona tem diferente audiÃªncia e SLA." }
      },
      {
        question: { es: "Â¿CÃ³mo manejÃ¡s schema evolution?", pt: "Como vocÃª lida com schema evolution?" },
        answer: { es: "Delta Lake permite agregar columnas sin romper lectores. Para cambios breaking, versionamos el schema y migramos gradualmente. Nunca cambios in-place.", pt: "Delta Lake permite adicionar colunas sem quebrar leitores. Para mudanÃ§as breaking, versionamos o schema e migramos gradualmente. Nunca mudanÃ§as in-place." }
      },
      {
        question: { es: "Â¿Data Lake vs Data Warehouse?", pt: "Data Lake vs Data Warehouse?" },
        answer: { es: "Data Lake: storage barato, schemas flexibles, datos raw. Data Warehouse: queries rÃ¡pidas, schemas fijos, datos modelados. Lakehouse combina ambos con Delta Lake.", pt: "Data Lake: armazenamento barato, schemas flexÃ­veis, dados raw. Data Warehouse: queries rÃ¡pidas, schemas fixos, dados modelados. Lakehouse combina ambos com Delta Lake." }
      }
    ],
    closingStatement: { es: "Medallion Architecture es el estÃ¡ndar de facto - si no lo conocÃ©s, estÃ¡s atrasado.", pt: "Medallion Architecture Ã© o padrÃ£o de fato - se vocÃª nÃ£o conhece, estÃ¡ atrasado." }
  },
  steps: [
    {
      order: 1,
      text: { es: 'ğŸ¥‰ DiseÃ±Ã¡ zona Bronze (Raw)', pt: 'ğŸ¥‰ Projete a zona Bronze (Raw)' },
      explanation: {
        es: `**Bronze = Datos crudos, sin modificar**

Estructura:
\`\`\`
s3://data-lake/bronze/
â”œâ”€â”€ ecommerce/
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ ingestion_date=2024-01-15/
â”‚   â”‚       â””â”€â”€ orders_20240115_143000.json
â”‚   â”œâ”€â”€ customers/
â”‚   â””â”€â”€ products/
â””â”€â”€ marketing/
    â””â”€â”€ campaigns/
\`\`\`

Reglas:
- Nunca modificar datos en Bronze
- Mantener formato original (JSON, CSV)
- Particionar por fecha de ingesta
- RetenciÃ³n: indefinida (es tu backup)`,
        pt: `**Bronze = Dados crus, sem modificar**

Estrutura:
\`\`\`
s3://data-lake/bronze/
â”œâ”€â”€ ecommerce/
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ ingestion_date=2024-01-15/
â”‚   â”‚       â””â”€â”€ orders_20240115_143000.json
â”‚   â”œâ”€â”€ customers/
â”‚   â””â”€â”€ products/
â””â”€â”€ marketing/
    â””â”€â”€ campaigns/
\`\`\`

Regras:
- Nunca modificar dados em Bronze
- Manter formato original (JSON, CSV)
- Particionar por data de ingestÃ£o
- RetenÃ§Ã£o: indefinida (Ã© seu backup)`
      },
      checkpoint: { es: 'Â¿Tu estructura de Bronze estÃ¡ clara?', pt: 'Sua estrutura de Bronze estÃ¡ clara?' }
    },
    {
      order: 2,
      text: { es: 'ğŸ¥ˆ DiseÃ±Ã¡ zona Silver (Cleaned)', pt: 'ğŸ¥ˆ Projete a zona Silver (Cleaned)' },
      explanation: {
        es: `**Silver = Datos limpios, validados, deduplicados**

Estructura:
\`\`\`
s3://data-lake/silver/
â”œâ”€â”€ ecommerce/
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ year=2024/month=01/day=15/
â”‚   â”‚       â””â”€â”€ part-00000.parquet
â”‚   â”œâ”€â”€ customers/
â”‚   â””â”€â”€ products/
â””â”€â”€ marketing/
    â””â”€â”€ campaigns/
\`\`\`

Transformaciones:
- Convertir a Parquet (columnar, comprimido)
- Limpiar nulos y duplicados
- Validar tipos y rangos
- Particionar por fecha del evento (no ingesta)`,
        pt: `**Silver = Dados limpos, validados, deduplicados**

Estrutura:
\`\`\`
s3://data-lake/silver/
â”œâ”€â”€ ecommerce/
â”‚   â”œâ”€â”€ orders/
â”‚   â”‚   â””â”€â”€ year=2024/month=01/day=15/
â”‚   â”‚       â””â”€â”€ part-00000.parquet
â”‚   â”œâ”€â”€ customers/
â”‚   â””â”€â”€ products/
â””â”€â”€ marketing/
    â””â”€â”€ campaigns/
\`\`\`

TransformaÃ§Ãµes:
- Converter para Parquet (colunar, comprimido)
- Limpar nulos e duplicados
- Validar tipos e faixas
- Particionar por data do evento (nÃ£o ingestÃ£o)`
      },
      checkpoint: { es: 'Â¿Definiste quÃ© transformaciones se hacen en Silver?', pt: 'Definiu que transformaÃ§Ãµes sÃ£o feitas em Silver?' }
    },
    {
      order: 3,
      text: { es: 'ğŸ¥‡ DiseÃ±Ã¡ zona Gold (Curated)', pt: 'ğŸ¥‡ Projete a zona Gold (Curated)' },
      explanation: {
        es: `**Gold = Datos agregados, listos para consumo**

Estructura:
\`\`\`
s3://data-lake/gold/
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ daily_sales/
â”‚   â”œâ”€â”€ customer_360/
â”‚   â””â”€â”€ product_performance/
â”œâ”€â”€ ml_features/
â”‚   â”œâ”€â”€ user_features/
â”‚   â””â”€â”€ product_features/
â””â”€â”€ reports/
    â”œâ”€â”€ executive_dashboard/
    â””â”€â”€ marketing_roi/
\`\`\`

CaracterÃ­sticas:
- Modelo dimensional (star schema)
- Pre-agregaciones para dashboards
- Features para ML
- Optimizado para queries especÃ­ficas`,
        pt: `**Gold = Dados agregados, prontos para consumo**

Estrutura:
\`\`\`
s3://data-lake/gold/
â”œâ”€â”€ analytics/
â”‚   â”œâ”€â”€ daily_sales/
â”‚   â”œâ”€â”€ customer_360/
â”‚   â””â”€â”€ product_performance/
â”œâ”€â”€ ml_features/
â”‚   â”œâ”€â”€ user_features/
â”‚   â””â”€â”€ product_features/
â””â”€â”€ reports/
    â”œâ”€â”€ executive_dashboard/
    â””â”€â”€ marketing_roi/
\`\`\`

CaracterÃ­sticas:
- Modelo dimensional (star schema)
- PrÃ©-agregaÃ§Ãµes para dashboards
- Features para ML
- Otimizado para queries especÃ­ficas`
      },
      checkpoint: { es: 'Â¿Tu Gold tiene modelos Ãºtiles para el negocio?', pt: 'Seu Gold tem modelos Ãºteis para o negÃ³cio?' }
    },
    {
      order: 4,
      text: { es: 'ğŸ“¥ ImplementÃ¡ ingesta a Bronze', pt: 'ğŸ“¥ Implemente ingestÃ£o para Bronze' },
      code: `# ingest_to_bronze.py
import boto3
from datetime import datetime
import json

def ingest_to_bronze(data: dict, source: str, table: str):
    """Ingesta datos crudos a Bronze."""
    s3 = boto3.client('s3')
    
    # Path con fecha de ingesta
    now = datetime.now()
    path = f"bronze/{source}/{table}/ingestion_date={now.strftime('%Y-%m-%d')}/{table}_{now.strftime('%Y%m%d_%H%M%S')}.json"
    
    # Subir sin modificar
    s3.put_object(
        Bucket='data-lake',
        Key=path,
        Body=json.dumps(data),
        ContentType='application/json'
    )
    
    print(f"Ingested to {path}")
    return path`,
      explanation: { es: 'Bronze recibe datos crudos sin transformaciÃ³n. La fecha de ingesta es metadata.', pt: 'Bronze recebe dados crus sem transformaÃ§Ã£o. A data de ingestÃ£o Ã© metadado.' }
    },
    {
      order: 5,
      text: { es: 'ğŸ”„ ImplementÃ¡ Bronze â†’ Silver', pt: 'ğŸ”„ Implemente Bronze â†’ Silver' },
      code: `# bronze_to_silver.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, when

spark = SparkSession.builder.appName("BronzeToSilver").getOrCreate()

def bronze_to_silver(source: str, table: str, date: str):
    """Transforma datos de Bronze a Silver."""
    
    # Leer de Bronze
    bronze_path = f"s3://data-lake/bronze/{source}/{table}/ingestion_date={date}/"
    df = spark.read.json(bronze_path)
    
    # Limpiar
    df_clean = df \\
        .dropDuplicates(['order_id']) \\
        .filter(col('total').isNotNull()) \\
        .withColumn('order_date', to_date(col('order_date'))) \\
        .withColumn('total', col('total').cast('decimal(10,2)'))
    
    # Validar
    invalid_count = df_clean.filter(col('total') < 0).count()
    if invalid_count > 0:
        print(f"Warning: {invalid_count} invalid records")
    
    # Guardar en Silver (particionado por fecha del evento)
    silver_path = f"s3://data-lake/silver/{source}/{table}/"
    df_clean.write \\
        .partitionBy('year', 'month', 'day') \\
        .mode('append') \\
        .parquet(silver_path)
    
    print(f"Wrote {df_clean.count()} records to Silver")`,
      explanation: { es: 'Silver limpia, valida, y cambia el particionamiento a fecha del evento.', pt: 'Silver limpa, valida e muda o particionamento para a data do evento.' }
    },
    {
      order: 6,
      text: { es: 'ğŸ“Š ImplementÃ¡ Silver â†’ Gold', pt: 'ğŸ“Š Implemente Silver â†’ Gold' },
      code: `# silver_to_gold.py
def silver_to_gold_daily_sales():
    """Genera mÃ©tricas diarias de ventas."""
    
    # Leer de Silver
    orders = spark.read.parquet("s3://data-lake/silver/ecommerce/orders/")
    products = spark.read.parquet("s3://data-lake/silver/ecommerce/products/")
    
    # Agregar
    daily_sales = orders \\
        .join(products, 'product_id') \\
        .groupBy('order_date', 'category') \\
        .agg(
            F.sum('total').alias('revenue'),
            F.count('order_id').alias('order_count'),
            F.countDistinct('customer_id').alias('unique_customers')
        )
    
    # Guardar en Gold
    daily_sales.write \\
        .mode('overwrite') \\
        .partitionBy('order_date') \\
        .parquet("s3://data-lake/gold/analytics/daily_sales/")`,
      explanation: { es: 'Gold contiene modelos optimizados para casos de uso especÃ­ficos.', pt: 'Gold contÃ©m modelos otimizados para casos de uso especÃ­ficos.' }
    },
    {
      order: 7,
      text: { es: 'ğŸ”„ UsÃ¡ Delta Lake para ACID', pt: 'ğŸ”„ Use Delta Lake para ACID' },
      code: `# Con Delta Lake
from delta import DeltaTable

# Escribir como Delta
df_clean.write \\
    .format('delta') \\
    .mode('append') \\
    .save("s3://data-lake/silver/ecommerce/orders/")

# Time travel (leer versiÃ³n anterior)
df_yesterday = spark.read \\
    .format('delta') \\
    .option('versionAsOf', 5) \\
    .load("s3://data-lake/silver/ecommerce/orders/")

# Schema evolution
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")`,
      explanation: { es: 'Delta Lake agrega ACID, time travel, y schema evolution sobre Parquet.', pt: 'Delta Lake adiciona ACID, time travel e schema evolution sobre Parquet.' },
      tip: { es: 'Delta Lake es el estÃ¡ndar para Data Lakes modernos.', pt: 'Delta Lake Ã© o padrÃ£o para Data Lakes modernos.' }
    },
    {
      order: 8,
      text: { es: 'ğŸ“ DocumentÃ¡ la arquitectura', pt: 'ğŸ“ Documente a arquitetura' },
      explanation: {
        es: `CreÃ¡ documentaciÃ³n con:
1. Diagrama de zonas (Bronze â†’ Silver â†’ Gold)
2. DescripciÃ³n de cada zona
3. Jobs que mueven datos entre zonas
4. PolÃ­ticas de retenciÃ³n
5. CÃ³mo hacer backfill`,
        pt: `Crie documentaÃ§Ã£o com:
1. Diagrama de zonas (Bronze â†’ Silver â†’ Gold)
2. DescriÃ§Ã£o de cada zona
3. Jobs que movem dados entre zonas
4. PolÃ­ticas de retenÃ§Ã£o
5. Como fazer backfill`
      },
      checkpoint: { es: 'Â¿Tu documentaciÃ³n explica cÃ³mo opera el Data Lake?', pt: 'Sua documentaÃ§Ã£o explica como opera o Data Lake?' }
    },
  ],
  deliverable: { es: 'CÃ³digo de ingesta + transformaciÃ³n + documentaciÃ³n de arquitectura', pt: 'CÃ³digo de ingestÃ£o + transformaÃ§Ã£o + documentaÃ§Ã£o de arquitetura' },
  evaluation: [
    { es: 'Â¿Las zonas estÃ¡n bien definidas?', pt: 'As zonas estÃ£o bem definidas?' },
    { es: 'Â¿Bronze mantiene datos crudos sin modificar?', pt: 'Bronze mantÃ©m dados crus sem modificar?' },
    { es: 'Â¿Silver tiene datos limpios y validados?', pt: 'Silver tem dados limpos e validados?' },
    { es: 'Â¿Gold tiene modelos Ãºtiles para el negocio?', pt: 'Gold tem modelos Ãºteis para o negÃ³cio?' },
    { es: 'Â¿Consideraste Delta Lake para ACID?', pt: 'Considerou Delta Lake para ACID?' },
  ],
  theory: {
    es: `## Data Lake vs Data Warehouse vs Data Lakehouse

| Aspecto | Data Lake | Data Warehouse | Data Lakehouse |
|---------|-----------|----------------|----------------|
| **Storage** | Barato (S3) | Caro (DW) | Barato (S3) |
| **Schema** | Schema-on-read | Schema-on-write | Ambos |
| **ACID** | âŒ | âœ… | âœ… (Delta/Iceberg) |
| **Queries** | Lentas | RÃ¡pidas | RÃ¡pidas |
| **ML/Raw Data** | âœ… | âŒ | âœ… |
| **BI/SQL** | âŒ | âœ… | âœ… |

**Data Lakehouse = Data Lake + ACID + Performance**

## Medallion Architecture (Bronze â†’ Silver â†’ Gold)

### Bronze (Raw Zone)
- **QuÃ©**: Datos crudos, exactamente como llegaron
- **Formato**: JSON, CSV, Avro (formato original)
- **Particionado**: Por fecha de ingesta (ingestion_date)
- **RetenciÃ³n**: Indefinida (es tu "backup")
- **Regla**: NUNCA modificar datos en Bronze

\`\`\`
s3://data-lake/bronze/
â”œâ”€â”€ orders/
â”‚   â””â”€â”€ ingestion_date=2024-01-15/
â”‚       â”œâ”€â”€ orders_batch_001.json
â”‚       â””â”€â”€ orders_batch_002.json
\`\`\`

### Silver (Cleaned Zone)
- **QuÃ©**: Datos limpios, validados, deduplicados
- **Formato**: Parquet o Delta Lake
- **Particionado**: Por fecha del evento (event_date)
- **Transformaciones**: Limpieza, tipos correctos, validaciones
- **RetenciÃ³n**: SegÃºn compliance (GDPR: 3 aÃ±os tÃ­pico)

\`\`\`python
# Bronze â†’ Silver
df_silver = (
    df_bronze
    .dropDuplicates(['order_id'])
    .filter(col('amount') > 0)
    .withColumn('event_date', to_date('created_at'))
)
\`\`\`

### Gold (Curated Zone)
- **QuÃ©**: Datos listos para consumo, modelados
- **Formato**: Delta Lake (para queries rÃ¡pidas)
- **Modelos**: Star schema, mÃ©tricas agregadas, features ML
- **Usuarios**: Analistas, BI tools, modelos ML

\`\`\`python
# Silver â†’ Gold (agregaciÃ³n)
df_gold = (
    df_silver
    .groupBy('product_category', 'event_date')
    .agg(
        sum('amount').alias('total_sales'),
        count('order_id').alias('order_count')
    )
)
\`\`\`

## Delta Lake / Apache Iceberg / Apache Hudi

Son "table formats" que agregan ACID a Data Lakes:

| Feature | Parquet | Delta Lake | Iceberg | Hudi |
|---------|---------|------------|---------|------|
| ACID | âŒ | âœ… | âœ… | âœ… |
| Time Travel | âŒ | âœ… | âœ… | âœ… |
| Schema Evolution | âŒ | âœ… | âœ… | âœ… |
| Upserts (MERGE) | âŒ | âœ… | âœ… | âœ… |
| Compaction | Manual | Auto | Auto | Auto |
| Vendor | - | Databricks | Netflix | Uber |

**RecomendaciÃ³n**: Delta Lake si usÃ¡s Databricks, Iceberg si no.

## Patrones de Ingesta

| PatrÃ³n | DescripciÃ³n | CuÃ¡ndo usar |
|--------|-------------|-------------|
| **Full Load** | Cargar todo cada vez | Tablas pequeÃ±as |
| **Incremental** | Solo datos nuevos | Tablas grandes |
| **CDC** | Capturar cambios | Real-time, DW sync |
| **Streaming** | Evento por evento | Latencia crÃ­tica |`,
    pt: `## Data Lake vs Data Warehouse vs Data Lakehouse

| Aspecto | Data Lake | Data Warehouse | Data Lakehouse |
|---------|-----------|----------------|----------------|
| **Storage** | Barato (S3) | Caro (DW) | Barato (S3) |
| **Schema** | Schema-on-read | Schema-on-write | Ambos |
| **ACID** | âŒ | âœ… | âœ… (Delta/Iceberg) |
| **Queries** | Lentas | RÃ¡pidas | RÃ¡pidas |
| **ML/Raw Data** | âœ… | âŒ | âœ… |
| **BI/SQL** | âŒ | âœ… | âœ… |

**Data Lakehouse = Data Lake + ACID + Performance**

## Medallion Architecture (Bronze â†’ Silver â†’ Gold)

### Bronze (Raw Zone)
- **O que**: Dados crus, exatamente como chegaram
- **Formato**: JSON, CSV, Avro (formato original)
- **Particionado**: Por data de ingestÃ£o (ingestion_date)
- **RetenÃ§Ã£o**: Indefinida (Ã© seu "backup")
- **Regra**: NUNCA modificar dados em Bronze

\`\`\`
s3://data-lake/bronze/
â”œâ”€â”€ orders/
â”‚   â””â”€â”€ ingestion_date=2024-01-15/
â”‚       â”œâ”€â”€ orders_batch_001.json
â”‚       â””â”€â”€ orders_batch_002.json
\`\`\`

### Silver (Cleaned Zone)
- **O que**: Dados limpos, validados, deduplicados
- **Formato**: Parquet ou Delta Lake
- **Particionado**: Por data do evento (event_date)
- **TransformaÃ§Ãµes**: Limpeza, tipos corretos, validaÃ§Ãµes
- **RetenÃ§Ã£o**: Segundo compliance (GDPR: 3 anos tÃ­pico)

\`\`\`python
# Bronze â†’ Silver
df_silver = (
    df_bronze
    .dropDuplicates(['order_id'])
    .filter(col('amount') > 0)
    .withColumn('event_date', to_date('created_at'))
)
\`\`\`

### Gold (Curated Zone)
- **O que**: Dados prontos para consumo, modelados
- **Formato**: Delta Lake (para queries rÃ¡pidas)
- **Modelos**: Star schema, mÃ©tricas agregadas, features ML
- **UsuÃ¡rios**: Analistas, ferramentas de BI, modelos ML

\`\`\`python
# Silver â†’ Gold (agregaÃ§Ã£o)
df_gold = (
    df_silver
    .groupBy('product_category', 'event_date')
    .agg(
        sum('amount').alias('total_sales'),
        count('order_id').alias('order_count')
    )
)
\`\`\`

## Delta Lake / Apache Iceberg / Apache Hudi

SÃ£o "table formats" que adicionam ACID a Data Lakes:

| Feature | Parquet | Delta Lake | Iceberg | Hudi |
|---------|---------|------------|---------|------|
| ACID | âŒ | âœ… | âœ… | âœ… |
| Time Travel | âŒ | âœ… | âœ… | âœ… |
| Schema Evolution | âŒ | âœ… | âœ… | âœ… |
| Upserts (MERGE) | âŒ | âœ… | âœ… | âœ… |
| Compaction | Manual | Auto | Auto | Auto |
| Vendor | - | Databricks | Netflix | Uber |

**RecomendaÃ§Ã£o**: Delta Lake se usar Databricks, Iceberg se nÃ£o.

## PadrÃµes de IngestÃ£o

| PadrÃ£o | DescriÃ§Ã£o | Quando usar |
|--------|-------------|-------------|
| **Full Load** | Carregar tudo cada vez | Tabelas pequenas |
| **Incremental** | Apenas dados novos | Tabelas grandes |
| **CDC** | Capturar mudanÃ§as | Real-time, DW sync |
| **Streaming** | Evento por evento | LatÃªncia crÃ­tica |`
  },
};


